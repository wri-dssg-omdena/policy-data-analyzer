{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a search engine based on sBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is a basic implementation of sBERT for searching a database of sentences with queries.\n",
    "\n",
    "The goal is to increase the amount of labeled data that we have in order to later fine tune a model to be used for sentence classification. First of all we have to find a pool of queries that represent the six labels of the six policy instruments. With these queries we can pull a set of sentences that can be automaticaly labeled with the same label of the query. In this way we can increase the diversity of labeled sentences in each label category. This approach will be complemented with a manual curation step to produce a high quality training data set.\n",
    "\n",
    "The policy instruments that we want to find and that correspond to the different labels are:\n",
    "* Direct payment (PES)\n",
    "* Tax deduction\n",
    "* Credit/guarantee\n",
    "* Technical assistance\n",
    "* Supplies\n",
    "* Fines\n",
    "\n",
    "This notebook is intended for the following purposes:\n",
    "* Try different query strategies to find the optimal retrieval of sentences in each policy instrument category\n",
    "* Try different transformers\n",
    "* Be the starting point for further enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is self contained, it does not depend on any other class of the sBERT folder.\n",
    "\n",
    "You just have to create an environment where you install the external dependencies. Usually the dependencies that you have to install are:\n",
    "\n",
    "**For the basic sentence similarity calculation**\n",
    "*  pandas\n",
    "*  boto3\n",
    "*  pytorch\n",
    "*  sentence_transformers\n",
    "\n",
    "**If you want to use ngrams to generate queries**\n",
    "*  nltk\n",
    "*  plotly\n",
    "*  wordcloud\n",
    "\n",
    "**If you want to do evaluation and ploting with pyplot**\n",
    "*  matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment is called nlp then you execute this cell otherwise you change the name of the environment\n",
    "!conda activate nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Libraries for model evaluation\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Libraries to be used in the process of definig queries\n",
    "import nltk # imports the natural language toolkit\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'JSON/ElSalvador.json'\n",
    "\n",
    "obj = s3.Object('wri-latin-talent',filename)\n",
    "serializedObject = obj.get()['Body'].read()\n",
    "policy_list = json.loads(serializedObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going through the dictionary to retrieve sentences, we define a function to reduce de number of sentences in the final \"sentences\" dictionary. This is just for testing purposes. The reason being that running the sentence embedding function takes time. So for initial testing purposes we can reduce the number of sentences in the testing dataset.\n",
    "\n",
    "The variable \"slim_by\" is the reduction factor. If it is set to 1, there will be no reduction and we will be working with the full dataset. It it is set to two, we will take one every two sentences and so one.\n",
    "\n",
    "<span style=\"color:red\"><strong>REMEMBER</strong></span> that you have to re-run the function \"get_sentences_dict\" with the \"slim_by\" variable set to 1 when you want to go for the final shoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_dict(counter, slim_factor): # This is to shrink the sentences dict by a user set factor. It will pick only one sentence every \"slim_factor\"\n",
    "    if counter % slim_factor == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sentence_length_filter(sentence_text, minLength, maxLength):\n",
    "    if len(sentence_text) > minLength:#len(sentence_text) < maxLength and\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_sentences_dict(docs_dict, is_not_incentive_dict, slim_factor, minLength, maxLength):\n",
    "    count = 0\n",
    "    result = {}\n",
    "    for key, value in docs_dict.items():\n",
    "        for item in value: \n",
    "            if item in is_not_incentive_dict:\n",
    "                continue\n",
    "            else:\n",
    "                for sentence in docs_dict[key][item]['sentences']:\n",
    "                    if sentence_length_filter(docs_dict[key][item]['sentences'][sentence][\"text\"], minLength, maxLength):\n",
    "                        count += 1\n",
    "                        if slim_dict(count, slim_by):\n",
    "                            result[sentence] = docs_dict[key][item]['sentences'][sentence]\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will run the function to get your sentences list in a dictionary of this form:\n",
    "\n",
    "{\"\\<sentence id\\>\" : \"\\<text of the sentence\\>\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "                    \"POR TANTO\" : 0,\n",
    "                    \"DISPOSICIONES GENERALES\" : 0,\n",
    "                    \"OBJETO\" : 0,\n",
    "                    \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "# is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "#                     \"POR TANTO\" : 0,\n",
    "#                     \"DISPOSICIONES GENERALES\" : 0,\n",
    "#                     \"OBJETO\" : 0,\n",
    "#                     \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0,\n",
    "#                    \"VISTO\" : 0,\n",
    "#                    \"HEADING\" : 0}\n",
    "\n",
    "slim_by = 1 # REMEMBER to set this variable to the desired value.\n",
    "min_length = 50 # Just to avoid short sentences which might be fragments or headings without a lot of value\n",
    "max_length = 250 # Just to avoid long sentences which might be artifacts or long legal jargon separated by semicolons\n",
    "\n",
    "sentences_2 = get_sentences_dict(policy_list, is_not_incentive, slim_by, min_length, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {**sentences_1, **sentences_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this data set there are 349 policies and 113244 sentences\n"
     ]
    }
   ],
   "source": [
    "# Just to check if the results look ok\n",
    "print(\"In this data set there are {} policies and {} sentences\".format(len(policy_list),len(sentences)))\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Art 11-Los certificados fiduciarios de participacion a que se refiere la presente ley, podran emitirse atendiendo a las caracteristicas siguientes: (',\n",
       " 'labels': []}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[\"70be962_99\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we use the excel file with the selected phrases of each country, process them and get N-grams to define basic queries for the SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'WRI_Policy_Tags (1).xlsx', sheet_name = None)\n",
    "df = None\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(df,pd.DataFrame):\n",
    "            df = value\n",
    "        else:\n",
    "            df = df.append(value)\n",
    "else:\n",
    "    df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = df[\"relevant sentences\"].apply(lambda x: x.split(\";\") if isinstance(x,str) else x)\n",
    "tagged_sentence = []\n",
    "\n",
    "for elem in tagged_sentences:\n",
    "    if isinstance(elem,float) or len(elem) == 0:\n",
    "        continue\n",
    "    elif isinstance(elem,list):\n",
    "        for i in elem:\n",
    "            if len(i.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                tagged_sentence.append(i.strip())\n",
    "    else:\n",
    "        if len(elem.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            tagged_sentence.append(elem.strip())\n",
    "\n",
    "tagged_sentence\n",
    "words_per_sentence = [len(x.split(\" \")) for x in tagged_sentence]\n",
    "plt.hist(words_per_sentence, bins = 50)\n",
    "plt.title(\"Histogram of number of words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "\n",
    "noise_words = []\n",
    "stopwords_corpus = nltk.corpus.stopword\n",
    "sp_stop_words = stopwords_corpus.words('spanish')\n",
    "noise_words.extend(sp_stop_words)\n",
    "print(len(noise_words))\n",
    "\n",
    "if \"no\" in noise_words:\n",
    "    noise_words.remove(\"no\")\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(''.join(tagged_sentence))\n",
    "word_freq = Counter(tokenized_words)\n",
    "# word_freq.most_common(20)\n",
    "# list(ngrams(tokenized_words, 3))\n",
    "\n",
    "word_tokens_clean = [re.findall(r\"[a-zA-Z]+\",each) for each in tokenized_words if each.lower() not in noise_words and len(each.lower()) > 1]\n",
    "word_tokens_clean = [each[0].lower() for each in word_tokens_clean if len(each)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the size of the n-gram that we want to find. The larger it is, the less frequent it will be, unless we substantially increase the number of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 2\n",
    "\n",
    "top_k_ngrams(word_tokens_clean, n_grams, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building queries with Parts-Of-Speech\n",
    "\n",
    "The following functions take a specific word and find the next or previous words according to the POS tags.\n",
    "\n",
    "An example is shown below with the text: <br>\n",
    "\n",
    "text = \"Generar empleo y garantizara la población campesina el bienestar y su participación e incorporación en el desarrollo nacional, y fomentará la actividad agropecuaria y forestal para el óptimo uso de la tierra, con obras de infraestructura, insumos, créditos, servicios de capacitación y asistencia técnica\" <br>\n",
    "\n",
    "next_words(text, \"empleo\", 3) <br>\n",
    "prev_words(text, \"garantizara\", 6) <br>\n",
    "\n",
    "Will return: <br>\n",
    "\n",
    ">['garantizara', 'población', 'campesina'] <br>\n",
    ">['Generar', 'empleo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = es_core_news_md.load()\n",
    "\n",
    "def ExtractInteresting(sentence, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "#     interesting = [k for k,v in nltk.pos_tag(words) if v in match]\n",
    "    doc = nlp(sentence)\n",
    "    interesting = [k.text for k in doc if k.pos_ in match]\n",
    "    return(interesting)\n",
    "\n",
    "def next_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "\n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        \n",
    "        pos_words = [k.text for k in doc[idx:] if k.pos_ in match]\n",
    "        if len(pos_words) > 1: \n",
    "            items.append(pos_words[1])\n",
    "            idx = text.index(pos_words[1])\n",
    "    \n",
    "    return items\n",
    "    \n",
    "def prev_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    \n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        pos_words = [k.text for k in doc[:idx] if k.pos_ in match]\n",
    "        if len(pos_words) >= 1: \n",
    "            items.insert(0, pos_words[-1]) #Add element in order and take the last element since it is the one before the word\n",
    "            idx = text.index(pos_words[-1])\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to find incentive policy instruments\n",
    "keywords = re.compile(r'(asistencia tecnica)|ayuda\\s*s*\\s*\\b|\\bbono\\s*s*\\b\\s*|credito\\s*s*\\b\\s*|incentivo\\s*s*\\b\\s*|insumo\\s*s*\\b\\s*|multa\\s*s*\\b\\s*')\n",
    "# deduccion\\s*(es)*\\b\\s*|devolucion\\s*(es)*\\b\\s*|\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "accents_out = re.compile(r'[áéíóúÁÉÍÓÚ]')\n",
    "accents_dict = {\"á\":\"a\",\"é\":\"e\",\"í\":\"i\",\"ó\":\"o\",\"ú\":\"u\",\"Á\":\"A\",\"É\":\"E\",\"Í\":\"I\",\"Ó\":\"O\",\"Ú\":\"U\"}\n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "# Dictionary to merge variants of a word\n",
    "families = {\n",
    "    \"asistencia tecnica\" : \"asistencia técnica\",\n",
    "    \"ayuda\" : \"ayuda\",\n",
    "    \"ayudas\" : \"ayuda\",\n",
    "    \"bono\" : \"bono\",\n",
    "    \"bonos\" : \"bono\",\n",
    "    \"credito\":  \"crédito\",\n",
    "    \"creditos\":  \"crédito\",\n",
    "#     \"deduccion\" : \"deducción\",\n",
    "#     \"deducciones\" : \"deducción\",\n",
    "#     \"devolucion\" : \"devolución\",\n",
    "#     \"devoluciones\" : \"devolución\",\n",
    "    \"incentivo\" : \"incentivo\",\n",
    "    \"incentivos\" : \"incentivo\",\n",
    "    \"insumo\" : \"insumo\",\n",
    "    \"insumos\" : \"insumo\",\n",
    "    \"multa\" : \"multa\",\n",
    "    \"multas\" : \"multa\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_in_sentences = []\n",
    "            \n",
    "for sentence in sentences:\n",
    "    line = remove_accents(sentences[sentence]['text'])\n",
    "    hit = keywords.search(line)\n",
    "    if hit:\n",
    "        keyword = hit.group(0).rstrip().lstrip()\n",
    "        keyword_in_sentences.append([families[keyword], sentence, sentences[sentence]['text']])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### print(len(keyword_in_sentences))\n",
    "# keyword_in_sentences = sorted(keyword_in_sentences, key = lambda x : x[0])\n",
    "# df_keyword_in_sentences = pd.DataFrame(keyword_in_sentences)\n",
    "\n",
    "# path = \"../output/\"\n",
    "# filename = \"keywords_match_labeling.csv\"\n",
    "# file = path + filename\n",
    "\n",
    "# df_keyword_in_sentences.to_csv(file)\n",
    "\n",
    "# print(keyword_in_sentences[0:20])\n",
    "filtered = [row for row in keyword_in_sentences if row[0] == \"asistencia técnica\"]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key, value in families.items():\n",
    "    if i % 2 == 0:\n",
    "        print(value, \"--\", len([row for row in keyword_in_sentences if row[0] == value]))\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incentives = {}\n",
    "\n",
    "for incentive in families:\n",
    "    incentives[families[incentive]] = 0\n",
    "    \n",
    "incentives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "Then we build a simple function that takes four inputs:\n",
    "1. The model as we have set it in the previous line of code\n",
    "2. A dictionary that contains the sentences {\"\\<sentence_ID\\>\" : {\"text\" : \"The actual sentence\", labels : []}\n",
    "3. A query in the form of a string\n",
    "4. A similarity treshold. It is a float that we can use to limit the results list to the most relevant.\n",
    "\n",
    "The output of the function is a list with three columns with the following content:\n",
    "1. Column 1 contains the id of the sentence\n",
    "2. Column 2 contains the similarity score\n",
    "3. Column 3 contains the text of the sentence that has been compared with the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently two multi language models available for sentence similarity\n",
    "\n",
    "* xlm-r-bert-base-nli-stsb-mean-tokens: Produces similar embeddings as the bert-base-nli-stsb-mean-token model. Trained on parallel data for 50+ languages.\n",
    "<span style=\"color:red\"><strong>Attention!</strong></span> Model \"xlm-r-100langs-bert-base-nli-mean-tokens\" which was the name used in the original Omdena-challenge script has changed to this \"xlm-r-bert-base-nli-stsb-mean-tokens\"\n",
    "\n",
    "* distiluse-base-multilingual-cased-v2: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. While the original mUSE model only supports 16 languages, this multilingual knowledge distilled version supports 50+ languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to create the embeddings for each transformer the embeddings in a json with the following structure:\n",
    "# INPUT PARAMETERS\n",
    "# transformers: a list with transformer names\n",
    "# sentences_dict: a dictionary with the sentences of the database with the form {\"<sentence id>\" : \"<sentence text>\"}}\n",
    "# file: the filepath and filename of the output json\n",
    "# OUTPUT\n",
    "# the embeddings of the sentences in a json with the following structure:\n",
    "# {\"<transformer name>\" : {\"<sentence id>\" : <sentence embedding>}}\n",
    "\n",
    "def create_sentence_embeddings(transformers, sentences_dict, file):\n",
    "    embeddings = {}\n",
    "    for transformer_name in transformers:\n",
    "        model = SentenceTransformer(transformer_name)\n",
    "        embeddings[transformer_name] = {}\n",
    "        for sentence in sentences_dict:\n",
    "            embeddings[transformer_name][sentence] = [model.encode(sentences_dict[sentence]['text'].lower())]\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump(embeddings, fp, cls=NumpyArrayEncoder)\n",
    "     \n",
    "   \n",
    "def highlight(transformer_name, model, sentence_emb, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query.lower())\n",
    "    highlights = []\n",
    "    for sentence in sentences_dict:\n",
    "        sentence_embedding = np.asarray(sentence_emb[sentence])[0]#[transformer_name][sentence])[0]\n",
    "        score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "        if score > similarity_treshold:\n",
    "            highlights.append([sentence, score, sentences_dict[sentence]['text']])\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for sentences in the database\n",
    "\n",
    "This piece of code it's to be executed only once every time the database is chaged or we want to get the embeddings of a new database. For example, we are going to use it once for El Salvador policies and we don't need to use it again until we add new policies to this database. Instead, whenever we want to run experiments on this database, we will load the json files with the embeddings which are in the \"input\" folder.\n",
    "\n",
    "So, the next cell will be kept commented for safety reasons. Un comment it and execute it whenvere you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "transformer_names =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "\n",
    "path = \"../input/\"\n",
    "filename = \"Embeddings_ElSalvador_201223.json\"\n",
    "file = path + filename\n",
    "\n",
    "create_sentence_embeddings(transformer_names, sentences, file)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"The building of a sentence embedding database for El Salvador in the two current models has taken {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the embeddings for database sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of the embeddings for a single country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/\"\n",
    "filename = \"Embeddings_Chile_201223.json\"\n",
    "file = path + filename\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    sentence_embeddings_chile = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and merging all the embeddings for all the countries in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\input\\Embeddings_Chile_201223.json\n",
      "..\\input\\Embeddings_ElSalvador_201223.json\n"
     ]
    }
   ],
   "source": [
    "paths = Path(\"../input/\").glob('**/*.json')\n",
    "\n",
    "i = 0\n",
    "for file_obj in paths:\n",
    "    # because path is object not string\n",
    "    file = str(file_obj)\n",
    "    if \"Embedding\" in file:\n",
    "        if i == 0:\n",
    "            with open(file, \"r\") as f:\n",
    "                print(file)\n",
    "                sentence_embeddings_1 = json.load(f)\n",
    "        else:\n",
    "            with open(file, \"r\") as f:\n",
    "                print(file)\n",
    "                sentence_embeddings_2 = json.load(f)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = {**sentence_embeddings_1[\"xlm-r-bert-base-nli-stsb-mean-tokens\"], **sentence_embeddings_2[\"xlm-r-bert-base-nli-stsb-mean-tokens\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116102\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_embeddings))\n",
    "# for key in sentence_embeddings:\n",
    "#     print(key)\n",
    "#     print(len(sentence_embeddings[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic search with single test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load transformers into the model by choosing one model from index\n",
    "transformer_names =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "model_index = 0\n",
    "model = SentenceTransformer(transformer_names[model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, perform single query searches by manually writing a query in the corresponding field\n",
    "Ti = time.perf_counter()\n",
    "\n",
    "highlighter_query = \"La Policia al tener conocimiento de cualquier infraccion\"\n",
    "similarity_limit = 0.00\n",
    "\n",
    "label_1 = highlight(transformer_names[model_index], model, sentence_embeddings, sentences, highlighter_query, similarity_limit)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"similarity search for El Salvador sentences done in {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_1))\n",
    "label_1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(highlighter_query)\n",
    "label_1[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further filtering of the results by using the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_treshold = 0.5\n",
    "filtered = [row for row in label_1 if row[1] > similarity_treshold]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "export_query = pd.DataFrame(label_1)\n",
    "#export file \n",
    "export_query = pd.DataFrame(label_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiparameter search design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code is just to limit the amount of items in the incentives dictionary for testing purposes\n",
    "# The \"incentives\" dictionari contains the keywords that represent policy instruments. This is to be used in\n",
    "# the following cell where we make a search based on (1) the keywords themselves (2) the first sentence found in policy documents\n",
    "# with each of the keywords.\n",
    "\n",
    "# dicti = {}\n",
    "# i= 0\n",
    "# for key in incentives:\n",
    "#     if i < 2:\n",
    "#         dicti[key] = 0\n",
    "#     i += 1\n",
    "# incentives = dicti\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below is to use a set of queries to search a database for similar sentences with different transformers.\n",
    "# The input parameters are:\n",
    "\n",
    "# Transformer_names: A list with the names of the transformers to be used. For multilingual similarity search we have two transformers\n",
    "# Queries: a list of the queries as strings, that we want to use for searching the database\n",
    "\n",
    "# Similarity_limit: The results are in the form of a similarity coefficient where 1 is a perfect match between the query embedding\n",
    "# and the sentence in the database (the two vectors overlap). If the similarity coefficient is 0 the two vectors are orthogonal,\n",
    "# they do not share anything in common. Thus, in order to restribt the number of results that are kept from the experiment we can\n",
    "# it by setting a similarity threshold.When we have a huge database a good treshold would be 0.3 to 0.5 or even higher.\n",
    "\n",
    "# Results_limit: instead of or complementary to Similarity_limit, we can limit our list of search results by the first sentences\n",
    "# in the similarity ranking. We can set the limit to high numbers in an exploration phase and then reduce this number in a \n",
    "# \"production\" phase\n",
    "\n",
    "# Filename: The results will be exported to the \"output/\" folder in json formate, we need to give it a name witout extension.\n",
    "\n",
    "def multiparameter_sentence_similarity_search(transformer_names, queries, similarity_limit, results_limit, filename):\n",
    "    results = {}\n",
    "    for transformer in transformer_names:\n",
    "        model = SentenceTransformer(transformer)\n",
    "#         results[transformer] = {}\n",
    "        for query in queries:\n",
    "            Ti = time.perf_counter()\n",
    "            similarities = highlight(transformer, model, sentence_embeddings, sentences, query, similarity_limit)\n",
    "            results[query] = similarities[0:results_limit]#results[transformer][query] = similarities[0:results_limit]\n",
    "            Tf = time.perf_counter()\n",
    "            print(f\"similarity search for model {transformer} and query {query} it's been done in {Tf - Ti:0.4f} seconds\")\n",
    "\n",
    "    path = \"../output/\"\n",
    "    filename = filename + \".json\"\n",
    "    file = path + filename\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "    return results\n",
    "\n",
    "# For experiments 2 and 3 this function helps debugging misspelling in the calues of the dictionary\n",
    "def check_dictionary_values(dictionary):\n",
    "    check_country = {}\n",
    "    check_incentive = {}\n",
    "    for key, value in dictionary.items():\n",
    "        incentive, country = value.split(\"-\")\n",
    "        check_incentive[incentive] = 0\n",
    "        check_country[country] = 0\n",
    "    print(check_incentive)\n",
    "    print(check_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key1 in results_Exp2:\n",
    "    print(key1)\n",
    "    for key2 in results_Exp2[key1]:\n",
    "        print(queries_dict_exp2[key2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query building\n",
    "\n",
    "The code to compute sentence similarity will take two imputs:\n",
    "\n",
    "* The queries that will by input as a list of strings. \n",
    "* The embeddings of the sentences in the database. \n",
    "\n",
    "At this point all we need to run the experiment is ready but the list of queries. One can write the list manually, or one can make it from other data flows. The next cells are ment to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><span style=\"color:red\">Experiment 1</span></strong> Queries extracted from the database itself\n",
    "\n",
    "What we do in this experiment is check the capacity of two models:\n",
    "\n",
    "* xlm-r-bert-base-nli-stsb-mean-tokens\n",
    "* distiluse-base-multilingual-cased-v2\n",
    "\n",
    "to find policy instruments for incentives, based in 9 categories:\n",
    "\n",
    "asistencia técnica; ayuda; bono; crédito; deducción; devolución; incentivo; insumo and multa\n",
    "\n",
    "We will compare two approaches:\n",
    "1. to perform a search with the keyword itself\n",
    "2. to perform a search with one of the sentences found in El Salvador policies which contain the keyword.\n",
    "\n",
    "User set parameters:\n",
    "<strong>Transformer names:</strong> this is a list with the different models to test. There are currently two.\n",
    "\n",
    "<strong>Similarity limit:</strong> just to filter out the search matches whith low similarity.\n",
    "\n",
    "<strong>Number of search results:</strong> the search is against all 40.000 sentences in the database, but we don't want to keep all, just the most relevant so we take 1500 as the keyword with most direct matches is \"multa\" with some 1352 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_queries = {\"asistencia técnica\" : \"00a55af_79\", \n",
    "             \"ayuda\" : \"00a55af_61\", \n",
    "             \"bono\" : \"00a55af_80\", \n",
    "             \"crédito\" : \"1cd36a0_11\", \n",
    "             \"incentivo\" : \"51a0d9e_30\",\n",
    "             \"insumo\" : \"731dbf0_11\",\n",
    "             \"multa\" : \"029d411_88\"\n",
    "}\n",
    "incentive = \"asistencia técnica\"\n",
    "[row for row in keyword_in_sentences if row[1] == S_queries[incentive]][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers =['xlm-r-bert-base-nli-stsb-mean-tokens']#, 'distiluse-base-multilingual-cased-v2']\n",
    "queries = [\"Conceder créditos a los productores o propietarios\"]\n",
    "similarity_threshold = 0.2\n",
    "search_results_limit = 100\n",
    "name = \"test201224\"\n",
    "\n",
    "results_dict = multiparameter_sentence_similarity_search(transformers, queries, similarity_threshold, search_results_limit, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "similarity_list = []\n",
    "for key1 in results_dict:\n",
    "    for key2 in results_dict[key1]:\n",
    "        for item in results_dict[key1][key2]:\n",
    "            similarity_list.append([i, item[1]])\n",
    "            if i == 0:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><span style=\"color:red\">Experiment 2</span></strong> Queries from the tagged database\n",
    "\n",
    "Here we use the databse of tagged sentences to define queries. The database is structured by countries. From a list of model documents the sentences were separated and tagged with a policy instrument label. The labels that were used are:\n",
    "\n",
    "* Credit\n",
    "* Direct payment\n",
    "* Fine\n",
    "* Guarantee\n",
    "* Supplies\n",
    "* Tax deduction\n",
    "* Technical assistance\n",
    "\n",
    "Not all countries have tagged sentences for each category so we ended up with 26 queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict_exp2 = {\n",
    "    \"Para efectos del otorgamiento de estímulos fiscales, crediticios o financieros por parte del Estado, se considerarán prioritarias las actividades relacionadas con la conservación y restauración de los hábitats, la protección del ambiente y el aprovechamiento sustentable de los recursos naturales.\" : \"Credit-México\",\n",
    "\"Obtener créditos blandos para mejorar la sostenibillidad y rentabilidad de las actividades de uso de la Diversidad Biológica. Estos créditos podrían beneficiar a sistemas productivos asociados a la pequeña y mediana producción, actividades de experimentación, investigación, producción y comercialización de la Diversidad Biológica, implementación de tecnologías de producción limpia, programas de reforestación u otros que se estipulen.\" : \"Credit-Perú\",\n",
    "\"Se asocia con créditos de enlace INDAP y Banco Estado\" : \"Credit-Chile\", \n",
    "\"El INAB establecerá un programa de garantía crediticia para la actividad forestal, mediante el cual se respaldarán los créditos que otorgue el sistema bancario para el fomento del sector forestal a los pequeños propietarios referidos en el articulo 83 de la presente ley, usando recursos del Fondo Forestal Privativo u otras fuentes, el reglamento debe regular los procedimientos del programa de garantía crediticia a la actividad forestal del pequeño propietario.\" : \"Credit-Guatemala\",\n",
    "\"El Banco Multisectorial de Inversiones establecerá líneas de crédito para que el Sistema Financiero apoye a la pequeña, mediana y microempresa, a fin de que puedan oportunamente adaptarse a las Disposiciones de la presente Ley.\" : \"Credit-El Salvador\",\n",
    "\"Dentro de los incentivos económicos se podrá crear un bono que incentive la conservación del recurso forestal por el Fondo Forestal Mexicano de acuerdo a la disponibilidad de recursos, a fin de retribuir a los propietarios o poseedores de terrenos forestales por los bienes y servicios ambientales generados.\" : \"Direct_payment-México\",\n",
    "\"Los fondos forestales serviran para el pago por arbol prendido a los dos anos de su instalación en terreno definitivo, siempre que provengan de viveros certificados.\" : \"Direct_payment-Perú\",\n",
    "\"El porcentaje de bonificación para pequeños propietarios forestales será del 90% de los costos de la forestación que efectúen en suelos de aptitud preferente ente forestal o en suelos degradados de cualquier clase, incluidas aquellas plantaciones con baja densidad para fines de uso silvopastoral, respecto de las primeras 15 hectáreas y de un 75% respecto de las restantes.\" : \"Direct_payment-Chile\",\n",
    "\"El Estado, en un período de 20 años contados a partir de la vigencia de la presente ley, dará incentivos al establecimiento de plantaciones, su mantenimiento y el manejo de bosques naturales, este incentivo se otorgará a los propietarios de tierras con vocación forestal, una sola vez, de acuerdo al plan de manejo y/o reforestación aprobado por el INAB.\" : \"Direct_payment-Guatemala\",\n",
    "\"Incentivos en dinero: para cubrir los costos directos e indirectos del establecimiento y manejo de areas con sistema agroforestal de cafe\" : \"Direct_payment-El Salvador\",\n",
    "\"Toda persona física o moral que ocasione directa o indirectamente un daño a los recursos forestales, los ecosistemas y sus componentes, estará obligada a repararlo o compensarlo, de conformidad con lo dispuesto en la Ley Federal de Responsabilidad Ambiental.\" : \"Fine-México\",\n",
    "\"Disminuir los riesgos para el inversionista implementando mecanismos de aseguramiento.\" : \"Guarantee-México\",\n",
    "\"Fianza: Podrá garantizarse el cumplimiento de repoblación forestal mediante fi anza otorgada a favor del INAB por cualquiera de las afi anzadoras legalmente autorizadas para funcionar en el país, en base al cuadro siguiente\" : \"Guarantee-Guatemala\",\n",
    "\"La/el sujeto de derecho podrá recibir en especie materiales, insumos, equipos, herramientas, para la instalación y operación de viveros comunitarios.\" : \"Supplies-México\",\n",
    "\"Ello, a través de la utilización de guías, manuales, protocolos, paquetes tecnológicos, procedimientos, entre otros.\" : \"Supplies-Perú\",\n",
    "\"Incentivos en especie: insumos agrícolas, herramientas, asistencia tecnica, estudios de factibilidad y pre factibilidad, elaboracion de planes de manejo, mejoramiento de vías de acceso a las plantaciones, comercializacion y capacitaciones.\" : \"Supplies-El Salvador\",\n",
    "\"Otorgar incentivos fiscales a las plantaciones forestales comerciales, incluyendo incentivos dirigidos a promover la industria ligada a las plantaciones comerciales forestales.\" : \"Tax_deduction-México\",\n",
    "\"25% de descuento en el pago del derecho de aprovechamiento, si el titular de la concesión reporte anualmente sus resultados de inventario forestal, de acuerdo a los lineamientos aprobados por el SERFOR.\" : \"Tax_deduction-Perú\",\n",
    "\"Las bonificaciones percibidas o devengadas se considerarán como ingresos diferidos en el pasivo circulante y no se incluirán para el cálculo de la tasa adicional del artículo 21 de la Ley de la Renta ni constituirán renta para ningún efecto legal hasta el momento en que se efectúe la explotación o venta del bosque que originó la bonificación, oportunidad en la que se amortizará abonándola al costo de explotación a medida y en la proporción en que ésta o la venta del bosque se realicen, aplicándose a las utilidades resultantes el artículo 14°, inciso primero, del presente decreto ley.\" : \"Tax_deduction-Chile\",\n",
    "\"Los contratistas que suscriban contratos de exploración y/o explotación y de sistemas estacionarios de transporte de hidrocarburos, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades que el contratista remese al exterior como pago a sus accionistas, asociados, partícipes o socios, así como las remesas en efectivo y/o en especie y los créditos contables que efectúen a sus casas matríces.\" : \"Tax_deduction-Guatemala\",\n",
    "\"Exención de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestación de Servicios, en la importación de sus bienes, equipos y accesorios, maquinaria, vehículos, aeronaves o embarcaciones para cabotaje y los materiales de construcción para las edificaciones del proyecto.\" : \"Tax_deduction-El Salvador\",\n",
    "\"Formación Permanente Además del acompañamiento técnico, los sujetos de derecho participarán en un proceso permanente de formación a lo largo de todo el año, que les permita enriquecer sus habilidades y capacidades en el ámbito social y productivo.\" : \"Technical_assistance-México\",\n",
    "\"Contribuir en la promoción para la gestión de las plantaciones forestales y agroforestales, a través de la capacitación, asesoramiento, asistencia técnica y educación de los usuarios, en coordinación con la ARFFS.\" : \"Technical_assistance-Perú\",\n",
    "\"Asesoría prestada al usuario por un operador acreditado, conducente a elaborar, acompañar y apoyar la adecuada ejecución técnica en terreno de aquellas prácticas comprometidas en el Plan de Manejo, sólo podrán postular, a esta asistencia, los pequeños productores agrícolas.\" : \"Technical_assistance-Chile\",\n",
    "\"Programas de Capacitación Para la ejecución de programas de capacitación, adiestramiento y otorgamiento de becas para la preparación de personal guatemalteco, así como para el desarrollo de tecnología en actividades directamente relacionadas con las operaciones petroleras objeto del contrato, todo contratista contribuirá con las cantidades de dólares de los Estados Unidos de América que se estipulen en el contrato.\" : \"Technical_assistance-Guatemala\",\n",
    "\"Apoyo técnico y en formulación de proyectos y conexión con mercados\" : \"Technical_assistance-El Salvador\"}\n",
    "\n",
    "queries = []\n",
    "for query in queries_dict_exp2:\n",
    "    queries.append(query)\n",
    "        \n",
    "# print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just to check the presence of misspelling in the values of the queries dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dictionary_values(queries_dict_exp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers =['xlm-r-bert-base-nli-stsb-mean-tokens']#, 'distiluse-base-multilingual-cased-v2']\n",
    "similarity_threshold = 0.2\n",
    "search_results_limit = 100\n",
    "name = \"Exp2_tagged_201228\"\n",
    "\n",
    "results_dict = multiparameter_sentence_similarity_search(transformers, queries, similarity_threshold, search_results_limit, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><span style=\"color:red\">Experiment 3</span></strong> Queries from the tagged database with modification\n",
    "\n",
    "Here we use the databse of tagged sentences to define queries. The database is structured by countries. From a list of model documents the sentences were separated and tagged with a policy instrument label. The labels that were used are:\n",
    "\n",
    "* Credit\n",
    "* Direct payment\n",
    "* Fine\n",
    "* Guarantee\n",
    "* Supplies\n",
    "* Tax deduction\n",
    "* Technical assistance\n",
    "\n",
    "Not all countries have tagged sentences for each category so we ended up with 26 queries\n",
    "\n",
    "The difference between this experiment and experiment 2 is that here we have reformulated the query sentences by extracting the core incentive meaning from the original sentences, eliminating all the vocabulary not strictly speaking about incentives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict_exp3 = {\n",
    "    \"Otorgamiento de estímulos crediticios por parte de el estado\" : \"Credit-México\",\n",
    "\"Estos créditos podrían beneficiar a sistemas productivos asociados a la pequeña y mediana producción\" : \"Credit-Perú\",\n",
    "\"Se asocia con créditos de enlace del Banco del Estado\" : \"Credit-Chile\", \n",
    "\"Acceso al programa de garantía crediticia para la actividad económica\" : \"Credit-Guatemala\",\n",
    "\"El banco establecerá líneas de crédito para que el sistema financiero apoye la pequeña, mediana y microempresa\" : \"Credit-El Salvador\",\n",
    "\"Dentro de los incentivos económicos se podrá crear un bono para retribuir a los propietarios por los bienes y servicios generados.\" : \"Direct_payment-México\",\n",
    "\"Acceso a los fondos forestales para el pago de actividad\" : \"Direct_payment-Perú\",\n",
    "\"Se bonificará el 90% de los costos de repoblación para las primeras 15 hectáreas y de un 75% respecto las restantes\" : \"Direct_payment-Chile\",\n",
    "\"El estado dará un incentivo que se pagará una sola vez a los propietarios forestales\" : \"Direct_payment-Guatemala\",\n",
    "\"Incentivos en dinero para cubrir los costos directos e indirectos del establecimiento y manejo de areas de producción\" : \"Direct_payment-El Salvador\",\n",
    "\"Toda persona física o moral que cause daños estará obligada a repararlo o compensarlo\" : \"Fine-México\",\n",
    "\"Disminuir los riesgos para el inversionista implementando mecanismos de aseguramiento\" : \"Guarantee-México\",\n",
    "\"Podrá garantizarse el cumplimiento de la actividad mediante fianza otorgada a favor del estado por cualquiera de las afianzadoras legalmente autorizadas.\" : \"Guarantee-Guatemala\",\n",
    "\"El sujeto de derecho podrá recibir insumos para la instalación y operación de infraestructuras para la actividad económica.\" : \"Supplies-México\",\n",
    "\"Se facilitará el soporte técnico a  través de la utilización de guías, manuales, protocolos, paquetes tecnológicos, procedimientos, entre otros.\" : \"Supplies-Perú\",\n",
    "\"Se concederán incentivos en especie para fomentar la actividad en forma de insumos\" : \"Supplies-El Salvador\",\n",
    "\"Se otorgarán incentivos fiscales para la actividad primaria y también la actividad de transformación\" : \"Tax_deduction-México\",\n",
    "\"De acuerdo con los lineamientos aprobados se concederá un 25% de descuento en el pago del derecho de aprovechamiento\" : \"Tax_deduction-Perú\",\n",
    "\"Las bonificaciones percibidas o devengadas se considerarán como ingresos diferidos en el pasivo circulante y no se incluirán para el cálculo de la tasa adicional ni constituirán renta para ningún efecto legal hasta el momento en que se efectúe la explotación o venta\" : \"Tax_deduction-Chile\",\n",
    "\"Los contratistas que suscriban contratos de exploración y/o explotación, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades\" : \"Tax_deduction-Guatemala\",\n",
    "\"Exención de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestación de Servicios, en la importación de sus bienes, equipos y accesorios, maquinaria, vehículos, aeronaves o embarcaciones\" : \"Tax_deduction-El Salvador\",\n",
    "\"Se facilitará formación Permanente Además del acompañamiento técnico, los sujetos de derecho participarán en un proceso permanente de formación a lo largo de todo el año, que les permita enriquecer sus habilidades y capacidades \" : \"Technical_assistance-México\",\n",
    "\"Contribuir en la promoción para la gestión, a través de la capacitación, asesoramiento, asistencia técnica y educación de los usuarios\" : \"Technical_assistance-Perú\",\n",
    "\"Asesoría prestada al usuario por un operador acreditado, conducente a elaborar, acompañar y apoyar la adecuada ejecución técnica en terreno de aquellas prácticas comprometidas en el Plan de Manejo\" : \"Technical_assistance-Chile\",\n",
    "\"Para la ejecución de programas de capacitación, adiestramiento y otorgamiento de becas para la preparación de personal , así como para el desarrollo de tecnología en actividades directamente relacionadas con las operaciones objeto del contrato\" : \"Technical_assistance-Guatemala\",\n",
    "\"Apoyo técnico y en formulación de proyectos y conexión con mercados\" : \"Technical_assistance-El Salvador\"}\n",
    "\n",
    "queries = []\n",
    "for query in queries_dict_exp3:\n",
    "    queries.append(query)\n",
    "        \n",
    "# print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just to check the presence of misspelling in the values of the queries dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dictionary_values(queries_dict_exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "similarity_threshold = 0.2\n",
    "search_results_limit = 1000\n",
    "name = \"Exp3_tagged_201231\"\n",
    "\n",
    "results_dict = multiparameter_sentence_similarity_search(transformers, queries, similarity_threshold, search_results_limit, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><span style=\"color:red\">Experiment 4</span></strong> Queries from the tagged database with modification\n",
    "\n",
    "This would be the last version before extensive tagging. These data will be used for fine-tuning the model. It will be very similar to experiment3 but we are going to change some parameters:\n",
    "\n",
    "* We are going to work with both, the chilean database and the El Salvador database\n",
    "* We are going to retrieve the first 200 results for each query\n",
    "* We are going to balance the number of queries to have 5 queries for each policy instrument.\n",
    "\n",
    "There are some policy instruments that are underrepresented in some countries. For example, in the tagged sentences data set there are only two sentences tagged as fine and they are both from Mexico. What we are going to do is to manually find more sentences in official datasets in order to have at least 5 queries in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict_exp4 = {\n",
    "    \"Otorgamiento de estímulos crediticios por parte de el estado\" : \"Credit-México\",\n",
    "\"Estos créditos podrían beneficiar a sistemas productivos asociados a la pequeña y mediana producción\" : \"Credit-Perú\",\n",
    "\"Se asocia con créditos de enlace del Banco del Estado\" : \"Credit-Chile\", \n",
    "\"Acceso al programa de garantía crediticia para la actividad económica\" : \"Credit-Guatemala\",\n",
    "\"El banco establecerá líneas de crédito para que el sistema financiero apoye la pequeña, mediana y microempresa\" : \"Credit-El Salvador\",\n",
    "\"Dentro de los incentivos económicos se podrá crear un bono para retribuir a los propietarios por los bienes y servicios generados.\" : \"Direct_payment-México\",\n",
    "\"Acceso a los fondos forestales para el pago de actividad\" : \"Direct_payment-Perú\",\n",
    "\"Se bonificará el 90% de los costos de repoblación para las primeras 15 hectáreas y de un 75% respecto las restantes\" : \"Direct_payment-Chile\",\n",
    "\"El estado dará un incentivo que se pagará una sola vez a los propietarios forestales\" : \"Direct_payment-Guatemala\",\n",
    "\"Incentivos en dinero para cubrir los costos directos e indirectos del establecimiento y manejo de areas de producción\" : \"Direct_payment-El Salvador\",\n",
    "\"Toda persona física o moral que cause daños estará obligada a repararlo o compensarlo\" : \"Fine-México\",\n",
    "\"El incumplimiento de cualquiera de los requisitos establecidos en la presente se sanciona con la aplicación de la multa y la ejecución de la Medida Complementaria según lo establecido\" : \"Fine-Perú\",\n",
    "\"Incumplimiento grave de las obligaciones del concesionario, tales como el incumplimiento grave del Plan de Manejo, o el incumplimiento grave de las demás normas y regulaciones dictadas para la respectiva área por la Autoridad\" : \"Fine-Chile\",    \n",
    "\"Quedan prohibidas las actividades que pongan en peligro o dañen las Áreas de Bosques, Áreas Naturales y Zonas de Amortiguamiento principalmente la tala ilegal y quema\" : \"Fine-Guatemala\",    \n",
    "\"Tala indiscriminada de árboles para uso habitacional, industrial, comercial y servicios, en cualquier zona urbana por la omisión de la autorización otorgada $ 2,000.00 por cada árbol\"  : \"Fine-El Salvador\",\n",
    "\"El sujeto de derecho podrá recibir insumos para la instalación y operación de infraestructuras para la actividad económica.\" : \"Supplies-México\",\n",
    "\"Se facilitará el soporte técnico a  través de la utilización de guías, manuales, protocolos, paquetes tecnológicos, procedimientos, entre otros.\" : \"Supplies-Perú\",\n",
    "\"Se concederán incentivos en especie para fomentar la actividad en forma de insumos\" : \"Supplies-El Salvador\",\n",
    "\"Se otorgarán incentivos fiscales para la actividad primaria y también la actividad de transformación\" : \"Tax_deduction-México\",\n",
    "\"De acuerdo con los lineamientos aprobados se concederá un 25% de descuento en el pago del derecho de aprovechamiento\" : \"Tax_deduction-Perú\",\n",
    "\"Las bonificaciones percibidas o devengadas se considerarán como ingresos diferidos en el pasivo circulante y no se incluirán para el cálculo de la tasa adicional ni constituirán renta para ningún efecto legal hasta el momento en que se efectúe la explotación o venta\" : \"Tax_deduction-Chile\",\n",
    "\"Los contratistas que suscriban contratos de exploración y/o explotación, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades\" : \"Tax_deduction-Guatemala\",\n",
    "\"Exención de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestación de Servicios, en la importación de sus bienes, equipos y accesorios, maquinaria, vehículos, aeronaves o embarcaciones\" : \"Tax_deduction-El Salvador\",\n",
    "\"Se facilitará formación Permanente Además del acompañamiento técnico, los sujetos de derecho participarán en un proceso permanente de formación a lo largo de todo el año, que les permita enriquecer sus habilidades y capacidades \" : \"Technical_assistance-México\",\n",
    "\"Contribuir en la promoción para la gestión, a través de la capacitación, asesoramiento, asistencia técnica y educación de los usuarios\" : \"Technical_assistance-Perú\",\n",
    "\"el beneficiario deberá presentar factura o boleta de honorarios que certifique el pago al operador por los servicios prestados en la confección y presentación del plan de manejo, según lo declarado como costo de asistencia técnica\" : \"Technical_assistance-Chile\",\n",
    "\"Asesoría prestada al usuario por un operador acreditado, conducente a elaborar, acompañar y apoyar la adecuada ejecución técnica en terreno de aquellas prácticas comprometidas en el Plan de Manejo\" : \"Technical_assistance-Chile\",\n",
    "\"Para la ejecución de programas de capacitación, adiestramiento y otorgamiento de becas para la preparación de personal , así como para el desarrollo de tecnología en actividades directamente relacionadas con las operaciones objeto del contrato\" : \"Technical_assistance-Guatemala\",\n",
    "\"Apoyo técnico y en formulación de proyectos y conexión con mercados\" : \"Technical_assistance-El Salvador\"}\n",
    "\n",
    "queries = []\n",
    "for query in queries_dict_exp4:\n",
    "    queries.append(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just to check the presence of misspelling in the values of the queries dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dictionary_values(queries_dict_exp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Otorgamiento de estímulos crediticios por parte de el estado it's been done in 6.5220 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Estos créditos podrían beneficiar a sistemas productivos asociados a la pequeña y mediana producción it's been done in 6.5349 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Se asocia con créditos de enlace del Banco del Estado it's been done in 6.3269 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Acceso al programa de garantía crediticia para la actividad económica it's been done in 7.4952 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query El banco establecerá líneas de crédito para que el sistema financiero apoye la pequeña, mediana y microempresa it's been done in 6.4589 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Dentro de los incentivos económicos se podrá crear un bono para retribuir a los propietarios por los bienes y servicios generados. it's been done in 6.3019 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Acceso a los fondos forestales para el pago de actividad it's been done in 6.2574 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Se bonificará el 90% de los costos de repoblación para las primeras 15 hectáreas y de un 75% respecto las restantes it's been done in 7.4622 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query El estado dará un incentivo que se pagará una sola vez a los propietarios forestales it's been done in 6.2323 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Incentivos en dinero para cubrir los costos directos e indirectos del establecimiento y manejo de areas de producción it's been done in 6.3610 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Toda persona física o moral que cause daños estará obligada a repararlo o compensarlo it's been done in 7.3487 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query El incumplimiento de cualquiera de los requisitos establecidos en la presente se sanciona con la aplicación de la multa y la ejecución de la Medida Complementaria según lo establecido it's been done in 6.3830 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Incumplimiento grave de las obligaciones del concesionario, tales como el incumplimiento grave del Plan de Manejo, o el incumplimiento grave de las demás normas y regulaciones dictadas para la respectiva área por la Autoridad it's been done in 6.3167 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Quedan prohibidas las actividades que pongan en peligro o dañen las Áreas de Bosques, Áreas Naturales y Zonas de Amortiguamiento principalmente la tala ilegal y quema it's been done in 6.2647 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Tala indiscriminada de árboles para uso habitacional, industrial, comercial y servicios, en cualquier zona urbana por la omisión de la autorización otorgada $ 2,000.00 por cada árbol it's been done in 7.3429 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query El sujeto de derecho podrá recibir insumos para la instalación y operación de infraestructuras para la actividad económica. it's been done in 6.4914 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Se facilitará el soporte técnico a  través de la utilización de guías, manuales, protocolos, paquetes tecnológicos, procedimientos, entre otros. it's been done in 6.3363 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Se concederán incentivos en especie para fomentar la actividad en forma de insumos it's been done in 7.3885 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Se otorgarán incentivos fiscales para la actividad primaria y también la actividad de transformación it's been done in 6.4196 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query De acuerdo con los lineamientos aprobados se concederá un 25% de descuento en el pago del derecho de aprovechamiento it's been done in 6.3126 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Las bonificaciones percibidas o devengadas se considerarán como ingresos diferidos en el pasivo circulante y no se incluirán para el cálculo de la tasa adicional ni constituirán renta para ningún efecto legal hasta el momento en que se efectúe la explotación o venta it's been done in 6.3274 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Los contratistas que suscriban contratos de exploración y/o explotación, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades it's been done in 7.5157 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Exención de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestación de Servicios, en la importación de sus bienes, equipos y accesorios, maquinaria, vehículos, aeronaves o embarcaciones it's been done in 6.3394 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Se facilitará formación Permanente Además del acompañamiento técnico, los sujetos de derecho participarán en un proceso permanente de formación a lo largo de todo el año, que les permita enriquecer sus habilidades y capacidades  it's been done in 6.2316 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Contribuir en la promoción para la gestión, a través de la capacitación, asesoramiento, asistencia técnica y educación de los usuarios it's been done in 7.5524 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query el beneficiario deberá presentar factura o boleta de honorarios que certifique el pago al operador por los servicios prestados en la confección y presentación del plan de manejo, según lo declarado como costo de asistencia técnica it's been done in 6.2641 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Asesoría prestada al usuario por un operador acreditado, conducente a elaborar, acompañar y apoyar la adecuada ejecución técnica en terreno de aquellas prácticas comprometidas en el Plan de Manejo it's been done in 6.5003 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Para la ejecución de programas de capacitación, adiestramiento y otorgamiento de becas para la preparación de personal , así como para el desarrollo de tecnología en actividades directamente relacionadas con las operaciones objeto del contrato it's been done in 7.4680 seconds\n",
      "similarity search for model xlm-r-bert-base-nli-stsb-mean-tokens and query Apoyo técnico y en formulación de proyectos y conexión con mercados it's been done in 6.2779 seconds\n"
     ]
    }
   ],
   "source": [
    "transformers =['xlm-r-bert-base-nli-stsb-mean-tokens']#, 'distiluse-base-multilingual-cased-v2']\n",
    "similarity_threshold = 0.2\n",
    "search_results_limit = 200\n",
    "name = \"Exp4_tagged_200105\"\n",
    "\n",
    "results_dict = multiparameter_sentence_similarity_search(transformers, queries, similarity_threshold, search_results_limit, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a temporary section to explore how to analyze the results. It is organized with the same structure as the section <strong>Defining queries</strong> as we are exploring the best search strategies based on different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-speach approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the results\n",
    "\n",
    "# path = \"../output/\"\n",
    "# filename = \"Experiment_201215_jordi_1500.json\"\n",
    "# file = path + filename\n",
    "\n",
    "# with open(file, \"r\") as f:\n",
    "#     experiment_results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the results and refactor data structures to better process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = results\n",
    "\n",
    "# Building a final dictionari of the results with a extra layer with sentence IDs as keys of the last layer\n",
    "experiment_results_full_dict = {}\n",
    "for model in experiment_results:\n",
    "    experiment_results_full_dict[model] = {}\n",
    "    i = 0\n",
    "    for keyword in experiment_results[model]:\n",
    "        if i % len(experiment_results[model]) == 0:\n",
    "            key = keyword + \"_K\"\n",
    "            experiment_results_full_dict[model][key] = {}\n",
    "            for result in experiment_results[model][keyword]:\n",
    "                experiment_results_full_dict[model][key][result[0]] = result[1:len(result)]\n",
    "        else:\n",
    "            key = key[0:-2] + \"_S\"\n",
    "            experiment_results_full_dict[model][key] = {}\n",
    "            for result in experiment_results[model][keyword]:\n",
    "                experiment_results_full_dict[model][key][result[0]] = result[1:len(result)]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_names =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "\n",
    "\n",
    "for incentive, sentence_list in keyword_hits.items():\n",
    "#     print(\"\\t\", incentive.center(25)\n",
    "    for sentence_id in sentence_list:\n",
    "        for model_name in transformer_names:\n",
    "            for key in experiment_results_full_dict[model_name]:\n",
    "                if incentive in key:\n",
    "                    if sentence_id in experiment_results_full_dict[model_name][key]:\n",
    "                        keyword_hits[incentive][sentence_id].append(experiment_results_full_dict[model_name][key][sentence_id][2])\n",
    "                        keyword_hits[incentive][sentence_id].append(round(experiment_results_full_dict[model_name][key][sentence_id][0], 2))\n",
    "                    else:\n",
    "                        keyword_hits[incentive][sentence_id].append(15000)\n",
    "                        keyword_hits[incentive][sentence_id].append(0.0)\n",
    "        i += 1\n",
    "#         for keyword in \n",
    "#             print(experiment_results_full_dict[model_name].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv = []\n",
    "for key, value in keyword_hits.items():\n",
    "    for sentence, res in value.items():\n",
    "        results_csv.append([key, sentence, res[0], res[1], res[2], res[3], res[4], res[5], res[6], res[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"keyword\", \"sentence_ID\", \"xlm_K-rank\", \"xlm_K-sim\", \"xlm_S-rank\", \"xlm_S-sim\", \"dist_K-rank\", \"dist_K-sim\", \"dist_S-rank\", \"dist_S-sim\"]\n",
    "df= pd.DataFrame(results_csv, columns = column_names)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../output/\"\n",
    "filename = \"Experiment_201217_jordi_1500.csv\"\n",
    "file = path + filename\n",
    "\n",
    "df.to_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged sentence approach\n",
    "\n",
    "Below, we define the functions that are going to be used in the post-processing and in the analysis of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the contents of the results dict, particularly, the length of the first element and its contents\n",
    "def show_results(results_dictionary):\n",
    "    i = 0\n",
    "    for key1 in results_dictionary:\n",
    "        for key2 in results_dictionary[key1]:\n",
    "            if i == 0:\n",
    "                print(len(results_dictionary[key1][key2]))\n",
    "                print(results_dictionary[key1][key2])\n",
    "            i += 1\n",
    "\n",
    "# Adding the rank to each result\n",
    "def add_rank(results_dictionary):\n",
    "#     for model in results_dictionary:\n",
    "    for keyword in results_dictionary:#[model]:\n",
    "        i = 1\n",
    "        for result in results_dictionary[keyword]:#[model][keyword]:\n",
    "            result.insert(1, i)\n",
    "            i += 1\n",
    "    return results_dictionary\n",
    "\n",
    "# For experiments 2 and 3 this function is to save results in separate csv files\n",
    "def save_results_as_separate_csv(results_dictionary, queries_dictionary, experiment_number, date):\n",
    "    name = \"Exp\" + experiment_number\n",
    "    path = \"../output/\" + name + \"/\" + date + \"/\"\n",
    "#     for model, value in results_dictionary.items():\n",
    "    name1 = name + \"_\" + \"Mxlm_\"\n",
    "    for exp_title, result in results_dictionary.items():#value.items():\n",
    "        filename = name1 + queries_dictionary[exp_title]\n",
    "        file = path + filename + \".tsv\"\n",
    "        with open(file, 'w', newline='', encoding='utf-8') as f:\n",
    "            write = csv.writer(f, delimiter='\\t')\n",
    "            write.writerows(result)\n",
    "#             print(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the analysis are saved as a json file. To further process the information we can upload the file contents into a dictionary.\n",
    "\n",
    "After loading the results, a rank value is added to the results from the highest similarity score to the lower one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json where there are the results that you want to analyze. CHANGE the file name accordingly.\n",
    "path = \"../output/\"\n",
    "filename = \"Exp4_tagged_210105.json\"\n",
    "file = path + filename\n",
    "with open(file, \"r\") as f:\n",
    "    results_ = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the rank in the results dictionary\n",
    "results = copy.deepcopy(add_rank(results_dict))\n",
    "# show_results(results_E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to simplify the analysis process and to make it available for a broader spectrum of analysts, the results are split into small \"tsv\" documents that can be easily imported in spreadsheets.\n",
    "\n",
    "The new files will contain only the results of a single query, this is it will contain all the 100 (or whatever number has been retrieved) sentences from the database which have the highest similarity score with the query. There will be the following columns:\n",
    "\n",
    "* Sentence ID\n",
    "* Rank of the sentence in the similarity results\n",
    "* Similarity score\n",
    "* Text of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results as separete csv files\n",
    "queries_dict = queries_dict_exp4 # CHANGE the queries dict accordingly!\n",
    "Experiment_number = \"4\" # CHANGE the experiment number accordingly!\n",
    "Date = \"210105\" # CHANGE the date accordingly!\n",
    "save_results_as_separate_csv(results, queries_dict, Experiment_number, Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell there is the code to retrieve the results that were saved in the previous cell for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder = \"Exp4/210105/\" # CHANGE the subfolder name accordingly!\n",
    "# subfolder = \"Exp3/201231/\" # CHANGE the subfolder name accordingly!\n",
    "paths = Path(\"../output/\" + subfolder).glob('**/*.tsv')\n",
    "transformers = [\"Mxlm\", \"Mdistiluse\"]\n",
    "policy_instruments = [\"Credit\", \"Direct_payment\", \"Fine\", \"Guarantee\", \"Supplies\", \"Tax_deduction\", \"Technical_assistance\"]\n",
    "countries = [\"Chile\", \"El Salvador\", \"Guatemala\", \"México\", \"Perú\"]\n",
    "transformer = \"Mxlm\"\n",
    "policy_instrument = \"Technical_assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\output\\Exp4\\210105\\Exp4_Mxlm_Technical_assistance-Chile.tsv\n",
      "..\\output\\Exp4\\210105\\Exp4_Mxlm_Technical_assistance-El Salvador.tsv\n",
      "..\\output\\Exp4\\210105\\Exp4_Mxlm_Technical_assistance-Guatemala.tsv\n",
      "..\\output\\Exp4\\210105\\Exp4_Mxlm_Technical_assistance-México.tsv\n",
      "..\\output\\Exp4\\210105\\Exp4_Mxlm_Technical_assistance-Perú.tsv\n"
     ]
    }
   ],
   "source": [
    "sentences_dict = {}\n",
    "unique_ids = {}\n",
    "\n",
    "for path in paths:\n",
    "    # because path is object not string\n",
    "    path_in_str = str(path)\n",
    "    if transformer in path_in_str:\n",
    "        if policy_instrument in path_in_str:\n",
    "            for country in countries:\n",
    "                if country in path_in_str:\n",
    "                    sentences_dict[country] = {}\n",
    "                    print(path_in_str)\n",
    "                    with open(path_in_str, \"r\", encoding = \"utf-8\") as f:\n",
    "                        file = csv.reader(f, delimiter='\\t')\n",
    "                        for row in file:\n",
    "                            sentences_dict[country][row[0]] = row\n",
    "                            unique_ids[row[0]] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Unique_sentence_IDs_\" + policy_instrument + \".tsv\"\n",
    "path = \"../output/Exp4/210105/\"\n",
    "file = path + name\n",
    "with open(file, 'w', newline = '', encoding = 'utf-8') as f:\n",
    "    write = csv.writer(f, delimiter='\\t')\n",
    "    for key, value in unique_ids.items():\n",
    "        write.writerow(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences_dict))\n",
    "print(len(sentences_dict[country]))\n",
    "print(len(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {}\n",
    "counts = 0\n",
    "i = 0\n",
    "for country in countries:\n",
    "    i += 1\n",
    "    j = 0\n",
    "    for ref_country in countries:\n",
    "        j += 1\n",
    "#         if j > i:\n",
    "        print(ref_country, \"---\", country)\n",
    "        for sentence in sentences_dict[country]:\n",
    "            if sentence in sentences_dict[ref_country]:\n",
    "                if sentence in sentences:\n",
    "                    sentences[sentence] = sentences[sentence] + 1\n",
    "                else:\n",
    "                    sentences[sentence] = 1\n",
    "#                     print(\"hit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)\n",
    "print(len(sentences))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_instrument = \"Direct_payment\"\n",
    "path = Path(\"../output/\")\n",
    "subfolder = Path(\"Exp3/201228/\" )# CHANGE the subfolder name accordingly!\n",
    "filename = \"Unique_Ids_Tagged_\" + policy_instrument + \".xlsx\"\n",
    "file = path / subfolder / filename\n",
    "df = pd.read_excel(file)\n",
    "tagged = df.values.tolist()\n",
    "tagged_dict = {}\n",
    "for item in tagged:\n",
    "    tagged_dict[item[0]] = [item[4], item[5], item[6]]\n",
    "\n",
    "for country in countries:\n",
    "    updated_file = []\n",
    "    filename = \"Exp3_Mxlm_\" + policy_instrument + \"-\" + country + \".tsv\"\n",
    "    file = path / subfolder / filename\n",
    "    with open(file, \"r\", encoding = \"utf-8\") as f:\n",
    "        file = csv.reader(f, delimiter='\\t')\n",
    "        for row in file:\n",
    "            updated_file.append([row[0], row[1], row[2], row[3], tagged_dict[row[0]][0], tagged_dict[row[0]][1], tagged_dict[row[0]][2]])\n",
    "    filename = \"Exp3_Mxlm_\" + policy_instrument + \"-\" + country + \"_tagged.tsv\"\n",
    "    file = path / subfolder / filename\n",
    "    with open(file, 'w', newline = '', encoding = 'utf-8') as f:\n",
    "        write = csv.writer(f, delimiter='\\t')\n",
    "        write.writerows(updated_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../output/\"\n",
    "filename = \"Exp2_tagged_201228.json\"\n",
    "file = path + filename\n",
    "with open(file, \"r\") as f:\n",
    "    results_Exp2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key1 in results_Exp2:\n",
    "    print(key1)\n",
    "    for key2 in results_Exp2[key1]:\n",
    "        print(queries_dict_exp2[key2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../output/\"\n",
    "filename = \"Exp3_tagged_201228.json\"\n",
    "file = path + filename\n",
    "with open(file, \"r\") as f:\n",
    "    results_Exp3 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the documents of selected sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD:old_folders_and_files/basic_model_testing.ipynb
   "version": "3.9.2"
=======
   "version": "3.8.8"
>>>>>>> master:tasks/data_augmentation/notebooks/Semi-Automatic Labeling/basic_model_testing.ipynb
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "757px",
    "left": "586px",
    "top": "630px",
    "width": "309px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
