{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a search engine based on sBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is a basic implementation of sBERT for searching a database of sentences with queries.\n",
    "\n",
    "The goal is to increase the amount of labeled data that we have in order to later fine tune a model to be used for sentence classification. First of all we have to find a pool of queries that represent the six labels of the six policy instruments. With these queries we can pull a set of sentences that can be automaticaly labeled with the same label of the query. In this way we can increase the diversity of labeled sentences in each label category. This approach will be complemented with a manual curation step to produce a high quality training data set.\n",
    "\n",
    "The policy instruments that we want to find and that correspond to the different labels are:\n",
    "* Direct payment (PES)\n",
    "* Tax deduction\n",
    "* Credit/guarantee\n",
    "* Technical assistance\n",
    "* Supplies\n",
    "* Fines\n",
    "\n",
    "This notebook is intended for the following purposes:\n",
    "* Try different query strategies to find the optimal retrieval of sentences in each policy instrument category\n",
    "* Try different transformers\n",
    "* Be the starting point for further enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is self contained, it does not depend on any other class of the sBERT folder.\n",
    "\n",
    "You just have to create an environment where you install the external dependencies. Usually the dependencies that you have to install are:\n",
    "\n",
    "**For the basic sentence similarity calculation**\n",
    "*  pandas\n",
    "*  boto3\n",
    "*  sentence_transformers\n",
    "\n",
    "**If you want to use ngrams to generate queries**\n",
    "*  nltk\n",
    "*  plotly\n",
    "*  wordcloud\n",
    "\n",
    "**If you want to do evaluation and ploting with pyplot**\n",
    "*  matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment is called nlp then you execute this cell otherwise you change the name of the environment\n",
    "!conda activate nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Model libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Libraries for model evaluation\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Libraries to be used in the process of definig queries\n",
    "import nltk # imports the natural language toolkit\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we use the excel file with the selected phrases of each country, process them and get N-grams to define basic queries for the SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'WRI_Policy_Tags (1).xlsx', sheet_name = None)\n",
    "df = None\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(df,pd.DataFrame):\n",
    "            df = value\n",
    "        else:\n",
    "            df = df.append(value)\n",
    "else:\n",
    "    df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[\"relevant sentences\"].apply(lambda x: x.split(\";\") if isinstance(x,str) else x)\n",
    "sentence = []\n",
    "\n",
    "for elem in sentences:\n",
    "    if isinstance(elem,float) or len(elem) == 0:\n",
    "        continue\n",
    "    elif isinstance(elem,list):\n",
    "        for i in elem:\n",
    "            if len(i.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sentence.append(i.strip())\n",
    "    else:\n",
    "        if len(elem.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            sentence.append(elem.strip())\n",
    "\n",
    "sentence\n",
    "words_per_sentence = [len(x.split(\" \")) for x in sentence]\n",
    "plt.hist(words_per_sentence, bins = 50)\n",
    "plt.title(\"Histogram of number of words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "\n",
    "noise_words = []\n",
    "stopwords_corpus = nltk.corpus.stopwords\n",
    "sp_stop_words = stopwords_corpus.words('spanish')\n",
    "noise_words.extend(sp_stop_words)\n",
    "print(len(noise_words))\n",
    "\n",
    "if \"no\" in noise_words:\n",
    "    noise_words.remove(\"no\")\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(''.join(sentence))\n",
    "word_freq = Counter(tokenized_words)\n",
    "# word_freq.most_common(20)\n",
    "# list(ngrams(tokenized_words, 3))\n",
    "\n",
    "word_tokens_clean = [re.findall(r\"[a-zA-Z]+\",each) for each in tokenized_words if each.lower() not in noise_words and len(each.lower()) > 1]\n",
    "word_tokens_clean = [each[0].lower() for each in word_tokens_clean if len(each)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams size\n",
    "We define the size of the n-gram that we want to find. The larger it is, the less frequent it will be, unless we substantially increase the number of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 2\n",
    "\n",
    "top_k_ngrams(word_tokens_clean, n_grams, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database from El Salvador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'JSON/ElSalvador.json'\n",
    "\n",
    "obj = s3.Object('wri-latin-talent',filename)\n",
    "serializedObject = obj.get()['Body'].read()\n",
    "policy_list = json.loads(serializedObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "                    \"POR TANTO\" : 0,\n",
    "                    \"DISPOSICIONES GENERALES\" : 0,\n",
    "                    \"OBJETO\" : 0,\n",
    "                    \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "sentences = {}\n",
    "for key, value in policy_list.items():\n",
    "    for item in value:\n",
    "        if item in is_not_incentive:\n",
    "            continue\n",
    "        else:\n",
    "            for sentence in policy_list[key][item]['sentences']:\n",
    "                sentences[sentence] = policy_list[key][item]['sentences'][sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "Then we build a simple function that takes four inputs:\n",
    "1. The model as we have set it in the previous line of code\n",
    "2. A dictionary that contains the sentences {\"\\<sentence_ID\\>\" : {\"text\" : \"The actual sentence\", labels : []}\n",
    "3. A query in the form of a string\n",
    "4. A similarity treshold. It is a float that we can use to limit the results list to the most relevant.\n",
    "\n",
    "The output of the function is a list with three columns with the following content:\n",
    "1. Column 1 contains the id of the sentence\n",
    "2. Column 2 contains the similarity score\n",
    "3. Column 3 contains the text of the sentence that has been compared with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_name='xlm-r-100langs-bert-base-nli-mean-tokens'\n",
    "transformer_name = \"distiluse-base-multilingual-cased\"\n",
    "model = SentenceTransformer(transformer_name)\n",
    "\n",
    "def highlight(model, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query)\n",
    "    highlights = []\n",
    "    for sentence in sentences_dict:\n",
    "        sentence_embedding = model.encode(sentence)\n",
    "        score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "        if score > similarity_treshold:\n",
    "            highlights.append([sentence, score, document[sentence]['text']])\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "highlighter_query = \"Todos aquellos agricultores que fueron beneficiados con el otorgamiento de hasta dos crÃ©ditos\"\n",
    "highlighter_precision = 0.2\n",
    "\n",
    "label_1 = highlight(model, sentences, highlighter_query, highlighter_precision)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"similarity search for El Salvador sentences done in {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_1))\n",
    "label_1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further filtering of the results by using the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_treshold = 0.5\n",
    "filtered = [row for row in label_1 if row[1] > similarity_treshold]\n",
    "filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
