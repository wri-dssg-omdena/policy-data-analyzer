{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish preprocessor/sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/propietari/Documents/GitHub/policy-data-analyzer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.text_preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup amazon client\n",
    "language = \"spanish\"\n",
    "bucket_name = \"wri-nlp-policy\"\n",
    "creds_filepath = \"/Users/dafirebanks/Documents/credentials.json\"\n",
    "creds_filepath = \"/home/propietari/Documents/claus/AWS_S3_keys_wri_sentence_splitting.json\"\n",
    "\n",
    "s3_client = S3Client(creds_filepath=creds_filepath, bucket_name=bucket_name, language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"dict\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-58afbe5e11e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2. Make sure we're getting the right files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms3_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_text_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File_id:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Text:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/policy-data-analyzer/tasks/data_loading/src/s3_client.py\u001b[0m in \u001b[0;36mload_text_files\u001b[0;34m(self, language)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_folder_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_text_files_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_text_files_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/boto3/resources/collection.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/boto3/resources/collection.py\u001b[0m in \u001b[0;36mpages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# we start processing and yielding individual items.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mpage_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inject_starting_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_request\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, current_kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             http, parsed_response = self._make_request(\n\u001b[0m\u001b[1;32m    663\u001b[0m                 operation_model, request_dict, request_context)\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m         logger.debug(\"Making request for %s with params: %s\",\n\u001b[1;32m    101\u001b[0m                      operation_model, request_dict)\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mattempts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         success_response, exception = self._get_response(\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mcreate_request\u001b[0;34m(self, params, operation_model)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mservice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 op_name=operation_model.name)\n\u001b[0;32m--> 115\u001b[0;31m             self._event_emitter.emit(event_name, request=request,\n\u001b[0m\u001b[1;32m    116\u001b[0m                                      operation_name=operation_model.name)\n\u001b[1;32m    117\u001b[0m         \u001b[0mprepared_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/signers.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# this method is invoked to sign the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Don't call this method directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     def sign(self, operation_name, request, region_name=None,\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/signers.py\u001b[0m in \u001b[0;36msign\u001b[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_choose_signer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigning_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/auth.py\u001b[0m in \u001b[0;36madd_auth\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mstring_to_sign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_to_sign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonical_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'StringToSign:\\n%s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_to_sign\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_to_sign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Signature:\\n%s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textPreprocessing/lib/python3.8/site-packages/botocore/auth.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(self, string_to_sign, request)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_to_sign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecret_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         k_date = self._sign(('AWS4' + key).encode('utf-8'),\n\u001b[0m\u001b[1;32m    349\u001b[0m                             request.context['timestamp'][0:8])\n\u001b[1;32m    350\u001b[0m         \u001b[0mk_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"dict\") to str"
     ]
    }
   ],
   "source": [
    "# 2. Make sure we're getting the right files\n",
    "i = 0\n",
    "for file_id, text in s3_client.load_text_files(language):\n",
    "    print(\"File_id:\", file_id)\n",
    "    print(\"Text:\", text[:100])\n",
    "    print(\"=======================================\")\n",
    "    i += 1\n",
    "    if i == 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Start preprocessing\n",
    "tokenizer = nltk.data.load(f\"tokenizers/punkt/{language}.pickle\")\n",
    "abbrevs = {\"ord\", \"num\", \"sra\", \"no\", \"corp\", \"art\", \"ltda\", \"ud\"}\n",
    "min_num_words = 5\n",
    "\n",
    "new_text_files_folder = f\"{language}_documents/text_files/new\"\n",
    "processed_text_files_folder = f\"{language}_documents/text_files/processed\"\n",
    "\n",
    "i = 0\n",
    "print_every = 100\n",
    "error_files = []\n",
    "\n",
    "for file_id, text in s3_client.load_text_files(language):\n",
    "    try:\n",
    "        file_id = file_id.replace(\"/\", \"\")\n",
    "        preprocessed_text = preprocess_spanish_text(text)\n",
    "        sents = get_nltk_sents(preprocessed_text, tokenizer, abbrevs)\n",
    "        postprocessed_sents = format_sents_for_output(remove_short_sents(sents, min_num_words), file_id)\n",
    "        s3_client.store_sentences(postprocessed_sents, file_id, language)\n",
    "        s3_client.move_object(file_id + \".txt\", new_text_files_folder, processed_text_files_folder)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_files.append({file_id: e})\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    # For testing and early stopping, uncomment this\n",
    "#     if i == 2:\n",
    "#         break\n",
    "\n",
    "    if i % print_every == 0:\n",
    "        print(\"----------------------------------------------\")\n",
    "        print(f\"Processing {i} documents...\")\n",
    "        print(f\"Number of errors so far: {len(error_files)}\")\n",
    "        print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to store the errors\n",
    "with open(f\"../output/{language}_sentence_splitting_errors.json\", \"w\") as f:\n",
    "    json.dump(error_files, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code\n",
    "\n",
    "From this section until the end, it's mainly experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import nltk.data\n",
    "import spacy \n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    return re.sub(re.compile('<.*?>'), '', text)\n",
    "\n",
    "def replace_links(text):\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "    return re.sub(r'www\\S+', '[URL]', text)\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def parse_emails(text):\n",
    "    \"\"\" \n",
    "    Remove the periods from emails in text, except the last one\n",
    "    \"\"\"\n",
    "    emails = [email if email[-1] != \".\" else email[:-1] for email in re.findall(r\"\\S*@\\S*\\s?\", text)]\n",
    "    \n",
    "    for email in emails:\n",
    "        new_email = email.replace(\".\", \"\")\n",
    "        text = text.replace(email, new_email)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def parse_acronyms(text):\n",
    "    \"\"\" \n",
    "    Remove the periods from acronyms in the text (i.e \"U.S.\" becomes \"US\") \n",
    "    \"\"\"\n",
    "\n",
    "    acronyms = re.findall(r\"\\b(?:[a-zA-Z]\\.){2,}\", text)\n",
    "         \n",
    "    for acronym in acronyms:\n",
    "        new_acronym = acronym.replace(\".\", \"\")\n",
    "        text = text.replace(acronym, new_acronym)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def spanish_preprocessing(txt, remove_new_lines=False):\n",
    "    \"\"\"\n",
    "    Steps in the preprocessing of text:\n",
    "        1. Remove HTML tags\n",
    "        2. Replace URLS by a tag [URL]\n",
    "        3. Replace new lines and tabs by normal spaces - sometimes sentences have new lines in the middle\n",
    "        4. Remove excessive spaces (more than 1 occurrence)\n",
    "        5. Parse abreviations and acronyms\n",
    "    \"\"\"\n",
    "    txt = replace_links(remove_html_tags(txt)).strip()\n",
    "    if remove_new_lines:\n",
    "        txt = txt.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "    txt = remove_multiple_spaces(txt)\n",
    "    txt = parse_emails(txt)\n",
    "    txt = parse_acronyms(txt)\n",
    "    \n",
    "    new_txt = \"\"\n",
    "    all_period_idx = set([indices.start() for indices in re.finditer(\"\\.\", txt)])\n",
    "    \n",
    "    for i, char in enumerate(txt):\n",
    "        if i in all_period_idx:\n",
    "            # Any char following a period that is NOT a space means that we should not add that period\n",
    "            if i + 1 < len(txt) and txt[i + 1] != \" \":\n",
    "                continue\n",
    "            \n",
    "            # Any char that is a number following a period will not count. \n",
    "            # For enumerations, we're counting on docs being enumerated as \"(a)\" or \"(ii)\", and if not, they will be separated by the . after the number (\"3. Something\" will just be \"Something\" as a sentence)\n",
    "            if i + 2 < len(txt) and txt[i + 2].isnumeric(): \n",
    "                continue\n",
    "            \n",
    "            # If we wanted to have all numbered lists together, uncomment this, and comment out the previous condition\n",
    "#             if i + 2 < len(txt) and not txt[i + 2].isalpha(): \n",
    "#                 continue\n",
    "            \n",
    "        new_txt += char\n",
    "\n",
    "    return unidecode.unidecode(new_txt)\n",
    "\n",
    "def get_nltk_sents(txt, tokenizer, extra_abbreviations=None):\n",
    "    if extra_abbreviations:\n",
    "        tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "        \n",
    "    sents = tokenizer.tokenize(txt)\n",
    "    return sents\n",
    "\n",
    "def spanish_postprocessing(sents, min_num_words=4):\n",
    "    \"\"\"\n",
    "    Remove sentences that are made of less than a given number of words. Default is 4\n",
    "    \"\"\"\n",
    "    \n",
    "    return [sent for sent in sents if len(sent.split()) >= min_num_words]\n",
    "\n",
    "def format_sents_for_output(sents, doc_id):\n",
    "    formatted_sents = {}\n",
    "\n",
    "    for i, sent in enumerate(sents):\n",
    "        formatted_sents.update({f\"{doc_id}_sent_{i}\": {\"text\": sent, \"label\": []}})\n",
    "\n",
    "    return formatted_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "USC_re = re.compile('[Uu]\\.*[Ss]\\.*[Cc]\\.]+')\n",
    "PAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\n",
    "BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_'), re.UNICODE)\n",
    "BULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\n",
    "DASH_RE = re.compile('--+')\n",
    "WHITESPACE_RE = re.compile('\\s+')\n",
    "EMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\n",
    "FIX_START_RE = re.compile('^[^A-Za-z]*')\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Borrowed from the FNDS text processing with additional logic added in.\n",
    "    Note: we do not take care of token breaking - assume SPACY's tokenizer\n",
    "    will handle this for us.\n",
    "    \"\"\"\n",
    "\n",
    "    # Indicate section headers, we need them for features\n",
    "    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)\n",
    "    # For simplicity later, remove '.' from most common acronym\n",
    "    text = text.replace(\"U.S.\", \"US\")\n",
    "    text = text.replace('SEC.', 'Section')\n",
    "    text = text.replace('Sec.', 'Section')\n",
    "    text = USC_re.sub('USC', text)\n",
    "\n",
    "    # Remove parantheticals because they are almost always references to laws \n",
    "    # We could add a special tag, but we just remove for now\n",
    "    # Note we dont get rid of nested parens because that is a complex re\n",
    "    #text = PAREN_re.sub('LAWREF', text)\n",
    "    text = PAREN_re.sub('', text)\n",
    "    \n",
    "\n",
    "    # Get rid of enums as bullets or ` as bullets\n",
    "    text = BULLET_RE.sub(' ',text)\n",
    "    \n",
    "    # Clean html \n",
    "    text = text.replace('&lt;all&gt;', '')\n",
    "\n",
    "    # Remove annoying punctuation, that's not relevant\n",
    "    text = BAD_PUNCT_RE.sub('', text)\n",
    "\n",
    "    # Get rid of long sequences of dashes - these are formating\n",
    "    text = DASH_RE.sub( ' ', text)\n",
    "\n",
    "    # removing newlines, tabs, and extra spaces.\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # If we ended up with \"empty\" sentences - get rid of them.\n",
    "    text = EMPTY_SENT_RE.sub('.', text)\n",
    "    \n",
    "    # Attempt to create sentences from bullets \n",
    "#    text = replace_semicolon(text)\n",
    "    \n",
    "    # Fix weird period issues + start of text weirdness\n",
    "    #text = re.sub('\\.(?=[A-Z])', '  . ', text)\n",
    "    # Get rid of anything thats not a word from the start of the text\n",
    "    text = FIX_START_RE.sub( '', text)\n",
    "    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   \n",
    "    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n",
    "\n",
    "    # Fix quotes\n",
    "    text = text.replace('``', '\"')\n",
    "    text = text.replace('\\'\\'', '\"')\n",
    "\n",
    "    # Add special punct back in\n",
    "    text = text.replace('SECTION-HEADER', '<SECTION-HEADER>')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../input/Mexico/\"\n",
    "chile_paths = [\"Chile1.txt\", \"Chile2.txt\", \"Chile3.txt\"]\n",
    "elsalvador_paths = [\"ElSalvador1.txt\", \"ElSalvador2.txt\", \"ElSalvador3.txt\"]\n",
    "mexico_paths = [\"Mexico1.txt\", \"Mexico2.txt\", \"Mexico3.txt\", \"Mexico4.txt\", \"Mexico5.txt\", \"Mexico6.txt\"]\n",
    "fname = mexico_paths[1]\n",
    "txt_path = base_path + fname\n",
    "\n",
    "with open(txt_path, \"r\") as txt_file:\n",
    "    txt = txt_file.read()\n",
    "    \n",
    "# txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial out of the box result\n",
    "es_tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
    "spa_abrevs = {\"ord\", \"num\", \"sra\", \"no\", \"corp\", \"art\", \"ltda\", \"ud\"}\n",
    "preprocessed1 = spanish_preprocessing(clean_text(txt))\n",
    "preprocessed2 = spanish_preprocessing(txt)\n",
    "s1 = get_nltk_sents(preprocessed1, es_tokenizer, spa_abrevs)\n",
    "s2 = get_nltk_sents(preprocessed2, es_tokenizer, spa_abrevs)\n",
    "len(s1), len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- We can add the `clean_text()` function but it may be more appropriate for english\n",
    "- With the execption of some weird characters, and some acronyms, sentences are parsed properly\n",
    "- Only concern is sometimes bullet points/nested enumerations are captured together or apart... but maybe that's for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surrounding_chars(txt, radius=1):\n",
    "    surrounding_chars = []\n",
    "    all_period_idx = [indices.start() for indices in re.finditer(\"\\.\", txt)]\n",
    "    \n",
    "    for period_idx in all_period_idx:\n",
    "        start_idx = period_idx - radius\n",
    "        end_idx = period_idx + radius + 1\n",
    "        substring = txt[start_idx: end_idx]\n",
    "        \n",
    "        if substring:\n",
    "            surrounding_chars.append(substring)\n",
    "    \n",
    "    return surrounding_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrounding_chars_1 = get_surrounding_chars(txt)\n",
    "surrounding_chars_2 = get_surrounding_chars(txt, radius=2)\n",
    "\n",
    "print(f\"For 1 character before and after a period, we have {len(set(surrounding_chars_1))} unique patterns\")\n",
    "print(f\"For 2 characters before and after a period, we have {len(set(surrounding_chars_2))} unique patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_chars(neighboring_chars):\n",
    "    possible_chars = defaultdict(list)\n",
    "\n",
    "    for pattern in neighboring_chars:\n",
    "        if pattern[-1] == \" \":\n",
    "            possible_chars[\" \"].append(pattern)\n",
    "        elif pattern[-1].isalpha():\n",
    "            possible_chars[\"alpha\"].append(pattern)\n",
    "        elif pattern[-1].isnumeric():\n",
    "            possible_chars[\"numeric\"].append(pattern)\n",
    "        elif not pattern[-1].isalnum():\n",
    "            possible_chars[\"symbol\"].append(pattern)\n",
    "        else:\n",
    "            possible_chars[\"other\"].append(pattern)\n",
    "    \n",
    "    print(f\"Total: {len(neighboring_chars)}\")\n",
    "    return possible_chars\n",
    "\n",
    "def print_char_stats(possible_chars):\n",
    "    print(f\"Space: {len(possible_chars[' '])}\"), \n",
    "    print(f\"Alpha: {len(possible_chars['alpha'])}\"), \n",
    "    print(f\"Numeric: {len(possible_chars['numeric'])}\"), \n",
    "    print(f\"Symbol: {len(possible_chars['symbol'])}\"), \n",
    "    print(f\"Other: {len(possible_chars['other'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_chars = get_possible_chars(surrounding_chars_1)\n",
    "print_char_stats(possible_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_chars = get_possible_chars(set(surrounding_chars_1))\n",
    "print_char_stats(possible_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
