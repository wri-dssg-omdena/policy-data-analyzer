{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.summarization import keywords, summarize\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from src.preprocessing import CorpusPreprocess\n",
    "from src import PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_ngrams(matrix, vocab, ngram, n=20):\n",
    "    \"\"\"\n",
    "    Function to get top n-grams from from document-term matrix and corresponding vocabulary\n",
    "    \"\"\"\n",
    "    # Filter vocab to include just n-grams\n",
    "    vocab = dict(filter(lambda x: len(x[0].split()) == ngram, vocab.items()))\n",
    "    # Filter matrix to include just n-grams\n",
    "    matrix = matrix[:, list(vocab.values())]\n",
    "    # Get inverse vocab mapping: new_matrix_index -> n-gram\n",
    "    inv_vocab = {i: k for i, (k, _) in enumerate(vocab.items())}\n",
    "    # Get count of each n-gram\n",
    "    counts = np.asarray(matrix.sum(axis=0)).flatten()\n",
    "    top_ngrams = defaultdict(int)\n",
    "    # Iterate over n argmax indexes of counts\n",
    "    for i in reversed(counts.argsort()[-n:]):\n",
    "        top_ngrams[inv_vocab[i]] = counts[i]\n",
    "    return top_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"tasks\", \"extract_text\", \"output\")\n",
    "with open(os.path.join(INPUT_PATH, \"pdf_files.json\")) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"filename\": data.keys(),\n",
    "        \"country\": [i[\"Country\"] for i in data.values()],\n",
    "        \"text\": [i[\"Text\"] for i in data.values()]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count field\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing document without text\n",
    "rmv = df.index[df['word_count'] == 1].tolist()\n",
    "print(df.loc[rmv, 'filename'])\n",
    "df = df.drop(rmv).reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing badly read documents\n",
    "bad_docs = [\"CreditoGanadero_Mexico\", \"Ley Especial Cafe_ElSalvador\", \"Sembrando Vida Report\"]\n",
    "df = df.drop(df.index[df['filename'].isin(bad_docs)].tolist()).reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment: Using a stanza pipeline -> turns out that lemmatization is not as necessary for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stanza \n",
    "\n",
    "# nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')\n",
    "# lemmatize_pipeline = stanza.Pipeline(lang='es', processors='tokenize, lemma')\n",
    "\n",
    "# def lemmatize_text(text):\n",
    "#     lemmatized_text = lemmatize_pipeline(text)\n",
    "#     return \" \".join([word.lemma for sentence in lemmatized_text.sentences for word in sentence.words])\n",
    "\n",
    "# df[\"pre_pretext\"] = df[\"pre_pretext\"].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mix common stopwords with words that we know are frequent, such as dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_stopwords = set(stopwords.words('spanish'))\n",
    "extra_stopwords = {\"ley\", \"artículo\", \"ser\", \"así\", \"según\", \"nº\", \"diario\", \n",
    "                   \"enero\", \"febrero\", \"marzo\", \"abril\", \"mayo\", \"junio\", \"julio\", \"agosto\", \"setiembre\", \"octubre\", \"noviembre\", \"diciembre\",\n",
    "                   \"lunes\", \"martes\", \"miercoles\", \"jueves\", \"viernes\", \"sabado\", \"domingo\"}\n",
    "spa_stopwords = spa_stopwords.union(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = CorpusPreprocess(\n",
    "    language='spanish', \n",
    "    stop_words=spa_stopwords,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    "    strip_numbers=True,\n",
    "    punctuation_list=punctuation,\n",
    "    strip_urls=True,\n",
    "#     stemmer=SnowballStemmer('spanish'), \n",
    "    max_df=0.9, \n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prep_text'] = prep.fit_transform(df['text'], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch word count for each document\n",
    "df['word_count'].plot(kind='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Describe word count\n",
    "df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we weight each document? Otherwise we could find keywords that do not represent each document in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "cv = CountVectorizer(max_features=20000, ngram_range=(1,7))\n",
    "bow_X = cv.fit_transform(df['prep_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top uni-grams\n",
    "top_unigrams = get_top_n_ngrams(bow_X, cv.vocabulary_, 1, 20)\n",
    "\n",
    "plt.bar(top_unigrams.keys(), top_unigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 unigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bi-grams\n",
    "top_bigrams = get_top_n_ngrams(bow_X, cv.vocabulary_, 2, 20)\n",
    "\n",
    "plt.bar(top_bigrams.keys(), top_bigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top tri-grams\n",
    "top_trigrams = get_top_n_ngrams(bow_X, cv.vocabulary_, 3, 20)\n",
    "\n",
    "plt.bar(top_trigrams.keys(), top_trigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 trigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we want to normalize by word counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_X_norm = bow_X / bow_X.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top uni-grams\n",
    "top_unigrams = get_top_n_ngrams(bow_X_norm, cv.vocabulary_, 1, 20)\n",
    "\n",
    "plt.bar(top_unigrams.keys(), top_unigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 unigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bi-grams\n",
    "top_bigrams = get_top_n_ngrams(bow_X_norm, cv.vocabulary_, 2, 20)\n",
    "\n",
    "plt.bar(top_bigrams.keys(), top_bigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top tri-grams\n",
    "top_trigrams = get_top_n_ngrams(bow_X_norm, cv.vocabulary_, 3, 20)\n",
    "\n",
    "plt.bar(top_trigrams.keys(), top_trigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 trigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "tv = TfidfVectorizer(max_features=20000, ngram_range=(1,3))\n",
    "tfidf_X = tv.fit_transform(df['prep_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top uni-grams\n",
    "top_unigrams = get_top_n_ngrams(tfidf_X, tv.vocabulary_, 1, 20)\n",
    "\n",
    "plt.bar(top_unigrams.keys(), top_unigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 unigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bi-grams\n",
    "top_bigrams = get_top_n_ngrams(tfidf_X, cv.vocabulary_, 2, 20)\n",
    "\n",
    "plt.bar(top_bigrams.keys(), top_bigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top tri-grams\n",
    "top_trigrams = get_top_n_ngrams(tfidf_X, cv.vocabulary_, 3, 20)\n",
    "\n",
    "plt.bar(top_trigrams.keys(), top_trigrams.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Top 20 trigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we see keywords for single document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[40, \"text\"][:1000],\"...\")\n",
    "\n",
    "print('\\nGet top uni-grams bow:')\n",
    "for k, v in get_top_n_ngrams(bow_X[40], cv.vocabulary_, 1, 10).items():\n",
    "    print(f\"\\\"{k}\\\" count: {round(v,3)}\")\n",
    "    \n",
    "print('\\nGet top uni-grams tfidf:')\n",
    "for k, v in get_top_n_ngrams(tfidf_X[40], tv.vocabulary_, 1, 10).items():\n",
    "    print(f\"\\\"{k}\\\" count: {round(v,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab = {k: v for k, v in sorted(cv.vocabulary_.items(), key=lambda item: item[1])}\n",
    "frequencies = np.asarray(bow_X.sum(axis=0)).flatten()\n",
    "word_freq = {k:v for k, v in zip(sorted_vocab.keys(), frequencies)}\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=100,\n",
    "    max_font_size=50, \n",
    "    random_state=42\n",
    ").generate_from_frequencies(word_freq)\n",
    "\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# fig.savefig(\"word1.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab = {k: v for k, v in sorted(cv.vocabulary_.items(), key=lambda item: item[1])}\n",
    "frequencies = np.asarray(bow_X_norm.sum(axis=0)).flatten()\n",
    "word_freq = {k:v for k, v in zip(sorted_vocab.keys(), frequencies)}\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=100,\n",
    "    max_font_size=50, \n",
    "    random_state=42\n",
    ").generate_from_frequencies(word_freq)\n",
    "\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# fig.savefig(\"word1.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab = {k: v for k, v in sorted(tv.vocabulary_.items(), key=lambda item: item[1])}\n",
    "frequencies = np.asarray(tfidf_X.sum(axis=0)).flatten()\n",
    "word_freq = {k:v for k, v in zip(sorted_vocab.keys(), frequencies)}\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=100,\n",
    "    max_font_size=50, \n",
    "    random_state=42\n",
    ").generate_from_frequencies(word_freq)\n",
    "\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# fig.savefig(\"word1.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword extraction algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (keep sentence structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['text'].apply(lambda x: sent_tokenize(x, language='spanish')).explode()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count per sentence\n",
    "sentences.str.split().apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = CorpusPreprocess(\n",
    "    language='spanish', \n",
    "    stop_words=spa_stopwords,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    "    strip_numbers=True,\n",
    "    strip_punctuation=punctuation,\n",
    "#   stemmer=SnowballStemmer('spanish'), \n",
    "    max_df=0.9, \n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "sentences_prep = pd.Series(prep.fit_transform(sentences, tokenize=False), index=sentences.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rake and TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in sentences_prep.index.unique():\n",
    "    # RAKE\n",
    "    rake = Rake(language=\"spanish\")\n",
    "    rake.extract_keywords_from_sentences(sentences_prep[ix])\n",
    "    rake_out = rake.get_ranked_phrases()\n",
    "    print(\"\\nRAKE OUTPUT:\\n> \", \"\\n> \".join(rake_out[:10]))\n",
    "    \n",
    "    # TextRankV1\n",
    "    textrankv1_out = keywords(\" \".join(sentences_prep[ix]), split=True)\n",
    "    print(\"\\nTEXTRANKV1 OUTPUT:\\n> \", \"\\n> \".join(textrankv1_out[:10]))\n",
    "    \n",
    "    # TextRankV2\n",
    "    textrankv2_out = summarize(\". \".join(sentences_prep[ix]), split=True)\n",
    "    print(\"\\nTEXTRANKV2 OUTPUT:\\n> \", \"\\n> \".join(textrankv2_out[:10]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look into \n",
    "https://boudinfl.github.io/pke/build/html/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
