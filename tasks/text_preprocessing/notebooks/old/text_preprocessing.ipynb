{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the preprocessing script\n",
    "We will try 3 different things:\n",
    "- The initial approach, of defining a single preprocessing function depending on the parameters given to the class (original)\n",
    "- Try using multiple map statements, with lambda functions\n",
    "- Try using multiple map statements, with existing static functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "\n",
    "class CorpusPreprocess(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, language='english', stop_words=None, lowercase=True, strip_accents=False,\n",
    "                 strip_numbers=False, strip_punctuation=None, stemmer=None, max_df=1.0, min_df=1):\n",
    "        \"\"\"Scikit-learn like Transformer for Corpus preprocessing.\n",
    "        Preprocesses text by applying multiple tasks (e.g. lowecasing, stemming, etc).\n",
    "        Fits the data for obtaining vocabulary_ (mapping of terms to document frequencies)\n",
    "         and stop_words_ (terms that were ignored because of either 'max_df', 'min_df' or 'stop_words').\n",
    "\n",
    "        Args:\n",
    "            language (str, optional): language of input text. Passed to word tokenizer. Defaults to 'english'.\n",
    "            stop_words (list, optional): list of stop words to be removed. Defaults to None.\n",
    "            lowercase (bool, optional): lowercases text if True. Defaults to True.\n",
    "            strip_accents (bool, optional): strips accents from text if True. Defaults to False.\n",
    "            strip_numbers (bool, optional): strips numbers from text if True. Defaults to False.\n",
    "            strip_punctuation (iterable, optional): strips provided punctuation from text if not None.\n",
    "             Defaults to None.\n",
    "            stemmer (Stemmer instance, optional): applies the provided Stemmer's stem method to text.\n",
    "             Defaults to None.\n",
    "            max_df (float in range [0.0, 1.0] or int, optional): ignore terms with a document frequency higher \n",
    "             than the given threshold. If float, the parameter represents a proportion of documents, integer \n",
    "             absolute counts. Defaults to 1.0.\n",
    "            min_df (float in range [0.0, 1.0] or int, optional): ignore terms with a document frequency lower \n",
    "             than the given threshold. If float, the parameter represents a proportion of documents, integer \n",
    "             absolute counts. Defaults to 1.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: max_df and min_df are bounded to range [0.0, 1.0]\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        self.stop_words = stop_words\n",
    "        self.lowercase = lowercase\n",
    "        self.strip_accents = strip_accents\n",
    "        self.strip_numbers = strip_numbers\n",
    "        self.strip_punctuation = strip_punctuation\n",
    "        self.stemmer = stemmer\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        if max_df < 0 or min_df < 0:\n",
    "            raise ValueError(\"negative value for max_df or min_df\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Building vocabulary_ and stop_words_\n",
    "        self.fit_transform(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None, tokenize=True):\n",
    "        # Preprocess and tokenize corpus\n",
    "        corpus = self._word_tokenizer(X)\n",
    "\n",
    "        # Build vocabulary document frequencies\n",
    "        vocab_df = defaultdict(int)\n",
    "        for doc in corpus:\n",
    "            for unique in set(doc):\n",
    "                vocab_df[unique] += 1\n",
    "\n",
    "        # Find stop_words_ based on max_df and min_df\n",
    "        if self.stop_words is None:\n",
    "            self.stop_words_ = set()\n",
    "        else:\n",
    "            self.stop_words_ = set(self.stop_words)\n",
    "\n",
    "        if self.max_df is not None:\n",
    "            if isinstance(self.max_df, float):\n",
    "                vocab_rel_df = {k: v / len(X) for k, v in vocab_df.items()}\n",
    "                self.stop_words_.update(\n",
    "                    {k for k, v in vocab_rel_df.items() if v > self.max_df})\n",
    "            else:\n",
    "                self.stop_words_.update(\n",
    "                    {k for k, v in vocab_df.items() if v > self.max_df})\n",
    "\n",
    "        if self.min_df is not None:\n",
    "            if isinstance(self.min_df, float):\n",
    "                vocab_rel_df = {k: v / len(X) for k, v in vocab_df.items()}\n",
    "                self.stop_words_.update(\n",
    "                    {k for k, v in vocab_rel_df.items() if v < self.min_df})\n",
    "            else:\n",
    "                self.stop_words_.update(\n",
    "                    {k for k, v in vocab_df.items() if v < self.min_df})\n",
    "\n",
    "        # Remove stop_words_ from vocabulary\n",
    "        for k in self.stop_words_:\n",
    "            vocab_df.pop(k, None)\n",
    "\n",
    "        # Set vocabulary_\n",
    "        self.vocabulary_ = vocab_df\n",
    "\n",
    "        # Remove stop_words from corpus\n",
    "        if self.stop_words is not None:\n",
    "            corpus = [[token for token in doc if token not in self.stop_words]\n",
    "                      for doc in corpus]\n",
    "\n",
    "        # Split vs merged\n",
    "        if not tokenize:\n",
    "            corpus = [\" \".join(doc) for doc in corpus]\n",
    "\n",
    "        return corpus\n",
    "\n",
    "    def transform(self, X, y=None, tokenize=True):\n",
    "        # Check if fit has been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Preprocess and tokenize corpus\n",
    "        corpus = self._word_tokenizer(X)\n",
    "\n",
    "        # Remove stop_words from corpus\n",
    "        corpus = [[token for token in doc if token not in self.stop_words_]\n",
    "                  for doc in corpus]\n",
    "\n",
    "        # Split vs merged\n",
    "        if not tokenize:\n",
    "            corpus = [\" \".join(doc) for doc in corpus]\n",
    "\n",
    "        return corpus\n",
    "\n",
    "    def _word_tokenizer(self, X):\n",
    "        \"\"\"Preprocesses and tokenizes each document by applying a \n",
    "         preprocessing function.\n",
    "\n",
    "        Args:\n",
    "            X (iterable): documents to preprocess\n",
    "\n",
    "        Returns:\n",
    "            list: preprocessed and tokenized documents\n",
    "        \"\"\"\n",
    "        # Define function conditionally so we only need to evaluate the condition once instead at every document\n",
    "        if self.strip_accents and self.lowercase and self.strip_numbers and self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        elif self.strip_accents and self.lowercase and self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        if self.strip_accents and self.lowercase and self.strip_numbers:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                return doc\n",
    "        if self.strip_accents and self.strip_numbers and self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        if self.lowercase and self.strip_numbers and self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        elif self.strip_accents and self.lowercase:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                return doc\n",
    "        elif self.strip_accents and self.strip_numbers:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                return doc\n",
    "        elif self.strip_accents and self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        elif self.lowercase and self.strip_numbers:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                return doc\n",
    "        elif self.lowercase and self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        elif self.strip_numbers and self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        elif self.strip_accents:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Remove accentuation\n",
    "                doc = unicodedata.normalize('NFKD', doc).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII')\n",
    "                return doc\n",
    "        elif self.lowercase:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Lowercase\n",
    "                doc = doc.lower()\n",
    "                return doc\n",
    "        elif self.strip_numbers:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Remove numbers\n",
    "                doc = doc.translate(str.maketrans('', '', \"0123456789\"))\n",
    "                return doc\n",
    "        elif self.strip_punctuation is not None:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                # Remove punctuation\n",
    "                doc = doc.translate(str.maketrans(\n",
    "                    '', '', self.strip_punctuation))\n",
    "                return doc\n",
    "        else:\n",
    "            def doc_preprocessing(doc):\n",
    "                # Removes HTML tags\n",
    "                doc = BeautifulSoup(doc, features=\"lxml\").get_text()\n",
    "                return doc\n",
    "\n",
    "        # Apply cleaning function over X\n",
    "        corpus = map(doc_preprocessing, X)\n",
    "\n",
    "        # Word tokenizer\n",
    "        corpus = [word_tokenize(doc, language=self.language) for doc in corpus]\n",
    "\n",
    "        if self.stemmer is not None:\n",
    "            corpus = [[self.stemmer.stem(token)\n",
    "                       for token in doc] for doc in corpus]\n",
    "            return corpus\n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    def _word_tokenizer1(self, X):\n",
    "        \"\"\"Preprocesses and tokenizes each document by applying a \n",
    "         preprocessing function.\n",
    "\n",
    "        Args:\n",
    "            X (iterable): documents to preprocess\n",
    "\n",
    "        Returns:\n",
    "            list: preprocessed and tokenized documents\n",
    "        \"\"\"\n",
    "        docs = map(lambda x: BeautifulSoup(x, features=\"lxml\").get_text(), X)\n",
    "        \n",
    "        if self.lowercase:\n",
    "            docs = map(str.lower, docs)\n",
    "        \n",
    "        if self.strip_accents:\n",
    "            docs = map(lambda x: unicodedata.normalize('NFKD', x).encode(\n",
    "                    'ASCII', 'ignore').decode('ASCII'), docs)\n",
    "        if self.strip_numbers:\n",
    "            docs = map(lambda x: x.translate(str.maketrans('', '', \"0123456789\")), docs)\n",
    "        \n",
    "        if self.strip_punctuation is not None:\n",
    "            docs = map(lambda x: x.translate(str.maketrans('', '', self.strip_punctuation)), docs)\n",
    "        \n",
    "        # Word tokenizer\n",
    "        corpus = [word_tokenize(doc, language=self.language) for doc in docs]\n",
    "\n",
    "        if self.stemmer is not None:\n",
    "            corpus = [[self.stemmer.stem(token)\n",
    "                       for token in doc] for doc in corpus]\n",
    "            return corpus\n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    def _word_tokenizer2(self, X):\n",
    "        \"\"\"Preprocesses and tokenizes each document by applying a \n",
    "         preprocessing function.\n",
    "\n",
    "        Args:\n",
    "            X (iterable): documents to preprocess\n",
    "\n",
    "        Returns:\n",
    "            list: preprocessed and tokenized documents\n",
    "        \"\"\"\n",
    "\n",
    "        docs = map(remove_html_tags, X)\n",
    "        \n",
    "        if self.lowercase:\n",
    "            docs = map(str.lower, docs)\n",
    "        \n",
    "        if self.strip_accents:\n",
    "            docs = map(remove_accents, docs)\n",
    "            \n",
    "        if self.strip_numbers:\n",
    "            docs = map(remove_numbers, docs)\n",
    "        \n",
    "        if self.strip_punctuation is not None:\n",
    "            docs = [remove_punctuation(doc, self.strip_punctuation) for doc in docs]\n",
    "        \n",
    "        # Word tokenizer\n",
    "        corpus = [word_tokenize(doc, language=self.language) for doc in docs]\n",
    "        \n",
    "        if self.stemmer is not None:\n",
    "            corpus = [[self.stemmer.stem(token)\n",
    "                       for token in doc] for doc in corpus]\n",
    "\n",
    "        return corpus\n",
    "\n",
    "    \n",
    "def remove_html_tags(x):\n",
    "    return BeautifulSoup(x, features=\"lxml\").get_text()\n",
    "\n",
    "def remove_accents(x):\n",
    "    return unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def remove_numbers(x):\n",
    "    return x.translate(str.maketrans('', '', \"0123456789\"))\n",
    "\n",
    "def remove_punctuation(x, punctuation_list):\n",
    "    return x.translate(str.maketrans('', '', punctuation_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.summarization import keywords, summarize\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from src import PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"tasks\", \"extract_text\", \"output\")\n",
    "with open(os.path.join(INPUT_PATH, \"pdf_files.json\")) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"filename\": data.keys(),\n",
    "        \"country\": [i[\"Country\"] for i in data.values()],\n",
    "        \"text\": [i[\"Text\"] for i in data.values()]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count field\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "# Removing document without text\n",
    "df = df.drop(df.index[df['word_count'] == 1].tolist()).reset_index(drop=True)\n",
    "# Removing badly read documents\n",
    "bad_docs = [\"CreditoGanadero_Mexico\", \"Ley Especial Cafe_ElSalvador\", \"Sembrando Vida Report\"]\n",
    "df = df.drop(df.index[df['filename'].isin(bad_docs)].tolist()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019CVE 1713470_Chile</td>\n",
       "      <td>Chile</td>\n",
       "      <td>CVE 1713470|Director: Juan Jorge Lazo Rodrígue...</td>\n",
       "      <td>10424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decreto 51_Chile</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Biblioteca del Congreso Nacional de Chile - ww...</td>\n",
       "      <td>22478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decreto 95_Chile</td>\n",
       "      <td>Chile</td>\n",
       "      <td>www.bcn.cl - Biblioteca del Congreso Nacional ...</td>\n",
       "      <td>6068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decreto8_Chile</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Biblioteca del Congreso Nacional de Chile - ww...</td>\n",
       "      <td>1209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ley 20412_Chile</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Biblioteca del Congreso Nacional de Chile - ww...</td>\n",
       "      <td>4349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename country  \\\n",
       "0  2019CVE 1713470_Chile   Chile   \n",
       "1       Decreto 51_Chile   Chile   \n",
       "2       Decreto 95_Chile   Chile   \n",
       "3         Decreto8_Chile   Chile   \n",
       "4        Ley 20412_Chile   Chile   \n",
       "\n",
       "                                                text  word_count  \n",
       "0  CVE 1713470|Director: Juan Jorge Lazo Rodrígue...       10424  \n",
       "1  Biblioteca del Congreso Nacional de Chile - ww...       22478  \n",
       "2  www.bcn.cl - Biblioteca del Congreso Nacional ...        6068  \n",
       "3  Biblioteca del Congreso Nacional de Chile - ww...        1209  \n",
       "4  Biblioteca del Congreso Nacional de Chile - ww...        4349  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename      58\n",
       "country       58\n",
       "text          58\n",
       "word_count    58\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_stopwords = set(stopwords.words('spanish'))\n",
    "extra_stopwords = {\"ley\", \"artículo\", \"ser\", \"así\", \"según\", \"nº\"}\n",
    "spa_stopwords = spa_stopwords.union(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = CorpusPreprocess(\n",
    "    language='spanish', \n",
    "    stop_words=spa_stopwords,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    "    strip_numbers=True,\n",
    "    strip_punctuation=punctuation,\n",
    "    stemmer=SnowballStemmer('spanish'), \n",
    "    max_df=0.9, \n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "We will compare performance after calling the word tokenizer function 10 times for each alternative:\n",
    "- Original approach: `_word_tokenizer()`\n",
    "- Map with lambda: `_word_tokenizer1()`\n",
    "- Map with existing functions: `_word_tokenizer2()`\n",
    "\n",
    "and print the average duration of a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func(func, args):\n",
    "    durations = list()\n",
    "    \n",
    "    for i in tqdm(range(10)):\n",
    "        start = time.time()\n",
    "        output = func(args)\n",
    "        stop = time.time()\n",
    "        durations.append(stop-start)\n",
    "        \n",
    "    return sum(durations)/len(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:30<00:00, 33.10s/it]\n"
     ]
    }
   ],
   "source": [
    "avg_duration_og = evaluate_func(prep._word_tokenizer, df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg duration for og option: 33.09642074108124\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg duration for og option:\", avg_duration_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:22<00:00, 32.29s/it]\n"
     ]
    }
   ],
   "source": [
    "avg_duration_first = evaluate_func(prep._word_tokenizer1, df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg duration for option 1: 32.29024872779846\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg duration for option 1:\", avg_duration_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:26<00:00, 32.65s/it]\n"
     ]
    }
   ],
   "source": [
    "avg_duration_second = evaluate_func(prep._word_tokenizer2, df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg duration for option 2: 32.64609534740448\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg duration for option 2:\", avg_duration_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "After comparing the 3 alternatives, a combination of `map()` + existing functions ends up being mildly more efficient and more readable. Whenever we want to add a new type of transformation to the text we just add another `if` statement and another separate function, then we call `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "dsci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
