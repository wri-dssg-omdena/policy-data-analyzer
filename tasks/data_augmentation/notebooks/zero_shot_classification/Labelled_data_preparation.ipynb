{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_dict = {'Credit' : 'Credit', 'Direct' : 'Direct payment', 'Fine' : 'Fine', 'General' : 'Unknown', \n",
    "#                'Guarantee' : 'Credit', 'Supplies' : 'Supplies', 'Tax' : 'Tax deduction', \n",
    "#                'Technical' : 'Technical assistance', 'Unknown' : 'Unknown', 'Other' : 'Unknown', 'Nan' : 'Unknown' }\n",
    "# policy_counter = {\"Credit\" : 0, \"Direct payment\" : 0, \"Fine\" : 0, \"Supplies\" : 0, \"Tax deduction\" : 0, \"Technical assistance\" : 0}\n",
    "# incentive_counter = {\"Incentive\" : 0, \"not_Incentive\" : 0}\n",
    "\n",
    "policy_dict = {'Loan' : 'Loan', 'Direct' : 'Direct payment', 'Fine' : 'Fine', 'General' : 'Unknown', \n",
    "               'Guarantee' : 'Credit', 'Supplies' : 'Supplies', 'Tax' : 'Tax benefit', \n",
    "               'Technical' : 'Technical assistance', 'Unknown' : 'Unknown', 'Other' : 'Unknown', 'Nan' : 'Unknown' }\n",
    "policy_counter = {\"Credit\" : 0, \"Direct payment\" : 0, \"Fine\" : 0, \"Supplies\" : 0, \"Tax benefit\" : 0, \"Technical assistance\" : 0}\n",
    "incentive_counter = {\"Incentive\" : 0, \"not_Incentive\" : 0}\n",
    "\n",
    "incentive_dict = {'Incentive' : 'Incentive', 'Disincentive' : 'Incentive', 'Unknown' : 'not_Incentive', 'Nan' : 'not_Incentive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_df(df, reference_column_to_drop_na):\n",
    "    df.dropna(subset = [reference_column_to_drop_na], inplace = True)\n",
    "    df.dropna(axis=1, how='all', inplace = True)\n",
    "    df.replace(np.nan, 'Nan', regex=True, inplace = True)\n",
    "    return df\n",
    "\n",
    "def process_new_labels(filename, reference_column_to_drop_na, policy):\n",
    "    df_temp = pd.concat(pd.read_excel(filename, engine='openpyxl', sheet_name = None, skiprows=[0]), ignore_index = True)\n",
    "    df_temp = clean_df(df_temp, reference_column_to_drop_na)\n",
    "    df_temp.insert(0, \"Document\", df_temp.apply(lambda row: row.Sentence_Id.split(\"_\")[0], axis = 1))\n",
    "    df_temp.loc[df_temp['Is_policy'] == 0, 'Is_policy'] = \"Nan\"\n",
    "    df_temp.loc[df_temp['Is_policy'] == 1, 'Is_policy'] = policy\n",
    "    if \"Other_instrument\"in df_temp.columns:\n",
    "        df_temp['Is_policy'] = np.where(df_temp['Is_policy'] == \"Nan\", df_temp['Other_instrument'], df_temp['Is_policy'])\n",
    "    df_temp.loc[df_temp['Is_incentive'] == 0, 'Is_incentive'] = \"Unknown\"\n",
    "    df_temp.loc[df_temp['Is_incentive'] == 1, 'Is_incentive'] = \"Incentive\"\n",
    "    return df_temp\n",
    "\n",
    "def label_cleaning(dictionary, label):\n",
    "    flag = 1\n",
    "    \n",
    "    for key in dictionary:\n",
    "#         print(key, \"----\", label)\n",
    "        if key in label:\n",
    "            return dictionary[key]\n",
    "            flag = 0\n",
    "            break\n",
    "    if flag == 1:\n",
    "#         print(label)\n",
    "        return label\n",
    "\n",
    "def merge_excel_to_list(filename):\n",
    "    df = process_new_labels(filename, \"Is_incentive\", policy_instrument)\n",
    "    List = df[[\"Document\", \"Sentence_Id\", \"Sentence\", \"Is_policy\", \"Is_incentive\"]].values.tolist()\n",
    "    \n",
    "    return List\n",
    "\n",
    "def merge_excel_to_list_new(filename):\n",
    "    flag = False\n",
    "    \n",
    "    for policy in policy_dict:\n",
    "        if policy in filename:\n",
    "            policy_instrument = policy\n",
    "            flag = True\n",
    "    if flag:\n",
    "#         print(filename)\n",
    "        df = process_new_labels(filename, \"Is_incentive\", policy_instrument)\n",
    "        List = df[[\"Document\", \"Sentence_Id\", \"Sentence\", \"Is_policy\", \"Is_incentive\"]].values.tolist()\n",
    "    \n",
    "    return List\n",
    "\n",
    "def merge_excel_to_list_old(filename):\n",
    "    df = pd.concat(pd.read_excel(filename, engine='openpyxl', sheet_name = None), ignore_index = True)\n",
    "    df = clean_df(df, \"Document\")\n",
    "    df = df[[\"Document\", \"Sentence\", \"Primary_Instrument\", \"Category\"]]\n",
    "    data = df.values.tolist()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def list_to_dict(List, dictionary, dataset):\n",
    "    Dictionary = {}\n",
    "    i = 0\n",
    "    for item in List:\n",
    "        i += 1\n",
    "        if dataset == \"Old\":\n",
    "            Dictionary[str(i)] = {\"text\" : item[1], \"labels\" : label_cleaning(policy_dict, item [2]), \"incentive\": label_cleaning(incentive_dict, item [3])}\n",
    "        elif dataset == \"New\":\n",
    "            dictionary[item[1]] = {\"text\" : item[2], \"labels\" : label_cleaning(policy_dict, item [3]), \"incentive\": label_cleaning(incentive_dict, item [4])}\n",
    "    if dataset == \"Old\":\n",
    "        return Dictionary\n",
    "    elif dataset == \"New\":\n",
    "        return dictionary\n",
    "        \n",
    "    \n",
    "\n",
    "def dictionary_to_final_lists(dictionary, classifier):\n",
    "    all_sents = []\n",
    "    all_labels = []\n",
    "        \n",
    "    for item in dictionary:\n",
    "        if classifier == \"Binary\":\n",
    "            all_sents.append(dictionary[item][\"text\"])\n",
    "            all_labels.append(dictionary[item][\"incentive\"])\n",
    "        elif classifier == \"Multiclass\":\n",
    "            if dictionary[item][\"labels\"] != \"Unknown\":\n",
    "                all_sents.append(dictionary[item][\"text\"])\n",
    "                all_labels.append(dictionary[item][\"labels\"])\n",
    "    \n",
    "    return all_sents, all_labels\n",
    "        \n",
    "\n",
    "def save_data(X_train, y_train, X_test, y_test, experiment, classifier):\n",
    "    base_path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Final_input_data/\"\n",
    "    \n",
    "    path = base_path + classifier + \"/\"\n",
    "    \n",
    "    filename = \"{}_train_sentences.csv\".format(experiment)\n",
    "    file = path + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerows(zip(X_train))\n",
    "\n",
    "    filename = \"{}_train_labels.csv\".format(experiment)\n",
    "    file = path + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerows(zip(y_train))\n",
    "\n",
    "    filename = \"{}_test_sentences.csv\".format(experiment)\n",
    "    file = path + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerows(zip(X_test))\n",
    "\n",
    "    filename = \"{}_test_labels.csv\".format(experiment)\n",
    "    file = path + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerows(zip(y_test))\n",
    "        \n",
    "def load_dictionary(file):\n",
    "    with open(file, 'r') as f:\n",
    "        dictionary = json.load(f)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_instrument' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f7d91ce4a3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mincentive_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_excel_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-1401e213d495>\u001b[0m in \u001b[0;36mmerge_excel_to_list\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmerge_excel_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_new_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Is_incentive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_instrument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Document\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sentence_Id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sentence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Is_policy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Is_incentive\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_instrument' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"/home/propietari/Documents/GitHub/policy-data-analyzer/tasks/data_augmentation/input/\"\n",
    "filename = \"pre_labeled_English_ready_short.xlsx\"\n",
    "file = path + filename\n",
    "\n",
    "incentive_list = merge_excel_to_list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raters multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_perc = 0.2\n",
    "rater = \"Rater1\"\n",
    "classifier = \"Multiclass\"\n",
    "base_path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Final_input_data/{}/\".format(rater))\n",
    "data_path = base_path.glob('**/')\n",
    "\n",
    "for path in data_path:\n",
    "#         results_list = []    \n",
    "    path_in_str = str(path)\n",
    "#         print(path_in_str)\n",
    "    if rater in path_in_str:\n",
    "        dictionary = {}\n",
    "#         print(path_in_str)\n",
    "        for file_obj in Path(path_in_str).glob('*.xlsx'):\n",
    "            file = str(file_obj)\n",
    "            if \"Unique\" in file:\n",
    "                print(file)\n",
    "                dictionary_new_labels = list_to_dict(merge_excel_to_list_new(file), dictionary, \"New\")\n",
    "            else:\n",
    "#                 print(file)\n",
    "                dictionary_old_labels = list_to_dict(merge_excel_to_list_old(file), dictionary, \"Old\")\n",
    "                all_sents_old, all_labels_old = dictionary_to_final_lists(dictionary_old_labels, classifier)\n",
    "                \n",
    "        all_sents_new, all_labels_new = dictionary_to_final_lists(dictionary_new_labels, classifier)\n",
    "print(len(all_sents_old), \" -- \", len(all_labels_old))\n",
    "print(len(all_sents_new), \" -- \", len(all_labels_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"EXP3\", \"EXP9\", \"EXP15\"]\n",
    "if rater == \"Rater3\" and classifier == \"Multiclass\":\n",
    "    for experiment in experiments:\n",
    "        if experiment == \"EXP3\":\n",
    "            X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "            save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "        if experiment == \"EXP9\":\n",
    "            save_data(all_sents_new, all_labels_new, all_sents_old, all_labels_old, experiment, classifier)\n",
    "        if experiment == \"EXP15\":\n",
    "            all_sents = all_sents_new + all_sents_old\n",
    "            all_labels = all_labels_new + all_labels_old\n",
    "            X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "            save_data(X_train, y_train, X_test, y_test, experiment, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"EXP1\", \"EXP7\", \"EXP13\"]\n",
    "if rater == \"Rater1\" and classifier == \"Multiclass\":\n",
    "    for experiment in experiments:\n",
    "        if experiment == \"EXP1\":\n",
    "            X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "            save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "        if experiment == \"EXP7\":\n",
    "            save_data(all_sents_new, all_labels_new, all_sents_old, all_labels_old, experiment, classifier)\n",
    "        if experiment == \"EXP13\":\n",
    "            all_sents = all_sents_new + all_sents_old\n",
    "            all_labels = all_labels_new + all_labels_old\n",
    "            X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "            save_data(X_train, y_train, X_test, y_test, experiment, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"EXP2\", \"EXP8\", \"EXP14\"]\n",
    "if rater == \"Rater2\" and classifier == \"Multiclass\":\n",
    "    for experiment in experiments:\n",
    "        if experiment == \"EXP2\":\n",
    "            X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "            save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "        if experiment == \"EXP8\":\n",
    "            save_data(all_sents_new, all_labels_new, all_sents_old, all_labels_old, experiment, classifier)\n",
    "        if experiment == \"EXP14\":\n",
    "            all_sents = all_sents_new + all_sents_old\n",
    "            all_labels = all_labels_new + all_labels_old\n",
    "            X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "            save_data(X_train, y_train, X_test, y_test, experiment, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raters = [\"Rater1\", \"Rater2\", \"Rater3\"]\n",
    "only_raters = [True, False]\n",
    "# base_path = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Final_input_data/\")\n",
    "base_path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Final_input_data/\")\n",
    "\n",
    "for rater in raters:\n",
    "    for only_rater in only_raters:\n",
    "        wraping_up(base_path, rater, only_rater)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merges multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raters = {\"Rater1\" : rater1, \"Rater2\" : rater2, \"Rater3\" : rater3}\n",
    "test_perc = 0.2\n",
    "classifier = \"Multiclass\"\n",
    "\n",
    "all = {}\n",
    "rater1 = {}\n",
    "rater2 = {}\n",
    "rater3 = {}\n",
    "\n",
    "for rater in raters:\n",
    "    base_path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Final_input_data/{}/\".format(rater))\n",
    "    data_path = base_path.glob('**/')\n",
    "\n",
    "    for path in data_path:\n",
    "    #         results_list = []    \n",
    "        path_in_str = str(path)\n",
    "    #         print(path_in_str)\n",
    "        if rater in path_in_str:\n",
    "            dictionary = {}\n",
    "    #         print(path_in_str)\n",
    "            for file_obj in Path(path_in_str).glob('*.xlsx'):\n",
    "                file = str(file_obj)\n",
    "                if \"Unique\" in file:\n",
    "                    dictionary_new_labels = list_to_dict(merge_excel_to_list_new(file), dictionary, \"New\")\n",
    "                else:\n",
    "                    dictionary_old_labels = list_to_dict(merge_excel_to_list_old(file), dictionary, \"Old\")\n",
    "                    all_old, all_labels_old = dictionary_to_final_lists(dictionary_old_labels, classifier)\n",
    "                    \n",
    "            if \"Rater1\" in path_in_str:\n",
    "                rater1 = dictionary_new_labels\n",
    "            elif \"Rater2\" in path_in_str:\n",
    "                rater2 = dictionary_new_labels          \n",
    "            elif \"Rater3\" in path_in_str:\n",
    "                rater3 = dictionary_new_labels\n",
    "all = {**rater1, **rater2, **rater3}\n",
    "\n",
    "print(len(rater1), len(rater2), len(rater3), len(all))\n",
    "\n",
    "# Next we loop for the dictionary to find the elements that meet the criteria of the different merges.\n",
    "\n",
    "incentive = \"labels\" #write \"labels\" if you want to work with policy instruments, write \"incentive\" to work with is_incentive\n",
    "merge1 = {}\n",
    "merge2 = {}\n",
    "merge3 = {}\n",
    "# classifier = \"labels\" #If you want to classify by is_incentive\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "m = 0\n",
    "n = 0\n",
    "# First we look for sentences that all three raters have labeled the same\n",
    "for sent in all:\n",
    "    i += 1\n",
    "    if sent in rater1 and sent in rater2 and sent in rater3:\n",
    "        j += 1\n",
    "        if rater1[sent][\"labels\"] == rater2[sent][\"labels\"] and rater2[sent][\"labels\"] == rater3[sent][\"labels\"]:\n",
    "            k += 1\n",
    "            if rater1[sent][\"labels\"] != 'Unknown':\n",
    "                merge3[sent] = rater1[sent]\n",
    "\n",
    "#Now we look for the sentences that at least two raters have labeled the same\n",
    "for sent in all:\n",
    "    if sent in rater1 and sent in rater2:\n",
    "        l += 1\n",
    "        if rater1[sent][\"labels\"] == rater2[sent][\"labels\"]:\n",
    "            if rater1[sent][\"labels\"] != 'Unknown':\n",
    "                m += 1\n",
    "                merge2[sent] = rater1[sent]\n",
    "    elif sent in rater1 and sent in rater3:\n",
    "        l += 1\n",
    "        if rater1[sent][\"labels\"] == rater3[sent][\"labels\"]:\n",
    "            if rater1[sent][\"labels\"] != 'Unknown':\n",
    "                m += 1\n",
    "                merge2[sent] = rater1[sent]\n",
    "    elif sent in rater2 and sent in rater3:\n",
    "        l += 1\n",
    "        if rater2[sent][\"labels\"] == rater3[sent][\"labels\"]:\n",
    "            if rater2[sent][\"labels\"] != 'Unknown':\n",
    "                m += 1\n",
    "                merge2[sent] = rater2[sent]\n",
    "            \n",
    "# Finally we build a dataset containing the sentences that at least one of the labelers have labeled with a label different from \"Unknown\"\n",
    "for sent in all:\n",
    "    label = {}\n",
    "    if sent in rater3 and rater3[sent][\"labels\"] != 'Unknown':\n",
    "#         label[rater3[sent][\"labels\"]] = \"rater3\"\n",
    "        merge1[sent] = rater3[sent]\n",
    "    elif sent in rater2 and rater2[sent][\"labels\"] != 'Unknown':\n",
    "#         label[rater2[sent][\"labels\"]] = \"rater2\"\n",
    "        merge1[sent] = rater2[sent]\n",
    "    elif sent in rater1 and rater1[sent][\"labels\"] != 'Unknown':\n",
    "#         label[rater1[sent][\"labels\"]] = \"rater1\"\n",
    "        merge1[sent] = rater1[sent]\n",
    "#     else:\n",
    "#         merge1[sent] = all[sent]\n",
    "\n",
    "print(f\"In the all-sentences dict there are {i} sentences\")\n",
    "print(f\"In the three raters lists there are {j} common sentences\")\n",
    "print(f\"In the three raters lists there are {k} common sentences which are rated identically\")\n",
    "print(f\"There are {l} sentences which are comon to at lest two rater's datasets\")\n",
    "print(f\"There are {m} sentences which are labeled identical in at least two rater's datasets\")\n",
    "print(len(merge3))\n",
    "print(len(merge2))\n",
    "print(len(merge1))\n",
    "\n",
    "merges = {\"Merge1\" : merge1, \"Merge2\" : merge2, \"Merge3\" : merge3}\n",
    "for merge in merges:\n",
    "    all_new = []\n",
    "    all_labels_new = []\n",
    "    for sent in merges[merge]:\n",
    "        all_new.append(merges[merge][sent][\"text\"])\n",
    "        all_labels_new.append(merges[merge][sent][\"labels\"])\n",
    "    print(merge)\n",
    "    if merge == \"Merge1\" and classifier == \"Multiclass\":\n",
    "        experiments = [\"EXP4\", \"EXP10\", \"EXP16\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP4\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP10\":\n",
    "                save_data(all_new, all_labels_new, all_old, all_labels_old, experiment, classifier)\n",
    "            if experiment == \"EXP16\":\n",
    "                all = all_new + all_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "    \n",
    "    if merge == \"Merge2\" and classifier == \"Multiclass\":\n",
    "        experiments = [\"EXP5\", \"EXP11\", \"EXP17\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP5\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP11\":\n",
    "                save_data(all_new, all_labels_new, all_old, all_labels_old, experiment, classifier)\n",
    "            if experiment == \"EXP17\":\n",
    "                all = all_new + all_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "                \n",
    "    if merge == \"Merge3\" and classifier == \"Multiclass\":\n",
    "        experiments = [\"EXP6\", \"EXP12\", \"EXP18\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP6\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP12\":\n",
    "                save_data(all_new, all_labels_new, all_old, all_labels_old, experiment, classifier)\n",
    "            if experiment == \"EXP18\":\n",
    "                all = all_new + all_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raters binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perc = 0.2\n",
    "# rater = \"Rater1\"\n",
    "classifier = \"Binary\"\n",
    "all = {}\n",
    "rater1 = {}\n",
    "rater2 = {}\n",
    "rater3 = {}\n",
    "raters = {\"Rater1\" : rater1, \"Rater2\" : rater2, \"Rater3\" : rater3}\n",
    "\n",
    "for rater in raters:\n",
    "    base_path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Final_input_data/{}/\".format(rater))\n",
    "    data_path = base_path.glob('**/')\n",
    "\n",
    "    for path in data_path:\n",
    "    #         results_list = []    \n",
    "        path_in_str = str(path)\n",
    "    #         print(path_in_str)\n",
    "        if rater in path_in_str:\n",
    "            dictionary = {}\n",
    "    #         print(path_in_str)\n",
    "            for file_obj in Path(path_in_str).glob('*.xlsx'):\n",
    "                file = str(file_obj)\n",
    "                if \"Unique\" in file:\n",
    "#                     print(file)\n",
    "                    dictionary_new_labels = list_to_dict(merge_excel_to_list_new(file), dictionary, \"New\")\n",
    "                else:\n",
    "    #                 print(file)\n",
    "                    dictionary_old_labels = list_to_dict(merge_excel_to_list_old(file), dictionary, \"Old\")\n",
    "                    all_sents_old, all_labels_old = dictionary_to_final_lists(dictionary_old_labels, classifier)\n",
    "            if \"Rater1\" in path_in_str:\n",
    "                raters[rater] = dictionary_new_labels\n",
    "            elif \"Rater2\" in path_in_str:\n",
    "                raters[rater] = dictionary_new_labels          \n",
    "            elif \"Rater3\" in path_in_str:\n",
    "                raters[rater] = dictionary_new_labels\n",
    "# all = {**rater1, **rater2, **rater3}\n",
    "\n",
    "    unknown = 0\n",
    "    known = 0\n",
    "    bugs = 0\n",
    "    all_sents_new = []\n",
    "    all_labels_new = []\n",
    "    for sent in raters[rater]:\n",
    "        all_sents_new.append(raters[rater][sent][\"text\"])\n",
    "        all_labels_new.append(raters[rater][sent][\"incentive\"])\n",
    "        if raters[rater][sent][\"incentive\"] == \"not_Incentive\":\n",
    "            unknown += 1\n",
    "        elif raters[rater][sent][\"incentive\"] == \"Incentive\":\n",
    "            known += 1\n",
    "        else:\n",
    "            bugs += 1\n",
    "    print(rater)\n",
    "    print(len(raters[rater]))\n",
    "    print(\"Incentive\", known, \" -- not_Incentive\", unknown, \" -- Others\", bugs)\n",
    "\n",
    "    if rater == \"Rater1\" and classifier == \"Binary\":\n",
    "        experiments = [\"EXP19\", \"EXP25\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP19\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP25\":\n",
    "                all_sents = all_sents_new + all_sents_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "                \n",
    "    if rater == \"Rater2\" and classifier == \"Binary\":\n",
    "        experiments = [\"EXP20\", \"EXP26\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP20\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP26\":\n",
    "                all_sents = all_sents_new + all_sents_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "                \n",
    "    if rater == \"Rater3\" and classifier == \"Binary\":\n",
    "        experiments = [\"EXP21\", \"EXP27\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP21\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP27\":\n",
    "                all_sents = all_sents_new + all_sents_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merges binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perc = 0.2\n",
    "# rater = \"Rater1\"\n",
    "classifier = \"Binary\"\n",
    "all = {}\n",
    "rater1 = {}\n",
    "rater2 = {}\n",
    "rater3 = {}\n",
    "raters = {\"Rater1\" : rater1, \"Rater2\" : rater2, \"Rater3\" : rater3}\n",
    "\n",
    "for rater in raters:\n",
    "    base_path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Final_input_data/{}/\".format(rater))\n",
    "    data_path = base_path.glob('**/')\n",
    "\n",
    "    for path in data_path:\n",
    "    #         results_list = []    \n",
    "        path_in_str = str(path)\n",
    "    #         print(path_in_str)\n",
    "        if rater in path_in_str:\n",
    "            dictionary = {}\n",
    "    #         print(path_in_str)\n",
    "            for file_obj in Path(path_in_str).glob('*.xlsx'):\n",
    "                file = str(file_obj)\n",
    "                if \"Unique\" in file:\n",
    "#                     print(file)\n",
    "                    dictionary_new_labels = list_to_dict(merge_excel_to_list_new(file), dictionary, \"New\")\n",
    "                else:\n",
    "    #                 print(file)\n",
    "                    dictionary_old_labels = list_to_dict(merge_excel_to_list_old(file), dictionary, \"Old\")\n",
    "                    all_sents_old, all_labels_old = dictionary_to_final_lists(dictionary_old_labels, classifier)\n",
    "            if \"Rater1\" in path_in_str:\n",
    "                rater1 = dictionary_new_labels\n",
    "            elif \"Rater2\" in path_in_str:\n",
    "                rater2 = dictionary_new_labels          \n",
    "            elif \"Rater3\" in path_in_str:\n",
    "                rater3 = dictionary_new_labels\n",
    "\n",
    "all = {**rater1, **rater2, **rater3}\n",
    "\n",
    "\n",
    "incentive = \"incentive\" #write incentive if you want to work with policy instruments, write \"incentive\" to work with is_incentive\n",
    "merge1 = {}\n",
    "merge2 = {}\n",
    "merge3 = {}\n",
    "# classifier = incentive #If you want to classify by is_incentive\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "m = 0\n",
    "n = 0\n",
    "# First we look for sentences that all three raters have labeled the same\n",
    "for sent in all:\n",
    "    i += 1\n",
    "    if sent in rater1 and sent in rater2 and sent in rater3:\n",
    "        j += 1\n",
    "        if rater1[sent][incentive] == rater2[sent][incentive] and rater2[sent][incentive] == rater3[sent][incentive]:\n",
    "            k += 1\n",
    "            merge3[sent] = rater1[sent]\n",
    "\n",
    "#Now we look for the sentences that at least two raters have labeled the same\n",
    "for sent in all:\n",
    "    if sent in rater1 and sent in rater2:\n",
    "        l += 1\n",
    "        if rater1[sent][incentive] == rater2[sent][incentive]:\n",
    "            m += 1\n",
    "            merge2[sent] = rater1[sent]\n",
    "    elif sent in rater1 and sent in rater3:\n",
    "        l += 1\n",
    "        if rater1[sent][incentive] == rater3[sent][incentive]:\n",
    "            m += 1\n",
    "            merge2[sent] = rater1[sent]\n",
    "    elif sent in rater2 and sent in rater3:\n",
    "        l += 1\n",
    "        if rater2[sent][incentive] == rater3[sent][incentive]:\n",
    "            m += 1\n",
    "            merge2[sent] = rater2[sent]\n",
    "            \n",
    "# Finally we build a dataset containing the sentences that at least one of the labelers have labeled\n",
    "for sent in all:\n",
    "    label = {}\n",
    "    if sent in rater3:\n",
    "#         label[rater3[sent][incentive]] = \"rater3\"\n",
    "        merge1[sent] = rater3[sent]\n",
    "    elif sent in rater2:\n",
    "#         label[rater2[sent][incentive]] = \"rater2\"\n",
    "        merge1[sent] = rater2[sent]\n",
    "    elif sent in rater1:\n",
    "#         label[rater1[sent][incentive]] = \"rater1\"\n",
    "        merge1[sent] = rater1[sent]\n",
    "#     else:\n",
    "#         merge1[sent] = all[sent]\n",
    "\n",
    "print(f\"In the all-sentences dict there are {i} sentences\")\n",
    "print(f\"In the three raters lists there are {j} common sentences\")\n",
    "print(f\"In the three raters lists there are {k} common sentences which are rated identically\")\n",
    "print(f\"There are {l} sentences which are comon to at lest two rater's datasets\")\n",
    "print(f\"There are {m} sentences which are labeled identical in at least two rater's datasets\")\n",
    "print(len(merge3))\n",
    "print(len(merge2))\n",
    "print(len(merge1))\n",
    "\n",
    "merges = {\"Merge1\" : merge1, \"Merge2\" : merge2, \"Merge3\" : merge3}\n",
    "for merge in merges:\n",
    "    all_sents_new = []\n",
    "    all_labels_new = []\n",
    "    for sent in merges[merge]:\n",
    "        all_sents_new.append(merges[merge][sent][\"text\"])\n",
    "        all_labels_new.append(merges[merge][sent][incentive])\n",
    "    print(merge)\n",
    "    \n",
    "    if merge == \"Merge1\" and classifier == \"Binary\":\n",
    "        experiments = [\"EXP22\", \"EXP28\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP22\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP28\":\n",
    "                all_sents = all_sents_new + all_sents_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "                \n",
    "    if merge == \"Merge2\" and classifier == \"Binary\":\n",
    "        experiments = [\"EXP23\", \"EXP29\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP23\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP29\":\n",
    "                all_sents = all_sents_new + all_sents_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "                \n",
    "    if merge == \"Merge3\" and classifier == \"Binary\":\n",
    "        experiments = [\"EXP24\", \"EXP30\"]\n",
    "        for experiment in experiments:\n",
    "            if experiment == \"EXP24\":\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents_new, all_labels_new, test_size=test_perc, stratify=all_labels_new, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)\n",
    "            if experiment == \"EXP30\":\n",
    "                all_sents = all_sents_new + all_sents_old\n",
    "                all_labels = all_labels_new + all_labels_old\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
    "                save_data(X_train, y_train, X_test, y_test, experiment, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
