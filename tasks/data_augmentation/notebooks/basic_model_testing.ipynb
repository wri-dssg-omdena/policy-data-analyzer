{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a search engine based on sBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is a basic implementation of sBERT for searching a database of sentences with queries.\n",
    "\n",
    "The goal is to increase the amount of labeled data that we have in order to later fine tune a model to be used for sentence classification. First of all we have to find a pool of queries that represent the six labels of the six policy instruments. With these queries we can pull a set of sentences that can be automaticaly labeled with the same label of the query. In this way we can increase the diversity of labeled sentences in each label category. This approach will be complemented with a manual curation step to produce a high quality training data set.\n",
    "\n",
    "The policy instruments that we want to find and that correspond to the different labels are:\n",
    "* Direct payment (PES)\n",
    "* Tax deduction\n",
    "* Credit/guarantee\n",
    "* Technical assistance\n",
    "* Supplies\n",
    "* Fines\n",
    "\n",
    "This notebook is intended for the following purposes:\n",
    "* Try different query strategies to find the optimal retrieval of sentences in each policy instrument category\n",
    "* Try different transformers\n",
    "* Be the starting point for further enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is self contained, it does not depend on any other class of the sBERT folder.\n",
    "\n",
    "You just have to create an environment where you install the external dependencies. Usually the dependencies that you have to install are:\n",
    "\n",
    "**For the basic sentence similarity calculation**\n",
    "*  pandas\n",
    "*  boto3\n",
    "*  pytorch\n",
    "*  sentence_transformers\n",
    "\n",
    "**If you want to use ngrams to generate queries**\n",
    "*  nltk\n",
    "*  plotly\n",
    "*  wordcloud\n",
    "\n",
    "**If you want to do evaluation and ploting with pyplot**\n",
    "*  matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment is called nlp then you execute this cell otherwise you change the name of the environment\n",
    "!conda activate nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Model libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Libraries for model evaluation\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Libraries to be used in the process of definig queries\n",
    "import nltk # imports the natural language toolkit\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database from El Salvador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'JSON/Chile.json'\n",
    "\n",
    "obj = s3.Object('wri-latin-talent',filename)\n",
    "serializedObject = obj.get()['Body'].read()\n",
    "policy_list = json.loads(serializedObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going through the dictionary to retrieve sentences, we define a function to reduce de number of sentences in the final \"sentences\" dictionary. This is just for testing purposes. The reason being that running the sentence embedding function takes time. So for initial testing purposes we can reduce the number of sentences in the testing dataset.\n",
    "\n",
    "The variable \"slim_by\" is the reduction factor. If it is set to 1, there will be no reduction and we will be working with the full dataset. It it is set to two, we will take one every two sentences and so one.\n",
    "\n",
    "<span style=\"color:red\"><strong>REMEMBER</strong></span> that you have to re-run the function \"get_sentences_dict\" with the \"slim_by\" variable set to 1 when you want to go for the final shoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_dict(counter, slim_factor): # This is to shrink the sentences dict by a user set factor. It will pick only one sentence every \"slim_factor\"\n",
    "    if counter % slim_factor == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sentence_length_filter(sentence_text, minLength, maxLength):\n",
    "    if len(sentence_text) > minLength:#len(sentence_text) < maxLength and\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_sentences_dict(docs_dict, is_not_incentive_dict, slim_factor, minLength, maxLength):\n",
    "    count = 0\n",
    "    result = {}\n",
    "    for key, value in docs_dict.items():\n",
    "        for item in value: \n",
    "            if item in is_not_incentive_dict:\n",
    "                continue\n",
    "            else:\n",
    "                for sentence in docs_dict[key][item]['sentences']:\n",
    "                    if sentence_length_filter(docs_dict[key][item]['sentences'][sentence][\"text\"], minLength, maxLength):\n",
    "                        count += 1\n",
    "                        if slim_dict(count, slim_by):\n",
    "                            result[sentence] = docs_dict[key][item]['sentences'][sentence]\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will run the function to get your sentences list in a dictionary of this form:\n",
    "\n",
    "{\"\\<sentence id\\>\" : \"\\<text of the sentence\\>\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "#                     \"POR TANTO\" : 0,\n",
    "#                     \"DISPOSICIONES GENERALES\" : 0,\n",
    "#                     \"OBJETO\" : 0,\n",
    "#                     \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "                    \"POR TANTO\" : 0,\n",
    "                    \"DISPOSICIONES GENERALES\" : 0,\n",
    "                    \"OBJETO\" : 0,\n",
    "                    \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "\n",
    "slim_by = 1 # REMEMBER to set this variable to the desired value.\n",
    "min_length = 50 # Just to avoid short sentences which might be fragments or headings without a lot of value\n",
    "max_length = 250 # Just to avoid long sentences which might be artifacts or long legal jargon separated by semicolons\n",
    "\n",
    "sentences = get_sentences_dict(policy_list, is_not_incentive, slim_by, min_length, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this data set there are 4842 policies and 101907 sentences\n"
     ]
    }
   ],
   "source": [
    "# Just to check if the results look ok\n",
    "print(\"In this data set there are {} policies and {} sentences\".format(len(policy_list),len(sentences)))\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we use the excel file with the selected phrases of each country, process them and get N-grams to define basic queries for the SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'WRI_Policy_Tags (1).xlsx', sheet_name = None)\n",
    "df = None\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(df,pd.DataFrame):\n",
    "            df = value\n",
    "        else:\n",
    "            df = df.append(value)\n",
    "else:\n",
    "    df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = df[\"relevant sentences\"].apply(lambda x: x.split(\";\") if isinstance(x,str) else x)\n",
    "tagged_sentence = []\n",
    "\n",
    "for elem in tagged_sentences:\n",
    "    if isinstance(elem,float) or len(elem) == 0:\n",
    "        continue\n",
    "    elif isinstance(elem,list):\n",
    "        for i in elem:\n",
    "            if len(i.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                tagged_sentence.append(i.strip())\n",
    "    else:\n",
    "        if len(elem.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            tagged_sentence.append(elem.strip())\n",
    "\n",
    "tagged_sentence\n",
    "words_per_sentence = [len(x.split(\" \")) for x in tagged_sentence]\n",
    "plt.hist(words_per_sentence, bins = 50)\n",
    "plt.title(\"Histogram of number of words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "\n",
    "noise_words = []\n",
    "stopwords_corpus = nltk.corpus.stopword\n",
    "sp_stop_words = stopwords_corpus.words('spanish')\n",
    "noise_words.extend(sp_stop_words)\n",
    "print(len(noise_words))\n",
    "\n",
    "if \"no\" in noise_words:\n",
    "    noise_words.remove(\"no\")\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(''.join(tagged_sentence))\n",
    "word_freq = Counter(tokenized_words)\n",
    "# word_freq.most_common(20)\n",
    "# list(ngrams(tokenized_words, 3))\n",
    "\n",
    "word_tokens_clean = [re.findall(r\"[a-zA-Z]+\",each) for each in tokenized_words if each.lower() not in noise_words and len(each.lower()) > 1]\n",
    "word_tokens_clean = [each[0].lower() for each in word_tokens_clean if len(each)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the size of the n-gram that we want to find. The larger it is, the less frequent it will be, unless we substantially increase the number of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 2\n",
    "\n",
    "top_k_ngrams(word_tokens_clean, n_grams, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building queries with Parts-Of-Speech\n",
    "\n",
    "The following functions take a specific word and find the next or previous words according to the POS tags.\n",
    "\n",
    "An example is shown below with the text: <br>\n",
    "\n",
    "text = \"Generar empleo y garantizara la población campesina el bienestar y su participación e incorporación en el desarrollo nacional, y fomentará la actividad agropecuaria y forestal para el óptimo uso de la tierra, con obras de infraestructura, insumos, créditos, servicios de capacitación y asistencia técnica\" <br>\n",
    "\n",
    "next_words(text, \"empleo\", 3) <br>\n",
    "prev_words(text, \"garantizara\", 6) <br>\n",
    "\n",
    "Will return: <br>\n",
    "\n",
    ">['garantizara', 'población', 'campesina'] <br>\n",
    ">['Generar', 'empleo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = es_core_news_md.load()\n",
    "\n",
    "def ExtractInteresting(sentence, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "#     interesting = [k for k,v in nltk.pos_tag(words) if v in match]\n",
    "    doc = nlp(sentence)\n",
    "    interesting = [k.text for k in doc if k.pos_ in match]\n",
    "    return(interesting)\n",
    "\n",
    "def next_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "\n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        \n",
    "        pos_words = [k.text for k in doc[idx:] if k.pos_ in match]\n",
    "        if len(pos_words) > 1: \n",
    "            items.append(pos_words[1])\n",
    "            idx = text.index(pos_words[1])\n",
    "    \n",
    "    return items\n",
    "    \n",
    "def prev_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    \n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        pos_words = [k.text for k in doc[:idx] if k.pos_ in match]\n",
    "        if len(pos_words) >= 1: \n",
    "            items.insert(0, pos_words[-1]) #Add element in order and take the last element since it is the one before the word\n",
    "            idx = text.index(pos_words[-1])\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to find incentive policy instruments\n",
    "keywords = re.compile(r'(asistencia tecnica)|ayuda\\s*s*\\s*\\b|\\bbono\\s*s*\\b\\s*|credito\\s*s*\\b\\s*|incentivo\\s*s*\\b\\s*|insumo\\s*s*\\b\\s*|multa\\s*s*\\b\\s*')\n",
    "# deduccion\\s*(es)*\\b\\s*|devolucion\\s*(es)*\\b\\s*|\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "accents_out = re.compile(r'[áéíóúÁÉÍÓÚ]')\n",
    "accents_dict = {\"á\":\"a\",\"é\":\"e\",\"í\":\"i\",\"ó\":\"o\",\"ú\":\"u\",\"Á\":\"A\",\"É\":\"E\",\"Í\":\"I\",\"Ó\":\"O\",\"Ú\":\"U\"}\n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "# Dictionary to merge variants of a word\n",
    "families = {\n",
    "    \"asistencia tecnica\" : \"asistencia técnica\",\n",
    "    \"ayuda\" : \"ayuda\",\n",
    "    \"ayudas\" : \"ayuda\",\n",
    "    \"bono\" : \"bono\",\n",
    "    \"bonos\" : \"bono\",\n",
    "    \"credito\":  \"crédito\",\n",
    "    \"creditos\":  \"crédito\",\n",
    "#     \"deduccion\" : \"deducción\",\n",
    "#     \"deducciones\" : \"deducción\",\n",
    "#     \"devolucion\" : \"devolución\",\n",
    "#     \"devoluciones\" : \"devolución\",\n",
    "    \"incentivo\" : \"incentivo\",\n",
    "    \"incentivos\" : \"incentivo\",\n",
    "    \"insumo\" : \"insumo\",\n",
    "    \"insumos\" : \"insumo\",\n",
    "    \"multa\" : \"multa\",\n",
    "    \"multas\" : \"multa\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_in_sentences = []\n",
    "            \n",
    "for sentence in sentences:\n",
    "    line = remove_accents(sentences[sentence]['text'])\n",
    "    hit = keywords.search(line)\n",
    "    if hit:\n",
    "        keyword = hit.group(0).rstrip().lstrip()\n",
    "        keyword_in_sentences.append([families[keyword], sentence, sentences[sentence]['text']])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### print(len(keyword_in_sentences))\n",
    "# keyword_in_sentences = sorted(keyword_in_sentences, key = lambda x : x[0])\n",
    "# df_keyword_in_sentences = pd.DataFrame(keyword_in_sentences)\n",
    "\n",
    "# path = \"../output/\"\n",
    "# filename = \"keywords_match_labeling.csv\"\n",
    "# file = path + filename\n",
    "\n",
    "# df_keyword_in_sentences.to_csv(file)\n",
    "\n",
    "# print(keyword_in_sentences[0:20])\n",
    "filtered = [row for row in keyword_in_sentences if row[0] == \"incentivo\"]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key, value in families.items():\n",
    "    if i % 2 == 0:\n",
    "        print(value, \"--\", len([row for row in keyword_in_sentences if row[0] == value]))\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incentives = {}\n",
    "\n",
    "for incentive in families:\n",
    "    incentives[families[incentive]] = 0\n",
    "    \n",
    "incentives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "Then we build a simple function that takes four inputs:\n",
    "1. The model as we have set it in the previous line of code\n",
    "2. A dictionary that contains the sentences {\"\\<sentence_ID\\>\" : {\"text\" : \"The actual sentence\", labels : []}\n",
    "3. A query in the form of a string\n",
    "4. A similarity treshold. It is a float that we can use to limit the results list to the most relevant.\n",
    "\n",
    "The output of the function is a list with three columns with the following content:\n",
    "1. Column 1 contains the id of the sentence\n",
    "2. Column 2 contains the similarity score\n",
    "3. Column 3 contains the text of the sentence that has been compared with the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently two multi language models available for sentence similarity\n",
    "\n",
    "* xlm-r-bert-base-nli-stsb-mean-tokens: Produces similar embeddings as the bert-base-nli-stsb-mean-token model. Trained on parallel data for 50+ languages.\n",
    "<span style=\"color:red\"><strong>Attention!</strong></span> Model \"xlm-r-100langs-bert-base-nli-mean-tokens\" which was the name used in the original Omdena-challenge script has changed to this \"xlm-r-bert-base-nli-stsb-mean-tokens\"\n",
    "\n",
    "* distiluse-base-multilingual-cased-v2: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. While the original mUSE model only supports 16 languages, this multilingual knowledge distilled version supports 50+ languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to create the embeddings for each transformer the embeddings in a json with the following structure:\n",
    "# INPUT PARAMETERS\n",
    "# transformers: a list with transformer names\n",
    "# sentences_dict: a dictionary with the sentences of the database with the form {\"<sentence id>\" : \"<sentence text>\"}}\n",
    "# file: the filepath and filename of the output json\n",
    "# OUTPUT\n",
    "# the embeddings of the sentences in a json with the following structure:\n",
    "# {\"<transformer name>\" : {\"<sentence id>\" : <sentence embedding>}}\n",
    "\n",
    "def create_sentence_embeddings(transformers, sentences_dict, file):\n",
    "    embeddings = {}\n",
    "    for transformer_name in transformers:\n",
    "        model = SentenceTransformer(transformer_name)\n",
    "        embeddings[transformer_name] = {}\n",
    "        for sentence in sentences_dict:\n",
    "            embeddings[transformer_name][sentence] = [model.encode(sentences_dict[sentence]['text'].lower())]\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump(embeddings, fp, cls=NumpyArrayEncoder)\n",
    "     \n",
    "   \n",
    "def highlight(transformer_name, model, sentence_emb, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query.lower())\n",
    "    highlights = []\n",
    "    for sentence in sentence_emb[transformer_name]:\n",
    "        sentence_embedding = np.asarray(sentence_emb[transformer_name][sentence])[0]\n",
    "        score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "        if score > similarity_treshold:\n",
    "            highlights.append([sentence, score, sentences_dict[sentence]['text']])\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embeddings for sentences in the database\n",
    "\n",
    "This piece of code it's to be executed only once every time the database is chaged or we want to get the embeddings of a new database. For example, we are going to use it once for El Salvador policies and we don't need to use it again until we add new policies to this database. Instead, whenever we want to run experiments on this database, we will load the json files with the embeddings which are in the \"input\" folder.\n",
    "\n",
    "So, the next cell will be kept commented for safety reasons. Un comment it and execute it whenvere you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The building of a sentence embedding database for El Salvador in the two current models has taken 2755.4988 seconds\n"
     ]
    }
   ],
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "transformer_names =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "\n",
    "path = \"../input/\"\n",
    "filename = \"Embeddings_ElSalvador_201223.json\"\n",
    "file = path + filename\n",
    "\n",
    "create_sentence_embeddings(transformer_names, sentences, file)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"The building of a sentence embedding database for El Salvador in the two current models has taken {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the embeddings for database sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/\"\n",
    "filename = \"Embeddings_ElSalvador_201223.json\"\n",
    "file = path + filename\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    sentence_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic search with single test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load transformers into the model by choosing one model from index\n",
    "transformer_names =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "model_index = 0\n",
    "model = SentenceTransformer(transformer_names[model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, perform single query searches by manually writing a query in the corresponding field\n",
    "Ti = time.perf_counter()\n",
    "\n",
    "highlighter_query = \"La Policia al tener conocimiento de cualquier infraccion\"\n",
    "similarity_limit = 0.00\n",
    "\n",
    "label_1 = highlight(transformer_names[model_index], model, sentence_embeddings, sentences, highlighter_query, similarity_limit)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"similarity search for El Salvador sentences done in {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_1))\n",
    "label_1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(highlighter_query)\n",
    "label_1[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further filtering of the results by using the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_treshold = 0.5\n",
    "filtered = [row for row in label_1 if row[1] > similarity_treshold]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "export_query = pd.DataFrame(label_1)\n",
    "#export file \n",
    "export_query = pd.DataFrame(label_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiparameter search design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code is just to limit the amount of items in the incentives dictionary for testing purposes\n",
    "# The \"incentives\" dictionari contains the keywords that represent policy instruments. This is to be used in\n",
    "# the following cell where we make a search based on (1) the keywords themselves (2) the first sentence found in policy documents\n",
    "# with each of the keywords.\n",
    "\n",
    "# dicti = {}\n",
    "# i= 0\n",
    "# for key in incentives:\n",
    "#     if i < 2:\n",
    "#         dicti[key] = 0\n",
    "#     i += 1\n",
    "# incentives = dicti\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do in this experiment is check the capacity of two models:\n",
    "\n",
    "* xlm-r-bert-base-nli-stsb-mean-tokens\n",
    "* distiluse-base-multilingual-cased-v2\n",
    "\n",
    "to find policy instruments for incentives, based in 9 categories:\n",
    "\n",
    "asistencia técnica; ayuda; bono; crédito; deducción; devolución; incentivo; insumo and multa\n",
    "\n",
    "We will compare two approaches:\n",
    "1. to perform a search with the keyword itself\n",
    "2. to perform a search with one of the sentences found in El Salvador policies which contain the keyword.\n",
    "\n",
    "User set parameters:\n",
    "<strong>Transformer names:</strong> this is a list with the different models to test. There are currently two.\n",
    "\n",
    "<strong>Similarity limit:</strong> just to filter out the search matches whith low similarity.\n",
    "\n",
    "<strong>Number of search results:</strong> the search is against all 40.000 sentences in the database, but we don't want to keep all, just the most relevant so we take 1500 as the keyword with most direct matches is \"multa\" with some 1352 matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_queries = {\"asistencia técnica\" : \"00a55af_79\", \n",
    "             \"ayuda\" : \"00a55af_61\", \n",
    "             \"bono\" : \"00a55af_80\", \n",
    "             \"crédito\" : \"1cd36a0_11\", \n",
    "             \"incentivo\" : \"51a0d9e_30\",\n",
    "             \"insumo\" : \"731dbf0_11\",\n",
    "             \"multa\" : \"029d411_88\"\n",
    "}\n",
    "incentive = \"asistencia técnica\"\n",
    "[row for row in keyword_in_sentences if row[1] == S_queries[incentive]][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_names =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "similarity_limit = 0.2\n",
    "number_of_search_results = 10000\n",
    "\n",
    "S_queries : {\"asistencia técnica\" : \"00a55af_79\", \n",
    "             \"ayuda\" : \"00a55af_61\", \n",
    "             \"bono\" : \"00a55af_80\", \n",
    "             \"crédito\" : \"1cd36a0_11\", \n",
    "             \"incentivo\" : \"51a0d9e_30\",\n",
    "             \"insumo\" : \"731dbf0_11\",\n",
    "             \"multa\" : \"029d411_88\"\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for transformer in transformer_names:\n",
    "    model = SentenceTransformer(transformer)\n",
    "    results[transformer] = {}\n",
    "    for incentive in incentives:\n",
    "#         queries = [incentive, [row for row in keyword_in_sentences if row[0] == incentive][0][2]]\n",
    "        print(incentive)\n",
    "        queries = [incentive, [row for row in keyword_in_sentences if row[1] == S_queries[incentive]][0][2]]\n",
    "        Ti = time.perf_counter()\n",
    "        for query in queries:\n",
    "            similarities = highlight(transformer, model, sentence_embeddings, sentences, query, similarity_limit)\n",
    "            results[transformer][query] = similarities[0:number_of_search_results]\n",
    "            Tf = time.perf_counter()\n",
    "            print(f\"similarity search for model {transformer} and query {query} it's been done in {Tf - Ti:0.4f} seconds\")\n",
    "\n",
    "path = \"../output/\"\n",
    "filename = \"Experiment_201218_jordi_1500.json\"\n",
    "file = path + filename\n",
    "with open(file, 'w') as fp:\n",
    "    json.dump(results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a temporary section to explore how to analyze the results. It is organized with the same structure as the section <strong>Defining queries</strong> as we are exploring the best search strategies based on different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-speach approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the results and refactor data structures to better process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the results\n",
    "\n",
    "# path = \"../output/\"\n",
    "# filename = \"Experiment_201215_jordi_1500.json\"\n",
    "# file = path + filename\n",
    "\n",
    "# with open(file, \"r\") as f:\n",
    "#     experiment_results = json.load(f)\n",
    "\n",
    "experiment_results = results\n",
    "\n",
    "# Adding the rank to each result\n",
    "for model in experiment_results:\n",
    "    for keyword in experiment_results[model]:\n",
    "        i = 1\n",
    "        for result in experiment_results[model][keyword]:\n",
    "            result.append(i)\n",
    "            i += 1\n",
    "# Building a final dictionari of the results with a extra layer with sentence IDs as keys of the last layer\n",
    "experiment_results_full_dict = {}\n",
    "for model in experiment_results:\n",
    "    experiment_results_full_dict[model] = {}\n",
    "    i = 0\n",
    "    for keyword in experiment_results[model]:\n",
    "        if i % 2 == 0:\n",
    "            key = keyword + \"_K\"\n",
    "            experiment_results_full_dict[model][key] = {}\n",
    "            for result in experiment_results[model][keyword]:\n",
    "                experiment_results_full_dict[model][key][result[0]] = result[1:len(result)]\n",
    "        else:\n",
    "            key = key[0:-2] + \"_S\"\n",
    "            experiment_results_full_dict[model][key] = {}\n",
    "            for result in experiment_results[model][keyword]:\n",
    "                experiment_results_full_dict[model][key][result[0]] = result[1:len(result)]\n",
    "        i += 1\n",
    "            \n",
    "# Building a dictionary with all sentences found by exact keyword matching. The dictionary is of the form:\n",
    "# {\"<incetive>\" : [\"<sentence_id_1>\", ... \"<sentence_id_n>\"]}\n",
    "\n",
    "keyword_hits = {}\n",
    "for item in keyword_in_sentences:\n",
    "    if item[0] in keyword_hits:\n",
    "        keyword_hits[item[0]][item[1]] = []\n",
    "    else:\n",
    "        keyword_hits[item[0]] = {}\n",
    "        keyword_hits[item[0]][item[1]] = []      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_names =['xlm-r-bert-base-nli-stsb-mean-tokens', 'distiluse-base-multilingual-cased-v2']\n",
    "\n",
    "\n",
    "for incentive, sentence_list in keyword_hits.items():\n",
    "#     print(\"\\t\", incentive.center(25)\n",
    "    for sentence_id in sentence_list:\n",
    "        for model_name in transformer_names:\n",
    "            for key in experiment_results_full_dict[model_name]:\n",
    "                if incentive in key:\n",
    "                    if sentence_id in experiment_results_full_dict[model_name][key]:\n",
    "                        keyword_hits[incentive][sentence_id].append(experiment_results_full_dict[model_name][key][sentence_id][2])\n",
    "                        keyword_hits[incentive][sentence_id].append(round(experiment_results_full_dict[model_name][key][sentence_id][0], 2))\n",
    "                    else:\n",
    "                        keyword_hits[incentive][sentence_id].append(15000)\n",
    "                        keyword_hits[incentive][sentence_id].append(0.0)\n",
    "        i += 1\n",
    "#         for keyword in \n",
    "#             print(experiment_results_full_dict[model_name].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv = []\n",
    "for key, value in keyword_hits.items():\n",
    "    for sentence, res in value.items():\n",
    "        results_csv.append([key, sentence, res[0], res[1], res[2], res[3], res[4], res[5], res[6], res[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"keyword\", \"sentence_ID\", \"xlm_K-rank\", \"xlm_K-sim\", \"xlm_S-rank\", \"xlm_S-sim\", \"dist_K-rank\", \"dist_K-sim\", \"dist_S-rank\", \"dist_S-sim\"]\n",
    "df= pd.DataFrame(results_csv, columns = column_names)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../output/\"\n",
    "filename = \"Experiment_201217_jordi_1500.csv\"\n",
    "file = path + filename\n",
    "\n",
    "df.to_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the documents of selected sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "757px",
    "left": "586px",
    "top": "630px",
    "width": "309px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
