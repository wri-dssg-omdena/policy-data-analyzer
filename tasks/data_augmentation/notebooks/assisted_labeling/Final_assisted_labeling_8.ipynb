{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import boto3\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sentencepiece\n",
    "from scipy.spatial import distance\n",
    "from json import JSONEncoder\n",
    "import sys\n",
    "sys.path.append(\"/Users/dafirebanks/Projects/policy-data-analyzer/\")\n",
    "sys.path.append(\"C:/Users/jordi/Documents/GitHub/policy-data-analyzer/\")\n",
    "from tasks.data_loading.src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]\n",
    "\n",
    "def aws_credentials(path, filename):\n",
    "    file = path + filename\n",
    "    with open(file, 'r') as dict:\n",
    "        key_dict = json.load(dict)\n",
    "    for key in key_dict:\n",
    "        KEY = key\n",
    "        SECRET = key_dict[key]\n",
    "    return KEY, SECRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimized full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_credentials(path, filename):\n",
    "    file = path + filename\n",
    "    with open(file, 'r') as dict:\n",
    "        key_dict = json.load(dict)\n",
    "    for key in key_dict:\n",
    "        KEY = key\n",
    "        SECRET = key_dict[key]\n",
    "    return KEY, SECRET\n",
    "\n",
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]\n",
    "\n",
    "def load_all_sentences(language, s3, bucket_name, init_doc, end_doc):\n",
    "    policy_dict = {}\n",
    "    sents_folder = f\"{language}_documents/sentences\"\n",
    "    \n",
    "    for i, obj in enumerate(s3.Bucket(bucket_name).objects.all().filter(Prefix=\"english_documents/sentences/\")):\n",
    "        \n",
    "        if not obj.key.endswith(\"/\") and init_doc <= i < end_doc:\n",
    "            \n",
    "            serializedObject = obj.get()['Body'].read()\n",
    "            policy_dict = {**policy_dict, **json.loads(serializedObject)}\n",
    "            \n",
    "    return labeled_sentences_from_dataset(policy_dict)\n",
    "\n",
    "def save_results_as_separate_csv(results_dictionary, queries_dictionary, init_doc, results_limit, aws_id, aws_secret):\n",
    "    path = \"s3://wri-nlp-policy/english_documents/assisted_labeling\"\n",
    "    col_headers = [\"sentence_id\", \"similarity_score\", \"text\"]\n",
    "    for i, query in enumerate(results_dictionary.keys()):\n",
    "        filename = f\"{path}/query_{queries_dictionary[query]}_{i}_results_{init_doc}.csv\"\n",
    "        pd.DataFrame(results_dictionary[query], columns=col_headers).head(results_limit).to_csv(filename, storage_options={\"key\": aws_id, \"secret\": aws_secret})\n",
    "\n",
    "def labeled_sentences_from_dataset(dataset):\n",
    "    sentence_tags_dict = {}\n",
    "\n",
    "    for document in dataset.values():\n",
    "        sentence_tags_dict.update(document['sentences'])\n",
    "\n",
    "    return sentence_tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up AWS\n",
    "credentials_file = '/Users/dafirebanks/Documents/credentials.json'\n",
    "aws_id, aws_secret = aws_credentials_from_file(credentials_file)\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jordi/Documents/claus/\"\n",
    "filename = \"AWS_S3_keys_wri.json\"\n",
    "aws_id, aws_secret = aws_credentials(path, filename)\n",
    "region = 'us-east-1'\n",
    "\n",
    "bucket = 'wri-nlp-policy'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "# Define params\n",
    "init_at_doc = 13136\n",
    "end_at_doc = 14778\n",
    "\n",
    "similarity_threshold = 0\n",
    "search_results_limit = 500\n",
    "\n",
    "language = \"english\"\n",
    "bucket_name = 'wri-nlp-policy'\n",
    "\n",
    "transformer_name = 'xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "model = SentenceTransformer(transformer_name)\n",
    "\n",
    "\n",
    "# Get all sentence documents\n",
    "\n",
    "sentences = load_all_sentences(language, s3, bucket_name, init_at_doc, end_at_doc )\n",
    "\n",
    "# Define queries\n",
    "path = \"../../input/\"\n",
    "filename = \"English_queries.xlsx\"\n",
    "file = path + filename\n",
    "df = pd.read_excel(file, engine='openpyxl', sheet_name = \"Hoja1\", usecols = \"A:C\")\n",
    "\n",
    "queries = {}\n",
    "for index, row in df.iterrows():\n",
    "    queries[row['Query sentence']] = row['Policy instrument']\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and store query embeddings\n",
    "query_embeddings = dict(zip(queries, [model.encode(query.lower(), show_progress_bar=False) for query in queries]))\n",
    "\n",
    "# For each sentence, calculate its embedding, and store the similarity\n",
    "query_similarities = defaultdict(list)\n",
    "\n",
    "i = 0\n",
    "for sentence_id, sentence in sentences.items():\n",
    "    sentence_embedding = model.encode(sentence['text'].lower(), show_progress_bar=False)\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    for query_text, query_embedding in query_embeddings.items():\n",
    "        score = round(1 - distance.cosine(sentence_embedding, query_embedding), 4)\n",
    "        if score > similarity_threshold:\n",
    "            query_similarities[query_text].append([sentence_id, score, sentences[sentence_id]['text']])\n",
    "            \n",
    "# Sort results by similarity score\n",
    "for query in query_similarities:\n",
    "    query_similarities[query] = sorted(query_similarities[query], key = lambda x : x[1], reverse=True)\n",
    "    \n",
    "# Store results\n",
    "save_results_as_separate_csv(query_similarities, queries, init_at_doc, search_results_limit, aws_id, aws_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
