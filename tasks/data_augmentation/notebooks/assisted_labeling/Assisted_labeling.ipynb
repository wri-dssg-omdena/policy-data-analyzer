{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assisted labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is a basic implementation of sBERT for searching a database of sentences with queries.\n",
    "\n",
    "The goal is to increase the amount of labeled data that we have in order to later fine tune a model to be used for sentence classification. First of all we have to find a pool of queries that represent the six labels of the six policy instruments. With these queries we can pull a set of sentences that can be automaticaly labeled with the same label of the query. In this way we can increase the diversity of labeled sentences in each label category. This approach will be complemented with a manual curation step to produce a high quality training data set.\n",
    "\n",
    "The policy instruments that we want to find and that correspond to the different labels are:\n",
    "* Direct payment (PES)\n",
    "* Tax deduction\n",
    "* Credit/guarantee\n",
    "* Technical assistance\n",
    "* Supplies\n",
    "* Fines\n",
    "\n",
    "This notebook is intended for the following purposes:\n",
    "* Loading of a database of sentences\n",
    "* Create sentence embeddings for further processing\n",
    "* Define the queries corresponding to the labels\n",
    "* Compute the cosine similarity score between the embeddings of the sentences and those of the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is self contained, it does not depend on any other class of the sBERT folder.\n",
    "\n",
    "You just have to create an environment where you install the external dependencies. Usually the dependencies that you have to install are:\n",
    "\n",
    "**For the basic sentence similarity calculation**\n",
    "*  pandas\n",
    "*  boto3\n",
    "*  pytorch\n",
    "*  sentence_transformers\n",
    "\n",
    "**If you want to do evaluation and ploting with pyplot**\n",
    "*  matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment is called nlp then you execute this cell otherwise you change the name of the environment\n",
    "!conda activate NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import boto3\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import time\n",
    "# from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sentencepiece\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "# path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"AWS_S3_keys_Omdena.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in key_dict:\n",
    "#     KEY = key\n",
    "#     SECRET = key_dict[key]\n",
    "    \n",
    "KEY = \"AKIA4GK7IHHCYRVARVE6\"\n",
    "SECRET = \"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\"\n",
    "region = 'us-east-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database\n",
    "\n",
    "Here we merge all documents databases for each country in a single dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Old code for Omdena bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = ['Chile.json','ElSalvador.json']\n",
    "# filter_language = \"_ES\"\n",
    "# filter_folder = \"JSON/\" #TODO: move everything to a \"Sentences\" folder\n",
    "# filter_prefix = filter_folder + filter_language # TODO: change naming convention for files adding _ES or _En according to language. \n",
    "\n",
    "# policy_dict = {}\n",
    "# for file in filename:\n",
    "#     filter_prefix  = filter_folder + file\n",
    "#     for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix=filter_prefix):\n",
    "#     #     obj = s3.Object('wri-latin-talent',filename)\n",
    "#         serializedObject = obj.get()['Body'].read()\n",
    "#         policy_dict = {**policy_dict, **json.loads(serializedObject)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- New code for WRI bucket\n",
    "\n",
    "1. First, get the data from the documents belonging to a given country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 7238: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f473eb5056ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# THIS WORKS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://wri-nlp-policy/India_20210310.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"secret\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSECRET\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cp1252\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/wri_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wri_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wri_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wri_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wri_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 7238: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# THIS WORKS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "bro = pd.read_csv(\"s3://wri-nlp-policy/India_20210310.csv\", storage_options={\"key\": KEY, \"secret\": SECRET}, encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now, get the actual documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_documents/test_sentences/USA_398eb8d0-2418-4efc-948c-7bbd6e97e733_sents.json\n",
      "english_documents/test_sentences/USA_8d6b33e2-9af2-4ea2-83b2-9ff78477b521_sents.json\n",
      "english_documents/test_sentences/USA_922ac17b-f842-479d-b5ff-05e1debdebcd_sents.json\n"
     ]
    }
   ],
   "source": [
    "policy_dict = {}\n",
    "objs = []\n",
    "for obj in s3.Bucket('wri-nlp-policy').objects.all().filter(Prefix=\"english_documents/test_sentences/USA_\"):\n",
    "    print(obj.key)\n",
    "#     objs.append(obj.get()['Body'].read())\n",
    "    serializedObject = obj.get()['Body'].read()\n",
    "    policy_dict = {**policy_dict, **json.loads(serializedObject)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['398eb8d0-2418-4efc-948c-7bbd6e97e733', '8d6b33e2-9af2-4ea2-83b2-9ff78477b521', '922ac17b-f842-479d-b5ff-05e1debdebcd'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'n_sentences': 162, 'filename': 'Federal Register, Volume 85 Issue 190 (Wednesday, September 30, 2020).htm', 'format': 'htm', 'country': 'USA'}, 'sentences': [{'sentence_0': 'Federal Register, Volume 85 Issue 190 (Wednesday, September 30, 2020) [Federal Register Volume 85, Number 190 (Wednesday, September 30, 2020)] [Rules and Regulations] [Pages 61614-61619] From the Federal Register Online via the Government Publishing Office [[URL] [FR Doc No: 2020-19109] ======================================================================= ----------------------------------------------------------------------- DEPARTMENT OF THE INTERIOR Fish and Wildlife Service 50 CFR Part 17 [Docket No. FWS-R4-ES-2018-0074; FF09E21000 FXES11110900000 201] RIN 1018-BD43 Endangered and Threatened Wildlife and Plants; Section 4(d) Rule for Trispot Darter AGENCY: Fish and Wildlife Service, Interior.', 'label': []}, {'sentence_1': '----------------------------------------------------------------------- SUMMARY: We, the US Fish and Wildlife Service (Service), adopt a rule under section 4(d) of the Endangered Species Act of 1973 (Act), as amended, for the trispot darter (Etheostoma trisella), a fish from Alabama, Georgia, and Tennessee.', 'label': []}, {'sentence_2': 'This rule provides measures that are necessary and advisable to conserve the species.', 'label': []}, {'sentence_3': 'DATES: This rule is effective October 30, 2020.', 'label': []}, {'sentence_4': 'ADDRESSES: This final rule is available on the internet at [URL] under Docket No. FWS-R4-ES-2018-0074 and at [URL] Comments and materials we received, as well as supporting documentation we used in preparing this rule, are available for public inspection at [URL] under Docket No. FWS-R4-ES-2018-0074.', 'label': []}, {'sentence_5': 'FOR FURTHER INFORMATION CONTACT: William Pearson, Field Supervisor, US Fish and Wildlife Service, Alabama Ecological Services Field Office, 1208-B Main Street, Daphne, AL 36526; telephone 251-441-5870.', 'label': []}, {'sentence_6': 'Persons who use a telecommunications device for the deaf (TDD) may call the Federal Relay Service at 800-877-8339.', 'label': []}, {'sentence_7': 'SUPPLEMENTARY INFORMATION: Previous Federal Actions On October 4, 2017, we published in the Federal Register (82 FR 46183) a proposed rule to list the trispot darter as a threatened species under the Act (16 USC 1531 et seq).', 'label': []}, {'sentence_8': \"On December 28, 2018, we published the following documents in the Federal Register: (1) The final rule listing the trispot darter as a threatened species (83 FR 67131), (2) the proposed rule to provide measures necessary and advisable to conserve the species under section 4(d) of the Act (a ``4(d) rule'') for the species (83 FR 67185), and (3) the proposed rule to designate critical habitat for the species (83 FR 67190).\", 'label': []}, {'sentence_9': \"Elsewhere in today's Federal Register, we issue a final rule to designate critical habitat for the trispot darter.\", 'label': []}, {'sentence_10': 'Please see these documents for additional previous Federal actions affecting the trispot darter.', 'label': []}, {'sentence_11': 'Summary of Changes From the Proposed Rule This final rule incorporates one change to our proposed rule based on the comments we received.', 'label': []}, {'sentence_12': \"Specifically, we replaced the term ``highest-standard best management practices'' with the term ``State best management practices'' In addition, in this rule, we change the way in which the provisions of the 4(d) rule for the trispot darter appear in 50 CFR 1744 from what we proposed; here, we do not refer to the prohibitions and provisions set forth in section 9(a)(1) of the Act.\", 'label': []}, {'sentence_13': 'Instead, we refer to the prohibitions set forth at 50 CFR 1721, which apply to endangered species.', 'label': []}, {'sentence_14': 'However, the substance of the prohibitions, and exceptions to those prohibitions, in the 4(d) rule for the trispot darter have not changed.', 'label': []}, {'sentence_15': 'Background The trispot darter is a small-bodied, freshwater fish found in the Coosa River System, above the fall line in the Ridge and Valley of Alabama, Georgia, and Tennessee.', 'label': []}, {'sentence_16': 'It is a migratory species that uses distinct breeding and nonbreeding habitats.', 'label': []}, {'sentence_17': 'From approximately April to October, the species inhabits its nonbreeding habitat, which consists of small to medium river margins and lower reaches of tributaries with slower velocities.', 'label': []}, {'sentence_18': 'After October, trispot darters move from the main channels into tributaries, eventually reaching adjacent seepage areas where they congregate and remain for the duration of spawning, until approximately late April.', 'label': []}, {'sentence_19': 'Breeding sites are intermittent seepage areas and ditches with little to no flow and shallow depths (12 inches (30 centimeters) or less).', 'label': []}, {'sentence_20': 'For a full summary of species information, please refer to the October 4, 2017, proposed listing rule (82 FR 46183) and the species status assessment (SSA) report for the trispot darter (Service 2018, entire).', 'label': []}, {'sentence_21': \"Both documents are available at [URL] under Docket No. FWS-R4-ES-2017-0063, and on the Service's South Atlantic-Gulf Region website at [URL] [[Page 61615]] Summary of Comments and Recommendations On December 28, 2018, we proposed a 4(d) rule for the trispot darter (83 FR 67185).\", 'label': []}, {'sentence_22': 'We accepted public comments on the proposed 4(d) rule for 60 days, ending February 26, 2019.', 'label': []}, {'sentence_23': 'During the comment period, we received 13 comments addressing the proposed 4(d) rule.', 'label': []}, {'sentence_24': 'Eight of the comments supported the general protections of the proposed 4(d) rule, and five explicitly expressed support for our conservation strategy of sustainable forest management and best management practices.', 'label': []}, {'sentence_25': 'None of the comments opposed the proposed 4(d) rule.', 'label': []}, {'sentence_26': 'All substantive information provided during the comment period has either been incorporated directly into this final rule or is addressed in our responses below.', 'label': []}, {'sentence_27': 'State Comments Comment: The Alabama Forestry Commission commented that limiting silvicultural and forest management activities to May 1 through December 31 is concerning and may be unnecessary with BMP compliance.', 'label': []}, {'sentence_28': 'Our Response: The 4(d) rule identifies actions that are prohibited in order to protect the darter, as well as actions that are excluded from those prohibitions, including certain forest management activities.', 'label': []}, {'sentence_29': \"Because trispot darters spawn from January through April, making this the most sensitive period of the species' lifecycle, the exclusions for forest management activities in spawning habitat apply only from May 1 through December 31.\", 'label': []}, {'sentence_30': 'During the spawning period, the exclusions do not apply in areas where spawning habitat is present; however, in non-spawning habitat, the exclusions apply year-round.', 'label': []}, {'sentence_31': 'In some cases, silvicultural and forest management activities may still be undertaken in areas that are spawning habitat during the spawning season, as long as there is consultation with the Service under section 7 of the Act or a conservation agreement under section 10 of the Act.', 'label': []}, {'sentence_32': 'Performing silvicultural and forest management activities in the range of the trispot darter between May 1 and December 31, while applying State best management practices, will not adversely affect, and may provide conservation benefits for, the species.', 'label': []}, {'sentence_33': \"Public Comments Comment: In regard to silviculture practices or forest management activities, we received three public comments and one comment from the Alabama Forestry Commission concerning our use of the term ``highest- standard best management practices'' Specifically, one public commenter requested clarification of the term, and two other public commenters requested amending the term to ``State best management practices'' The State agency commented that Alabama's best management practices for forestry clearly state that stream management zones, stream crossings, and forest roads must always be sufficient in design and filtering capacity to not impact water quality and passage of aquatic species.\", 'label': []}, {'sentence_34': \"Thus, complying with Alabama's best management practices should ensure water quality and the reference for the need to ``implement the highest-standard best management practices'' is not needed.\", 'label': []}, {'sentence_35': 'Our Response: Best management practices can change over time as new scientific and commercial information becomes available.', 'label': []}, {'sentence_36': \"Therefore, rather than specifying a particular set of best management practices, we interpreted ``highest-standard best management practices'' to refer to the most stringent ones available at the time of project implementation.\", 'label': []}, {'sentence_37': \"To clarify the terminology, we removed the term ``highest-standard'' and now refer to these practices (the most stringent ones currently available) as ``State best management practices,'' which constitute the highest standard.\", 'label': []}, {'sentence_38': 'Final Rule Issued Under Section 4(d) of the Act Background Section 4(d) of the Act contains two sentences.', 'label': []}, {'sentence_39': \"The first sentence states that the ``Secretary shall issue such regulations as he deems necessary and advisable to provide for the conservation'' of species listed as threatened.\", 'label': []}, {'sentence_40': \"The US Supreme Court has noted that statutory language like ``necessary and advisable'' demonstrates a large degree of deference to the agency (see Webster v. Doe, 486 US 592 (1988)).\", 'label': []}, {'sentence_41': \"Conservation is defined in the Act to mean ``the use of all methods and procedures which are necessary to bring any endangered species or threatened species to the point at which the measures provided pursuant to [the Act] are no longer necessary'' Additionally, the second sentence of section 4(d) of the Act states that the Secretary ``may by regulation prohibit with respect to any threatened species any act prohibited under section 9(a)(1), in the case of fish or wildlife, or 9(a)(2), in the case of plants'' Thus, the combination of the two sentences of section 4(d) of the Act provide the Secretary with wide latitude of discretion to select and promulgate appropriate regulations tailored to the specific conservation needs of the threatened species.\", 'label': []}, {'sentence_42': 'The second sentence grants particularly broad discretion to the Service when adopting the prohibitions under section 9.', 'label': []}, {'sentence_43': \"The courts have recognized the extent of the Secretary's discretion under this standard to develop rules that are appropriate for the conservation of a species.\", 'label': []}, {'sentence_44': 'For example, courts have upheld rules developed under section 4(d) as a valid exercise of agency authority where they prohibited take of threatened wildlife, or include a limited taking prohibition (see Alsea Valley Alliance v. Lautenbacher, 2007 US Dist. Lexis 60203 (D. Or 2007); Washington Environmental Council v. National Marine Fisheries Service, 2002 US Dist. Lexis 5432 (WD Wash 2002)).', 'label': []}, {'sentence_45': 'Courts have also upheld 4(d) rules that do not address all of the threats a species faces (see State of Louisiana v. Verity, 853 F2d 322 (5th Cir 1988)).', 'label': []}, {'sentence_46': 'As noted in the legislative history when the Act was initially enacted, ``once an animal is on the threatened list, the Secretary has an almost infinite number of options available to him with regard to the permitted activities for those species.', 'label': []}, {'sentence_47': \"He may, for example, permit taking, but not importation of such species, or he may choose to forbid both taking and importation but allow the transportation of such species'' (HR Rep. No 412, 93rd Cong, 1st Sess 1973).\", 'label': []}, {'sentence_48': \"Exercising its authority under section 4(d), the Service has developed a final rule for the trispot darter that is designed to address the species' specific threats and conservation needs.\", 'label': []}, {'sentence_49': \"Although the statute does not require the Service to make a ``necessary and advisable'' finding with respect to the adoption of specific prohibitions under section 9, we find that this final 4(d) rule as a whole satisfies the requirement in section 4(d) of the Act to issue regulations deemed necessary and advisable to provide for the conservation of the trispot darter.\", 'label': []}, {'sentence_50': 'As discussed in the final listing rule (83 FR 67131; December 28, 2018), the Service has concluded that the trispot darter is likely to become in danger of extinction within the foreseeable future primarily due to threats that degrade instream habitat and reduce water quality and water quantity, all which reduce connectivity between populations.', 'label': []}, {'sentence_51': 'The provisions of this final 4(d) rule promote conservation of the trispot darter by encouraging management of stream systems and the landscapes they drain while also meeting land use management considerations.', 'label': []}, {'sentence_52': 'The provisions of this final 4(d) rule are one of many tools that the Service will use [[Page 61616]] to promote the conservation of the trispot darter.', 'label': []}, {'sentence_53': 'Provisions of the 4(d) Rule for Trispot Darter This final 4(d) rule provides for the conservation of the trispot darter by prohibiting the following activities, except as otherwise authorized or permitted: Importing or exporting; take; possession and other acts with unlawfully taken specimens; delivering, receiving, transporting, or shipping in interstate or foreign commerce in the course of commercial activity; and selling or offering for sale in interstate or foreign commerce.', 'label': []}, {'sentence_54': 'We also include several standard exclusions to these prohibitions, which are set forth under Regulation Promulgation, below, as well as some species-specific exclusions.', 'label': []}, {'sentence_55': 'As discussed in the final listing rule (83 FR 67131) a range of natural and anthropogenic factors that affect aquatic systems may impact the status of the trispot darter.', 'label': []}, {'sentence_56': 'The largest threat to the future viability of the species is habitat degradation from stressors that influence four habitat elements: water quality, water quantity, instream habitat, and habitat connectivity.', 'label': []}, {'sentence_57': 'These stressors include hydrologic alteration, sedimentation, loss of connectivity, loss of riparian vegetation, contaminants entering the water system due to agricultural activities (such as excessive poultry litter and livestock entering streams), and urbanization within the watersheds inhabited by the species.', 'label': []}, {'sentence_58': 'Regulating these activities would reduce their combined negative effects, providing for the conservation of the trispot darter by helping to preserve remaining populations.', 'label': []}, {'sentence_59': 'Conservation actions that benefit the trispot darter include habitat restoration and protection.', 'label': []}, {'sentence_60': 'Additionally, conservation may be achieved through augmentation of populations to increase their size (number of individuals), which increases resiliency to adverse events such as storms and droughts, inadvertent runoff of pollutants and sediment, and contaminant spills.', 'label': []}, {'sentence_61': \"This rule provides exceptions from the Act's incidental take prohibitions, accommodating species restoration efforts by State wildlife agencies, channel restoration projects, and streambank stabilization projects.\", 'label': []}, {'sentence_62': 'Further, this rule enhances habitat protection, by providing exceptions to incidental take provisions for silviculture and forest management that implement best management practices; transportation projects that provide for fish passage in waters occupied by the trispot darter; and projects carried out under the Working Lands for Wildlife program of the Natural Resources Conservation Service, US Department of Agriculture.', 'label': []}, {'sentence_63': \"The provisions in this rule for channel restoration and habitat protection can occur only between May 1 and December 31, to avoid the trispot darter's spawning period, when seasonal spawning areas are wetted.\", 'label': []}, {'sentence_64': 'This curtails the likelihood of incidental take occurring.', 'label': []}, {'sentence_65': 'Although the exceptions for certain activities may result in some minimal level of harm or temporary disturbance to the trispot darter, overall, these activities benefit the species by contributing to conservation and recovery.', 'label': []}, {'sentence_66': 'The provisions in this rule are necessary and advisable because the species needs active conservation to improve the quality of its habitat and, absent protections, the species is likely to become in danger of extinction within the foreseeable future.', 'label': []}, {'sentence_67': 'These provisions can encourage cooperation by landowners and other affected parties in implementing conservation measures.', 'label': []}, {'sentence_68': 'This allows for use of the land while at the same time ensuring the preservation of suitable habitat and minimizing impact on the species.', 'label': []}, {'sentence_69': \"Under the Act, ``take'' means to harass, harm, pursue, hunt, shoot, wound, kill, trap, capture, or collect, or to attempt to engage in any such conduct.\", 'label': []}, {'sentence_70': 'Some of these provisions have been further defined in regulation at 50 CFR 173.', 'label': []}, {'sentence_71': 'Take can result knowingly or otherwise, by direct and indirect impacts, intentionally or incidentally.', 'label': []}, {'sentence_72': \"Regulating intentional and incidental take under this 4(d) rule will help preserve the species' remaining populations; enable beneficial management actions to occur; and decrease synergistic, negative effects from other stressors.\", 'label': []}, {'sentence_73': 'We may issue permits to carry out otherwise prohibited activities, including those described above, involving threatened wildlife under certain circumstances.', 'label': []}, {'sentence_74': 'Regulations governing permits are codified at 50 CFR 1732.', 'label': []}, {'sentence_75': 'With regard to threatened wildlife, a permit may be issued for the following purposes: For scientific purposes, to enhance propagation or survival, for economic hardship, for zoological exhibition, for educational purposes, for incidental taking, or for special purposes consistent with the purposes of the Act.', 'label': []}, {'sentence_76': 'There are also certain statutory exemptions from the prohibitions, which are found in sections 9 and 10 of the Act.', 'label': []}, {'sentence_77': 'The Service recognizes the special and unique relationship with our State natural resource agency partners in contributing to conservation of listed species.', 'label': []}, {'sentence_78': 'State agencies often possess scientific data and valuable expertise on the status and distribution of endangered, threatened, and candidate species of wildlife and plants.', 'label': []}, {'sentence_79': 'State agencies, because of their authorities and their close working relationships with local governments and landowners, are in a unique position to assist the Services in implementing all aspects of the Act.', 'label': []}, {'sentence_80': 'In this regard, section 6 of the Act provides that the Services shall cooperate to the maximum extent practicable with the States in carrying out programs authorized by the Act.', 'label': []}, {'sentence_81': 'Therefore, any qualified employee or agent of a State conservation agency that is a party to a cooperative agreement with the Service in accordance with section 6(c) of the Act, who is designated by his or her agency for such purposes, would be able to conduct activities designed to conserve trispot darter that may result in otherwise prohibited take without additional authorization.', 'label': []}, {'sentence_82': 'Nothing in this 4(d) rule changes in any way the recovery planning provisions of section 4(f) of the Act, the consultation requirements under section 7 of the Act, or the ability of the Service to enter into partnerships for the management and protection of the trispot darter.', 'label': []}, {'sentence_83': 'However, interagency cooperation may be further streamlined through planned programmatic consultations for the species between Federal agencies and the Service.', 'label': []}, {'sentence_84': 'Required Determinations Regulatory Planning and Review (Executive Orders 12866 and 13563) Executive Order (EO) 12866 provides that the Office of Information and Regulatory Affairs (OIRA) in the Office of Management and Budget will review all significant rules.', 'label': []}, {'sentence_85': 'OIRA has determined that this rule is not significant.', 'label': []}, {'sentence_86': \"EO 13563 reaffirms the principles of EO 12866 while calling for improvements in the nation's regulatory system to promote predictability, to reduce uncertainty, and to use the best, most innovative, and least burdensome tools for achieving regulatory ends.\", 'label': []}, {'sentence_87': 'The executive order directs agencies to consider regulatory approaches that reduce burdens and maintain flexibility and freedom of choice for the public where these approaches are relevant, feasible, and consistent with regulatory objectives.', 'label': []}, {'sentence_88': 'EO 13563 emphasizes further that regulations must be based on the best available science and that the rulemaking process must allow for public participation and an open exchange of ideas.', 'label': []}, {'sentence_89': 'We have developed [[Page 61617]] this final 4(d) rule in a manner consistent with these requirements.', 'label': []}, {'sentence_90': 'Regulatory Flexibility Act (5 USC 601 et seq) Under the Regulatory Flexibility Act (RFA; 5 USC 601 et seq), as amended by the Small Business Regulatory Enforcement Fairness Act of 1996 (SBREFA; 5 USC 801 et seq), whenever an agency is required to publish a notice of rulemaking for any proposed or final rule, it must prepare and make available for public comment a regulatory flexibility analysis that describes the effects of the rule on small entities (ie, small businesses, small organizations, and small government jurisdictions).', 'label': []}, {'sentence_91': 'However, no regulatory flexibility analysis is required if the head of the agency certifies the rule will not have a significant economic impact on a substantial number of small entities.', 'label': []}, {'sentence_92': 'The SBREFA amended the RFA to require Federal agencies to provide a certification statement of the factual basis for certifying that the rule will not have a significant economic impact on a substantial number of small entities.', 'label': []}, {'sentence_93': \"Thus, for a regulatory flexibility analysis to be required, impacts must exceed a threshold for ``significant impact'' and a threshold for a ``substantial number of small entities'' See 5 USC 605(b).\", 'label': []}, {'sentence_94': 'Based on the information that is available to us at this time, we certify that this rule will not have a significant economic impact on a substantial number of small entities.', 'label': []}, {'sentence_95': 'The following discussion explains our rationale.', 'label': []}, {'sentence_96': 'On December 28, 2018, we published the final rule listing the trispot darter as a threatened species (83 FR 67131).', 'label': []}, {'sentence_97': 'In this issue of the Federal Register, we publish (1) this final rule to establish a 4(d) rule for the trispot darter, and (2) a final rule designating critical habitat for the species.', 'label': []}, {'sentence_98': 'Any economic impacts resulting from these three final rules under the Act stem from listing the trispot darter and designating its critical habitat rather than this 4(d) rule.', 'label': []}, {'sentence_99': 'This 4(d) rule will not add to any costs due to section 7 consultation for the species and its critical habitat because, while this rule establishes prohibitions on acts with regard to the trispot darter, it also provides exceptions to those prohibitions.', 'label': []}, {'sentence_100': 'These exceptions will reduce the amount of time, and therefore costs, to complete consultations because the effects of the activities excepted by the 4(d) rule will not require analysis during those consultations.', 'label': []}, {'sentence_101': 'Therefore, a final regulatory flexibility analysis is not required.', 'label': []}, {'sentence_102': \"Executive Order 13771 This rule is not an Executive Order (EO) 13771 (``Reducing Regulation and Controlling Regulatory Costs'') (82 FR 9339, February 3, 2017) regulatory action because this rule is not significant under EO 12866.\", 'label': []}, {'sentence_103': 'Energy Supply, Distribution or Use (Executive Order 13211) Executive Order 13211 (Actions Concerning Regulations That Significantly Affect Energy Supply, Distribution, or Use) requires agencies to prepare Statements of Energy Effects when undertaking actions that significantly affect energy supply, distribution, or use.', 'label': []}, {'sentence_104': 'This rule will not have any significant effect, nor is it likely to have any effect, on energy supplies, distribution, or use.', 'label': []}, {'sentence_105': 'Therefore, this action is not a significant energy action, and no Statement of Energy Effects is required.', 'label': []}, {'sentence_106': 'Unfunded Mandates Reform Act (2 USC 1501 et seq) In accordance with the Unfunded Mandates Reform Act (2 USC 1501 et seq), we make the following finding: This rule would not produce a Federal mandate.', 'label': []}, {'sentence_107': \"In general, a Federal mandate is a provision in legislation, statute, or regulation that would impose an enforceable duty upon State, local, or tribal governments, or the private sector, and includes both ``Federal intergovernmental mandates'' and ``Federal private sector mandates'' These terms are defined in 2 USC 658(5)-(7).\", 'label': []}, {'sentence_108': \"``Federal intergovernmental mandate'' includes a regulation that ``would impose an enforceable duty upon State, local, or tribal governments'' with two exceptions.\", 'label': []}, {'sentence_109': \"It excludes ``a condition of Federal assistance'' It also excludes ``a duty arising from participation in a voluntary Federal program,'' unless the regulation ``relates to a then-existing Federal program under which $500,000,000 or more is provided annually to State, local, and tribal governments under entitlement authority,'' if the provision would ``increase the stringency of conditions of assistance'' or ``place caps upon, or otherwise decrease, the Federal Government's responsibility to provide funding,'' and the State, local, or tribal governments ``lack authority'' to adjust accordingly.\", 'label': []}, {'sentence_110': 'At the time of enactment, these entitlement programs were: Medicaid; Aid to Families with Dependent Children work programs; Child Nutrition; Food Stamps; Social Services Block Grants; Vocational Rehabilitation State Grants; Foster Care, Adoption Assistance, and Independent Living; Family Support Welfare Services; and Child Support Enforcement.', 'label': []}, {'sentence_111': \"``Federal private sector mandate'' includes a regulation that ``would impose an enforceable duty upon the private sector, except (i) a condition of Federal assistance or (ii) a duty arising from participation in a voluntary Federal program'' This rule will not impose an unfunded mandate on State, local, or tribal governments, or the private sector of more than $100 million per year.\", 'label': []}, {'sentence_112': 'The rule will not have a significant or unique effect on State, local, or tribal governments or the private sector.', 'label': []}, {'sentence_113': 'A statement containing the information required by the Unfunded Mandates Reform Act (2 USC 1501 et seq) is not required.', 'label': []}, {'sentence_114': 'Takings--Executive Order 12630 In accordance with Executive Order 12630 (Government Actions and Interference with Constitutionally Protected Private Property Rights), this rule does not have significant takings implications.', 'label': []}, {'sentence_115': 'We have determined that the rule has no potential takings of private property implications as defined by this Executive Order because this 4(d) rule, with limited exceptions, maintains the regulatory status quo regarding activities currently allowed under the Endangered Species Act.', 'label': []}, {'sentence_116': 'A takings implication assessment is not required.', 'label': []}, {'sentence_117': 'Federalism--Executive Order 13132 In accordance with EO 13132 (Federalism), this 4(d) rule does not have significant Federalism effects.', 'label': []}, {'sentence_118': 'A federalism summary impact statement is not required.', 'label': []}, {'sentence_119': 'This rule will not have substantial direct effects on the States, on the relationship between the Federal government and the States, or on the distribution of powers and responsibilities among the various levels of government.', 'label': []}, {'sentence_120': 'Civil Justice Reform--Executive Order 12988 In accordance with Executive Order 12988 (Civil Justice Reform), the Office of the Solicitor has determined that the rule does not unduly burden the judicial system and that it meets the requirements of sections 3(a) and 3(b)(2) of the Order.', 'label': []}, {'sentence_121': 'We issue this 4(d) rule in accordance with the provisions of the Act.', 'label': []}, {'sentence_122': 'To assist the public in understanding the conservation needs of the species, the rule identifies the prohibitions and exclusions to those prohibitions that are necessary and advisable to the conservation of the species.', 'label': []}, {'sentence_123': '[[Page 61618]] Paperwork Reduction Act of 1995 (44 USC 3501 et seq) This rule does not contain information collection requirements, and a submission to the Office of Management and Budget (OMB) under the Paperwork Reduction Act of 1995 (44 USC 3501 et seq) is not required.', 'label': []}, {'sentence_124': 'We may not conduct or sponsor and you are not required to respond to a collection of information unless it displays a currently valid OMB control number.', 'label': []}, {'sentence_125': 'National Environmental Policy Act (42 USC 4321 et seq) We have prepared a final environmental assessment, as defined under the authority of the National Environmental Policy Act of 1969.', 'label': []}, {'sentence_126': 'For information on how to obtain a copy of the final environmental assessment, see ADDRESSES, above.', 'label': []}, {'sentence_127': \"Government-to-Government Relationships With Tribes In accordance with the President's memorandum of April 29, 1994 (Government-to-Government Relations with Native American Tribal Governments; 59 FR 22951), Executive Order 13175 (Consultation and Coordination with Indian Tribal Governments), and the Department of the Interior's manual at 512 DM 2, we readily acknowledge our responsibility to communicate meaningfully with recognized Federal Tribes on a government-to-government basis.\", 'label': []}, {'sentence_128': 'In accordance with Secretarial Order 3206 of June 5, 1997 (American Indian Tribal Rights, Federal-Tribal Trust Responsibilities, and the Endangered Species Act), we readily acknowledge our responsibilities to work directly with tribes in developing programs for healthy ecosystems, to acknowledge that tribal lands are not subject to the same controls as Federal public lands, to remain sensitive to Indian culture, and to make information available to tribes.', 'label': []}, {'sentence_129': 'We have determined that no tribal interests will be affected by this rule.', 'label': []}, {'sentence_130': 'References Cited A complete list of references cited in this rulemaking is available on the internet at [URL] and upon request from the Alabama Ecological Services Field Office (see FOR FURTHER INFORMATION CONTACT).', 'label': []}, {'sentence_131': \"Authors The primary authors of this rule are the staff members of the Fish and Wildlife Service's Species Assessment Team and the Alabama Ecological Services Field Office.\", 'label': []}, {'sentence_132': 'List of Subjects in 50 CFR Part 17 Endangered and threatened species, Exports, Imports, Reporting and recordkeeping requirements, Transportation.', 'label': []}, {'sentence_133': 'Regulation Promulgation Accordingly, we amend part 17, subchapter B of chapter I, title 50 of the Code of Federal Regulations, as follows: PART 17--ENDANGERED AND THREATENED WILDLIFE AND PLANTS 0 1.', 'label': []}, {'sentence_134': 'The authority citation for part 17 continues to read as follows: Authority: 16 USC 1361-1407; 1531-1544; and 4201-4245, unless otherwise noted 0 2.', 'label': []}, {'sentence_135': \"Amend Sec 1711 in paragraph (h) by revising the entry for ``Darter, trispot'' under FISHES in the List of Endangered and Threatened Wildlife to read as set forth below.\", 'label': []}, {'sentence_136': 'Sec 1711 Endangered and threatened wildlife.', 'label': []}, {'sentence_137': '* * * * * (h) * * * ---------------------------------------------------------------------------------------------------------------- Listing citations and Common name Scientific name Where listed Status applicable rules ---------------------------------------------------------------------------------------------------------------- * * * * * * * Fishes * * * * * * * Darter, trispot.', 'label': []}, {'sentence_138': 'T 83 FR 67131, 12/28/ trisella 2018; 50 CFR 1744(q); \\\\4d\\\\ 50 CFR 1795(e)\\\\CH\\\\ * * * * * * * ---------------------------------------------------------------------------------------------------------------- 0 3.', 'label': []}, {'sentence_139': 'Amend Sec 1744 by adding paragraph (q) to read as follows: Sec 1744 Special rules--fishes.', 'label': []}, {'sentence_140': '* * * * * (q) Trispot darter (Etheostoma trisella).', 'label': []}, {'sentence_141': 'The following prohibitions that apply to endangered wildlife also apply to the trispot darter.', 'label': []}, {'sentence_142': 'Except as provided under paragraph (q)(2) of this section and Sec. Sec 174 and 175, it is unlawful for any person subject to the jurisdiction of the United States to commit, to attempt to commit, to solicit another to commit, or cause to be committed, any of the following acts in regard to the trispot darter: (i) Import or export, as set forth at Sec 1721(b) for endangered wildlife.', 'label': []}, {'sentence_143': '(ii) Take, as set forth at Sec 1721(c)(1) for endangered wildlife.', 'label': []}, {'sentence_144': '(iii) Possession and other acts with unlawfully taken specimens, as set forth at Sec 1721(d)(1) for endangered wildlife.', 'label': []}, {'sentence_145': '(iv) Interstate or foreign commerce in the course of commercial activity, as set forth at Sec 1721(e) for endangered wildlife.', 'label': []}, {'sentence_146': '(v) Sale or offer for sale, as set forth at Sec 1721(f) for endangered wildlife.', 'label': []}, {'sentence_147': 'In regard to this species, you may: (i) Conduct activities as authorized by a permit issued under Sec 1732.', 'label': []}, {'sentence_148': '(ii) Take, as set forth at Sec 1721(c)(2) through (c)(4) for endangered wildlife.', 'label': []}, {'sentence_149': '(iii) Take, as set forth at Sec 1731(b).', 'label': []}, {'sentence_150': '(iv) Take incidental to an otherwise lawful activity caused by: (A) Species restoration efforts by State wildlife agencies, including collection of broodstock, tissue collection for genetic analysis, captive propagation, and subsequent stocking into currently occupied and unoccupied areas within the historical range of the species.', 'label': []}, {'sentence_151': '(B) Channel restoration projects that create natural, physically stable, ecologically functioning streams (or stream and wetland systems) that are reconnected with their groundwater aquifers and, if the projects involve known trispot darter spawning habitat, that take place between May 1 and December 31.', 'label': []}, {'sentence_152': 'These projects can be accomplished using a variety of methods, but the desired outcome is a natural channel with low shear stress (force of water moving against the channel); bank heights that enable reconnection to the floodplain; a reconnection of surface and groundwater systems, resulting in perennial flows in the channel; riffles and pools comprised of existing soil, rock, and wood instead of large [[Page 61619]] imported materials; low compaction of soils within adjacent riparian areas; and inclusion of riparian wetlands.', 'label': []}, {'sentence_153': '(C) Streambank stabilization projects that utilize bioengineering methods to replace pre-existing, bare, eroding stream banks with vegetated, stable stream banks, thereby reducing bank erosion and instream sedimentation and improving habitat conditions for the species.', 'label': []}, {'sentence_154': 'Stream banks may be stabilized using live stakes (live, vegetative cuttings inserted or tamped into the ground in a manner that allows the stake to take root and grow), live fascines (live branch cuttings, usually willows, bound together into long, cigar-shaped bundles), or brush layering (cuttings or branches of easily rooted tree species layered between successive lifts of soil fill).', 'label': []}, {'sentence_155': 'Stream banks must not be stabilized solely through the use of quarried rock (rip- rap) or the use of rock baskets or gabion structures.', 'label': []}, {'sentence_156': '(D) Silviculture practices and forest management activities that: (1) Implement State best management practices, particularly for streamside management zones, for stream crossings, for forest roads, for erosion control, and to maintain stable channel morphology; or (2) Remove logging debris or any other large material placed within natural or artificial wet weather conveyances or ephemeral, intermittent, or perennial stream channels; and (3) When such activities involve trispot darter spawning habitat, are carried out between May 1 and December 31.', 'label': []}, {'sentence_157': '(E) Transportation projects that provide for fish passage at stream crossings that are performed between May 1 and December 31 to avoid the time period when the trispot darter will be found within spawning habitat, if such habitat is affected by the activity.', 'label': []}, {'sentence_158': \"(F) Projects carried out in the species' range under the Working Lands for Wildlife program of the Natural Resources Conservation Service, US Department of Agriculture, that: (1) Do not alter habitats known to be used by the trispot darter beyond the fish's tolerances; and (2) Are performed between May 1 and December 31 to avoid the time period when the trispot darter will be found within its spawning habitat, if such habitat is affected by the activity.\", 'label': []}, {'sentence_159': '(v) Possess and engage in other acts with unlawfully taken wildlife, as set forth at Sec 1721(d)(2) for endangered wildlife.', 'label': []}, {'sentence_160': '* * * * * Aurelia Skipwith, Director, US Fish and Wildlife Service.', 'label': []}, {'sentence_161': '[FR Doc 2020-19109 Filed 9-29-20; 8:45 am] BILLING CODE 4333-15-P', 'label': []}]}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in policy_dict:\n",
    "    if i == 0:\n",
    "        print(policy_dict[item])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: DEPRECATED\n",
    "\n",
    "For now, this only trims down by sentence size or samples from the general pool - the first one can be done in the sentence-splitting aspect, and the second one may not be necessary for now\n",
    "\n",
    "------------------------------\n",
    "The main purpose of the following code is to apply a series of filters to the sentence database to reduce the final number of sentences used:\n",
    "* The most important is to keep the sentences that are in relevant parts of the documents, and leave aside the ones which are in parts of the documents that will not contain incentives by nature.\n",
    "\n",
    "* There is a second filter by sentence length\n",
    "\n",
    "* There is a for-testing-only filter which arbitrarily selects a sample of sentences. The reason being that running the sentence embedding function takes time. The variable \"slim_by\" is the reduction factor. If it is set to 1, there will be no reduction and we will be working with the full dataset. It it is set to two, we will take one every two sentences and so one.\n",
    "\n",
    "The output of the function is a dictionary of this form:\n",
    "\n",
    "{\"\\<sentence id\\>\" : \"\\<text of the sentence\\>\"}.\n",
    "\n",
    "<span style=\"color:red\"><strong>REMEMBER</strong></span> that you have to re-run the function \"get_sentences_dict\" with the \"slim_by\" variable set to 1 when you want to go for the final shoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to shrink the sentences dict by a user set factor. It will pick only one sentence every \"slim_factor\"\n",
    "def slim_dict(counter, slim_factor):\n",
    "    if counter % slim_factor == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# This is to trim sentences that are either too short or too large to be meaningful.\n",
    "# This function is based on number of characters, but it can easily be adated to trim by word number.\n",
    "def sentence_length_filter(sentence_text, minLength, maxLength):\n",
    "    if len(sentence_text) > minLength:#len(sentence_text) < maxLength and\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_sentences_dict(docs_dict, is_not_incentive_dict, slim_factor, minLength, maxLength):\n",
    "    count = 0\n",
    "    result = {}\n",
    "    for key, value in docs_dict.items():\n",
    "        for item in value: \n",
    "            if item in is_not_incentive_dict:\n",
    "                continue\n",
    "            else:\n",
    "                for sentence in docs_dict[key][item]['sentences']:\n",
    "                    if sentence_length_filter(docs_dict[key][item]['sentences'][sentence][\"text\"], minLength, maxLength):\n",
    "                        count += 1\n",
    "                        if slim_dict(count, slim_by):\n",
    "                            result[sentence] = docs_dict[key][item]['sentences'][sentence]\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_incentive = {}\n",
    "# is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "#                     \"POR TANTO\" : 0,\n",
    "#                     \"DISPOSICIONES GENERALES\" : 0,\n",
    "#                     \"OBJETO\" : 0,\n",
    "#                     \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "# is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "#                     \"POR TANTO\" : 0,\n",
    "#                     \"DISPOSICIONES GENERALES\" : 0,\n",
    "#                     \"OBJETO\" : 0,\n",
    "#                     \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0,\n",
    "#                    \"VISTO\" : 0,\n",
    "#                    \"HEADING\" : 0}\n",
    "\n",
    "slim_by = 10000 # REMEMBER to set this variable to the desired value.\n",
    "min_length = 50 # Just to avoid short sentences which might be fragments or headings without a lot of value\n",
    "max_length = 250 # Just to avoid long sentences which might be artifacts or long legal jargon separated by semicolons\n",
    "\n",
    "sentences = get_sentences_dict(policy_dict, is_not_incentive, slim_by, min_length, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check if the results look ok\n",
    "print(\"In this data set there are {} policies and {} sentences\".format(len(policy_dict),len(sentences)))\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[\"70be962_99\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all sentences\n",
    "import sys\n",
    "sys.path.append(\"/Users/dafirebanks/Projects/policy-data-analyzer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.data_loading.src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES!!!!!!!!!!!!!\n",
    "- CHANGE THE WAY WE ARE STORING STUFF ACCORDING TO PREVIOUSLY KNOWN WAY OR UPDATE DATA LOADER\n",
    "- SENTENCE ID SHOULD BE DOCID + \"sentence_X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_0', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_1', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_2', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_3', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_4', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_5', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_6', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_7', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_8', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_9', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_10', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_11', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_12', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_13', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_14', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_15', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_16', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_17', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_18', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_19', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_20', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_21', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_22', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_23', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_24', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_25', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_26', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_27', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_28', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_29', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_30', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_31', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_32', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_33', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_34', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_35', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_36', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_37', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_38', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_39', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_40', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_41', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_42', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_43', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_44', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_45', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_46', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_47', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_48', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_49', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_50', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_51', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_52', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_53', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_54', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_55', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_56', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_57', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_58', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_59', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_60', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_61', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_62', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_63', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_64', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_65', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_66', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_67', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_68', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_69', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_70', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_71', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_72', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_73', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_74', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_75', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_76', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_77', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_78', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_79', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_80', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_81', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_82', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_83', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_84', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_85', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_86', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_87', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_88', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_89', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_90', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_91', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_92', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_93', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_94', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_95', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_96', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_97', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_98', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_99', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_100', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_101', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_102', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_103', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_104', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_105', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_106', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_107', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_108', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_109', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_110', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_111', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_112', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_113', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_114', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_115', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_116', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_117', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_118', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_119', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_120', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_121', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_122', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_123', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_124', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_125', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_126', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_127', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_128', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_129', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_130', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_131', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_132', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_133', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_134', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_135', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_136', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_137', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_138', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_139', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_140', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_141', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_142', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_143', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_144', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_145', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_146', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_147', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_148', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_149', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_150', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_151', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_152', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_153', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_154', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_155', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_156', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_157', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_158', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_159', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_160', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_161', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_0', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_1', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_2', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_3', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_4', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_5', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_6', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_7', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_8', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_9', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_10', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_11', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_12', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_13', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_14', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_15', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_16', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_17', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_18', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_19', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_20', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_21', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_22', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_0', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_1', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_2', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_3', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_4', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_5', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_6', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_7', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_8', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_9', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_10', 'label'])\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "dict_keys(['sentence_11', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# TEMPORARY LOADING FUNCTION, WE GOTTA CHANGE THIS LATER ACCORDING TO DOC ID!!!!\n",
    "def labeled_sentences_from_dataset(dataset):\n",
    "    labeled_sentences = {}\n",
    "    \n",
    "    for doc_id, document in dataset.items():\n",
    "        for sentence in document['sentences']:\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%\")\n",
    "            print(sentence.keys())\n",
    "            labeled_sentences.update(sentence)\n",
    "\n",
    "    return labeled_sentences\n",
    "\n",
    "sentences = labeled_sentences_from_dataset(policy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = {\"sent1\": \n",
    "              {\"text\": \"sent 1 text\", \"labels\": []},\n",
    "              \"sent2\": \n",
    "              {\"text\": \"sent 2 text\", \"labels\": []}\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "Then we build a simple function that takes four inputs:\n",
    "1. The model as we have set it in the previous line of code\n",
    "2. A dictionary that contains the sentences {\"\\<sentence_ID\\>\" : {\"text\" : \"The actual sentence\", labels : []}\n",
    "3. A query in the form of a string\n",
    "4. A similarity treshold. It is a float that we can use to limit the results list to the most relevant.\n",
    "\n",
    "The output of the function is a list with three columns with the following content:\n",
    "1. Column 1 contains the id of the sentence\n",
    "2. Column 2 contains the similarity score\n",
    "3. Column 3 contains the text of the sentence that has been compared with the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently two multi language models available for sentence similarity\n",
    "\n",
    "* xlm-r-bert-base-nli-stsb-mean-tokens: Produces similar embeddings as the bert-base-nli-stsb-mean-token model. Trained on parallel data for 50+ languages.\n",
    "<span style=\"color:red\"><strong>Attention!</strong></span> Model \"xlm-r-100langs-bert-base-nli-mean-tokens\" which was the name used in the original Omdena-challenge script has changed to this \"xlm-r-bert-base-nli-stsb-mean-tokens\"\n",
    "\n",
    "* distiluse-base-multilingual-cased-v2: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. While the original mUSE model only supports 16 languages, this multilingual knowledge distilled version supports 50+ languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to create the embeddings for each transformer the embeddings in a json with the following structure:\n",
    "# INPUT PARAMETERS\n",
    "# transformers: a list with transformer names\n",
    "# sentences_dict: a dictionary with the sentences of the database with the form {\"<sentence id>\" : \"<sentence text>\"}}\n",
    "# file: the filepath and filename of the output json\n",
    "# OUTPUT\n",
    "# the embeddings of the sentences in a json with the following structure:\n",
    "# {\"<transformer name>\" : {\"<sentence id>\" : <sentence embedding>}}\n",
    "\n",
    "def create_sentence_embeddings(model, sentences_dict, file):\n",
    "    embeddings = {}\n",
    "    for sentence in sentences_dict:\n",
    "        embeddings[sentence] = [model.encode(sentence['text'].lower(), show_progress_bar=False)]\n",
    "        \n",
    "    return embeddings\n",
    "#     with open(file, 'w') as fp:\n",
    "#         json.dump(embeddings, fp, cls = NumpyArrayEncoder)\n",
    "#     file_name = \"pre_embeddings/\" + file.split(\"/\")[3]\n",
    "#     s3.Object('wri-latin-talent', file_name).put(Body=open(file, 'rb'))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for sentences in the database with the pre-trained model\n",
    "\n",
    "This piece of code it's to be executed only once every time the database is changed or when we want to get the embeddings of a new database. For example, we are going to use it once for El Salvador policies and we don't need to use it again until we add new policies to this database.\n",
    "\n",
    "Instead, whenever we want to run experiments on this database, we will load the json files with the embeddings which are in the \"input\" folder.\n",
    "\n",
    "So, the next cell will be kept commented for safety reasons. Un comment it and execute it whenvere you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-6bba64dd2f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sentence_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mTf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-f881ce40fdfa>\u001b[0m in \u001b[0;36mcreate_sentence_embeddings\u001b[0;34m(model, sentences_dict, file)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "# We will use only one transformer to compute embeddings\n",
    "transformer_name = 'xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "path = \"../../input/\"\n",
    "today = datetime.date.today()\n",
    "today = today.strftime('%Y-%m-%d')\n",
    "filename = \"Embeddings_\" + today + \"_ES.json\"\n",
    "file = path + filename\n",
    "\n",
    "\n",
    "model = SentenceTransformer(transformer_name)\n",
    "embs = create_sentence_embeddings(model, test_sents, file)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"The building of a sentence embedding database for El Salvador in the two current models has taken {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained model embeddings for database sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** in 1 pre_embeddings/Embeddings_2021-03-04_ES.json\n",
      "\n",
      "*** in 2 pre_embeddings/Embeddings_2021-03-04_ES.json\n"
     ]
    }
   ],
   "source": [
    "filter_language = \"_ES\"\n",
    "filter_prefix = \"pre_embeddings/\" #TODO: move everything to a \"Sentences\" folder\n",
    "\n",
    "sentence_embeddings = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix=filter_prefix):\n",
    "    if filter_language in obj.key:\n",
    "          serializedObject = obj.get()['Body'].read()\n",
    "          sentence_embeddings = {**sentence_embeddings, **json.loads(serializedObject)}\n",
    "    #     obj = s3.Object('wri-latin-talent',filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_embeddings))\n",
    "# for key in sentence_embeddings:\n",
    "#     print(key)\n",
    "#     print(len(sentence_embeddings[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assisted labeling by query search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below is to use a set of queries to search a database for similar sentences with different transformers.\n",
    "# The input parameters are:\n",
    "\n",
    "# Transformer_names: A list with the names of the transformers to be used. For multilingual similarity search we have two transformers\n",
    "# Queries: a list of the queries as strings, that we want to use for searching the database\n",
    "\n",
    "# Similarity_limit: The results are in the form of a similarity coefficient where 1 is a perfect match between the query embedding\n",
    "# and the sentence in the database (the two vectors overlap). If the similarity coefficient is 0 the two vectors are orthogonal,\n",
    "# they do not share anything in common. Thus, in order to restribt the number of results that are kept from the experiment we can\n",
    "# it by setting a similarity threshold.When we have a huge database a good treshold would be 0.3 to 0.5 or even higher.\n",
    "\n",
    "# Results_limit: instead of or complementary to Similarity_limit, we can limit our list of search results by the first sentences\n",
    "# in the similarity ranking. We can set the limit to high numbers in an exploration phase and then reduce this number in a \n",
    "# \"production\" phase\n",
    "\n",
    "# Filename: The results will be exported to the \"output/\" folder in json formate, we need to give it a name witout extension.\n",
    "\n",
    "def sentence_similarity_search(model, queries, similarity_limit, results_limit, filename):\n",
    "    results = {}\n",
    "    for query in queries:\n",
    "        Ti = time.perf_counter()\n",
    "        similarities = get_distance(model, sentence_embeddings, sentences, query, similarity_limit)\n",
    "        results[query] = similarities[0:results_limit]#results[transformer][query] = similarities[0:results_limit]\n",
    "        Tf = time.perf_counter()\n",
    "        print(f\"similarity search for query {query} it's been done in {Tf - Ti:0.4f} seconds\")\n",
    "\n",
    "    path = \"../../output/\"\n",
    "    filename = filename + \".json\"\n",
    "    file = path + filename\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "    return results\n",
    "\n",
    "# This function helps debugging misspelling in the values of the dictionary\n",
    "def check_dictionary_values(dictionary):\n",
    "    check_country = {}\n",
    "    check_incentive = {}\n",
    "    for key, value in dictionary.items():\n",
    "        incentive, country = value.split(\"-\")\n",
    "        check_incentive[incentive] = 0\n",
    "        check_country[country] = 0\n",
    "    print(check_incentive)\n",
    "    print(check_country)\n",
    "\n",
    "   \n",
    "def get_distance( model, sentence_emb, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query.lower(), show_progress_bar=False)\n",
    "    highlights = []\n",
    "    for sentence in sentences_dict:\n",
    "        sentence_embedding = np.asarray(sentence_emb[sentence])[0]\n",
    "        score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "        if score > similarity_treshold:\n",
    "            highlights.append([sentence, score, sentences_dict[sentence]['text']])\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query building\n",
    "\n",
    "The code to compute sentence similarity will take two imputs:\n",
    "\n",
    "* The queries that will by input as a list of strings. \n",
    "* The embeddings of the sentences in the database. \n",
    "\n",
    "At this point all we need to run the experiment is ready but the list of queries. One can write the list manually, or one can make it from other data flows. The next cells are ment to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the databse of tagged sentences to define queries. The database is structured by countries. From a list of model documents the sentences were separated and tagged with a policy instrument label. The labels that were used are:\n",
    "\n",
    "* Credit\n",
    "* Direct payment\n",
    "* Fine\n",
    "* Guarantee\n",
    "* Supplies\n",
    "* Tax deduction\n",
    "* Technical assistance\n",
    "\n",
    "Not all countries have tagged sentences for each category so we ended up with 26 queries\n",
    "\n",
    "The difference between this experiment and experiment 2 is that here we have reformulated the query sentences by extracting the core incentive meaning from the original sentences, eliminating all the vocabulary not strictly speaking about incentives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict = {\n",
    "    \"Otorgamiento de estmulos crediticios por parte de el estado\" : \"Credit-Mxico\",\n",
    "\"Estos crditos podran beneficiar a sistemas productivos asociados a la pequea y mediana produccin\" : \"Credit-Per\",\n",
    "\"Se asocia con crditos de enlace del Banco del Estado\" : \"Credit-Chile\", \n",
    "\"Acceso al programa de garanta crediticia para la actividad econmica\" : \"Credit-Guatemala\",\n",
    "\"El banco establecer lneas de crdito para que el sistema financiero apoye la pequea, mediana y microempresa\" : \"Credit-El Salvador\",\n",
    "\"Dentro de los incentivos econmicos se podr crear un bono para retribuir a los propietarios por los bienes y servicios generados.\" : \"Direct_payment-Mxico\",\n",
    "\"Acceso a los fondos forestales para el pago de actividad\" : \"Direct_payment-Per\",\n",
    "\"Se bonificar el 90% de los costos de repoblacin para las primeras 15 hectreas y de un 75% respecto las restantes\" : \"Direct_payment-Chile\",\n",
    "\"El estado dar un incentivo que se pagar una sola vez a los propietarios forestales\" : \"Direct_payment-Guatemala\",\n",
    "\"Incentivos en dinero para cubrir los costos directos e indirectos del establecimiento y manejo de areas de produccin\" : \"Direct_payment-El Salvador\",\n",
    "\"Toda persona fsica o moral que cause daos estar obligada a repararlo o compensarlo\" : \"Fine-Mxico\",\n",
    "\"Disminuir los riesgos para el inversionista implementando mecanismos de aseguramiento\" : \"Guarantee-Mxico\",\n",
    "\"Podr garantizarse el cumplimiento de la actividad mediante fianza otorgada a favor del estado por cualquiera de las afianzadoras legalmente autorizadas.\" : \"Guarantee-Guatemala\",\n",
    "\"El sujeto de derecho podr recibir insumos para la instalacin y operacin de infraestructuras para la actividad econmica.\" : \"Supplies-Mxico\",\n",
    "\"Se facilitar el soporte tcnico a  travs de la utilizacin de guas, manuales, protocolos, paquetes tecnolgicos, procedimientos, entre otros.\" : \"Supplies-Per\",\n",
    "\"Se concedern incentivos en especie para fomentar la actividad en forma de insumos\" : \"Supplies-El Salvador\",\n",
    "\"Se otorgarn incentivos fiscales para la actividad primaria y tambin la actividad de transformacin\" : \"Tax_deduction-Mxico\",\n",
    "\"De acuerdo con los lineamientos aprobados se conceder un 25% de descuento en el pago del derecho de aprovechamiento\" : \"Tax_deduction-Per\",\n",
    "\"Las bonificaciones percibidas o devengadas se considerarn como ingresos diferidos en el pasivo circulante y no se incluirn para el clculo de la tasa adicional ni constituirn renta para ningn efecto legal hasta el momento en que se efecte la explotacin o venta\" : \"Tax_deduction-Chile\",\n",
    "\"Los contratistas que suscriban contratos de exploracin y/o explotacin, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades\" : \"Tax_deduction-Guatemala\",\n",
    "\"Exencin de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestacin de Servicios, en la importacin de sus bienes, equipos y accesorios, maquinaria, vehculos, aeronaves o embarcaciones\" : \"Tax_deduction-El Salvador\",\n",
    "\"Se facilitar formacin Permanente Adems del acompaamiento tcnico, los sujetos de derecho participarn en un proceso permanente de formacin a lo largo de todo el ao, que les permita enriquecer sus habilidades y capacidades \" : \"Technical_assistance-Mxico\",\n",
    "\"Contribuir en la promocin para la gestin, a travs de la capacitacin, asesoramiento, asistencia tcnica y educacin de los usuarios\" : \"Technical_assistance-Per\",\n",
    "\"Asesora prestada al usuario por un operador acreditado, conducente a elaborar, acompaar y apoyar la adecuada ejecucin tcnica en terreno de aquellas prcticas comprometidas en el Plan de Manejo\" : \"Technical_assistance-Chile\",\n",
    "\"Para la ejecucin de programas de capacitacin, adiestramiento y otorgamiento de becas para la preparacin de personal , as como para el desarrollo de tecnologa en actividades directamente relacionadas con las operaciones objeto del contrato\" : \"Technical_assistance-Guatemala\",\n",
    "\"Apoyo tcnico y en formulacin de proyectos y conexin con mercados\" : \"Technical_assistance-El Salvador\"}\n",
    "\n",
    "queries = []\n",
    "for query in queries_dict:\n",
    "    queries.append(query)\n",
    "        \n",
    "# print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just to check the presence of misspelling in the values of the queries dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Credit': 0, 'Direct_payment': 0, 'Fine': 0, 'Guarantee': 0, 'Supplies': 0, 'Tax_deduction': 0, 'Technical_assistance': 0}\n",
      "{'Mxico': 0, 'Per': 0, 'Chile': 0, 'Guatemala': 0, 'El Salvador': 0}\n"
     ]
    }
   ],
   "source": [
    "check_dictionary_values(queries_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity search for query Otorgamiento de estmulos crediticios por parte de el estado it's been done in 0.4415 seconds\n",
      "similarity search for query Estos crditos podran beneficiar a sistemas productivos asociados a la pequea y mediana produccin it's been done in 0.0155 seconds\n",
      "similarity search for query Se asocia con crditos de enlace del Banco del Estado it's been done in 0.0154 seconds\n",
      "similarity search for query Acceso al programa de garanta crediticia para la actividad econmica it's been done in 0.0148 seconds\n",
      "similarity search for query El banco establecer lneas de crdito para que el sistema financiero apoye la pequea, mediana y microempresa it's been done in 0.0165 seconds\n",
      "similarity search for query Dentro de los incentivos econmicos se podr crear un bono para retribuir a los propietarios por los bienes y servicios generados. it's been done in 0.0172 seconds\n",
      "similarity search for query Acceso a los fondos forestales para el pago de actividad it's been done in 0.0152 seconds\n",
      "similarity search for query Se bonificar el 90% de los costos de repoblacin para las primeras 15 hectreas y de un 75% respecto las restantes it's been done in 0.0165 seconds\n",
      "similarity search for query El estado dar un incentivo que se pagar una sola vez a los propietarios forestales it's been done in 0.0153 seconds\n",
      "similarity search for query Incentivos en dinero para cubrir los costos directos e indirectos del establecimiento y manejo de areas de produccin it's been done in 0.0162 seconds\n",
      "similarity search for query Toda persona fsica o moral que cause daos estar obligada a repararlo o compensarlo it's been done in 0.0159 seconds\n",
      "similarity search for query Disminuir los riesgos para el inversionista implementando mecanismos de aseguramiento it's been done in 0.0149 seconds\n",
      "similarity search for query Podr garantizarse el cumplimiento de la actividad mediante fianza otorgada a favor del estado por cualquiera de las afianzadoras legalmente autorizadas. it's been done in 0.0157 seconds\n",
      "similarity search for query El sujeto de derecho podr recibir insumos para la instalacin y operacin de infraestructuras para la actividad econmica. it's been done in 0.0167 seconds\n",
      "similarity search for query Se facilitar el soporte tcnico a  travs de la utilizacin de guas, manuales, protocolos, paquetes tecnolgicos, procedimientos, entre otros. it's been done in 0.0192 seconds\n",
      "similarity search for query Se concedern incentivos en especie para fomentar la actividad en forma de insumos it's been done in 0.0152 seconds\n",
      "similarity search for query Se otorgarn incentivos fiscales para la actividad primaria y tambin la actividad de transformacin it's been done in 0.0155 seconds\n",
      "similarity search for query De acuerdo con los lineamientos aprobados se conceder un 25% de descuento en el pago del derecho de aprovechamiento it's been done in 0.0147 seconds\n",
      "similarity search for query Las bonificaciones percibidas o devengadas se considerarn como ingresos diferidos en el pasivo circulante y no se incluirn para el clculo de la tasa adicional ni constituirn renta para ningn efecto legal hasta el momento en que se efecte la explotacin o venta it's been done in 0.0189 seconds\n",
      "similarity search for query Los contratistas que suscriban contratos de exploracin y/o explotacin, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades it's been done in 0.0182 seconds\n",
      "similarity search for query Exencin de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestacin de Servicios, en la importacin de sus bienes, equipos y accesorios, maquinaria, vehculos, aeronaves o embarcaciones it's been done in 0.0187 seconds\n",
      "similarity search for query Se facilitar formacin Permanente Adems del acompaamiento tcnico, los sujetos de derecho participarn en un proceso permanente de formacin a lo largo de todo el ao, que les permita enriquecer sus habilidades y capacidades  it's been done in 0.0186 seconds\n",
      "similarity search for query Contribuir en la promocin para la gestin, a travs de la capacitacin, asesoramiento, asistencia tcnica y educacin de los usuarios it's been done in 0.0149 seconds\n",
      "similarity search for query Asesora prestada al usuario por un operador acreditado, conducente a elaborar, acompaar y apoyar la adecuada ejecucin tcnica en terreno de aquellas prcticas comprometidas en el Plan de Manejo it's been done in 0.0181 seconds\n",
      "similarity search for query Para la ejecucin de programas de capacitacin, adiestramiento y otorgamiento de becas para la preparacin de personal , as como para el desarrollo de tecnologa en actividades directamente relacionadas con las operaciones objeto del contrato it's been done in 0.0185 seconds\n",
      "similarity search for query Apoyo tcnico y en formulacin de proyectos y conexin con mercados it's been done in 0.0149 seconds\n"
     ]
    }
   ],
   "source": [
    "transformer_name ='xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "similarity_threshold = 0.2\n",
    "search_results_limit = 1000\n",
    "today = datetime.date.today()\n",
    "today = today.strftime('%Y-%m-%d')\n",
    "name = \"Pre_tagged_\" + today + \"_\" + filter_language\n",
    "\n",
    "model = SentenceTransformer(transformer_name)\n",
    "results_dict = sentence_similarity_search(model, queries, similarity_threshold, search_results_limit, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a temporary section to explore how to analyze the results. It is organized with the same structure as the section <strong>Defining queries</strong> as we are exploring the best search strategies based on different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the functions that are going to be used in the post-processing and in the analysis of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the contents of the results dict, particularly, the length of the first element and its contents\n",
    "def show_results(results_dictionary):\n",
    "    i = 0\n",
    "    for key1 in results_dictionary:\n",
    "        for key2 in results_dictionary[key1]:\n",
    "            if i == 0:\n",
    "                print(len(results_dictionary[key1][key2]))\n",
    "                print(results_dictionary[key1][key2])\n",
    "            i += 1\n",
    "\n",
    "# Adding the rank to each result\n",
    "def add_rank(results_dictionary):\n",
    "#     for model in results_dictionary:\n",
    "    for keyword in results_dictionary:#[model]:\n",
    "        i = 1\n",
    "        for result in results_dictionary[keyword]:#[model][keyword]:\n",
    "            result.insert(1, i)\n",
    "            i += 1\n",
    "    return results_dictionary\n",
    "\n",
    "# For experiments 2 and 3 this function is to save results in separate csv files\n",
    "def save_results_as_separate_csv(results_dictionary, queries_dictionary, date):\n",
    "    path = \"../../output/pre_labeled/\"\n",
    "#     for model, value in results_dictionary.items():\n",
    "    for exp_title, result in results_dictionary.items():#value.items():\n",
    "        filename = queries_dictionary[exp_title]\n",
    "        file = path + filename + \".csv\"\n",
    "        with open(file, 'w', newline='', encoding='utf-8') as f:\n",
    "            write = csv.writer(f)\n",
    "            write.writerows(result)\n",
    "#             print(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the analysis are saved as a json file. To further process the information we can upload the file contents into a dictionary.\n",
    "\n",
    "After loading the results, a rank value is added to the results from the highest similarity score to the lower one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json where there are the results that you want to analyze. CHANGE the file name accordingly.\n",
    "path = \"../../output/\"\n",
    "filename = \"Pre_tagged_2021-03-04__ES.json\"\n",
    "file = path + filename\n",
    "with open(file, \"r\") as f:\n",
    "    results_ = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = add_rank(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the rank to the results dictionary if we use the computed version not the uploaded\n",
    "results = copy.deepcopy(add_rank(results_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to simplify the analysis process and to make it available for a broader spectrum of analysts, the results are split into small \"csv\" documents that can be easily imported in spreadsheets.\n",
    "\n",
    "The new files will contain only the results of a single query, this is it will contain all the 100 (or whatever number has been retrieved) sentences from the database which have the highest similarity score with the query. There will be the following columns:\n",
    "\n",
    "* Sentence ID\n",
    "* Rank of the sentence in the similarity results\n",
    "* Similarity score\n",
    "* Text of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results as separete csv files\n",
    "\n",
    "save_results_as_separate_csv(results, queries_dict, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wri-env",
   "language": "python",
   "name": "wri-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "757px",
    "left": "586px",
    "top": "630px",
    "width": "523px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
