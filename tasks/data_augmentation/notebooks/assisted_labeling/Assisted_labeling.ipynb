{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assisted labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is a basic implementation of sBERT for searching a database of sentences with queries.\n",
    "\n",
    "The goal is to increase the amount of labeled data that we have in order to later fine tune a model to be used for sentence classification. First of all we have to find a pool of queries that represent the six labels of the six policy instruments. With these queries we can pull a set of sentences that can be automaticaly labeled with the same label of the query. In this way we can increase the diversity of labeled sentences in each label category. This approach will be complemented with a manual curation step to produce a high quality training data set.\n",
    "\n",
    "The policy instruments that we want to find and that correspond to the different labels are:\n",
    "* Direct payment (PES)\n",
    "* Tax deduction\n",
    "* Credit/guarantee\n",
    "* Technical assistance\n",
    "* Supplies\n",
    "* Fines\n",
    "\n",
    "This notebook is intended for the following purposes:\n",
    "* Loading of a database of sentences\n",
    "* Create sentence embeddings for further processing\n",
    "* Define the queries corresponding to the labels\n",
    "* Compute the cosine similarity score between the embeddings of the sentences and those of the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is self contained, it does not depend on any other class of the sBERT folder.\n",
    "\n",
    "You just have to create an environment where you install the external dependencies. Usually the dependencies that you have to install are:\n",
    "\n",
    "**For the basic sentence similarity calculation**\n",
    "*  pandas\n",
    "*  boto3\n",
    "*  pytorch\n",
    "*  sentence_transformers\n",
    "\n",
    "**If you want to do evaluation and ploting with pyplot**\n",
    "*  matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment is called nlp then you execute this cell otherwise you change the name of the environment\n",
    "!conda activate NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import boto3\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import time\n",
    "# from pathlib import Path\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sentencepiece\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "# path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"AWS_S3_keys_Omdena.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)\n",
    "# for key in key_dict:\n",
    "#     KEY = key\n",
    "#     SECRET = key_dict[key]\n",
    "    \n",
    "region = 'us-east-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file = '/Users/dafirebanks/Documents/credentials.json'\n",
    "aws_id, aws_secret = aws_credentials_from_file(credentials_file)\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")\n",
    "\n",
    "bucket = 'wri-nlp-policy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database\n",
    "\n",
    "Here we merge all documents databases for each country in a single dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Old code for Omdena bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = ['Chile.json','ElSalvador.json']\n",
    "# filter_language = \"_ES\"\n",
    "# filter_folder = \"JSON/\" #TODO: move everything to a \"Sentences\" folder\n",
    "# filter_prefix = filter_folder + filter_language # TODO: change naming convention for files adding _ES or _En according to language. \n",
    "\n",
    "# policy_dict = {}\n",
    "# for file in filename:\n",
    "#     filter_prefix  = filter_folder + file\n",
    "#     for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix=filter_prefix):\n",
    "#     #     obj = s3.Object('wri-latin-talent',filename)\n",
    "#         serializedObject = obj.get()['Body'].read()\n",
    "#         policy_dict = {**policy_dict, **json.loads(serializedObject)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_documents/sentences/\n",
      "english_documents/sentences/0002304f1f671ea916ae0a1f784484eb4874ceaa_sents.json\n",
      "english_documents/sentences/0002a815db93aaba959b04dbeaa17e87f8585734_sents.json\n",
      "english_documents/sentences/0005bd689cd9cc6ab99194b7bb5aed32fe0bbb47_sents.json\n",
      "english_documents/sentences/0007adab1d0acfc53274596b784bd85cdfbe502e_sents.json\n",
      "english_documents/sentences/000cc1ce72ee0d48230e5da68fe69c11ac428cfa_sents.json\n",
      "english_documents/sentences/0010b6d44c3d296ba1bfaa24c718ab16d9ae017b_sents.json\n",
      "english_documents/sentences/00175bb6c2a2b3368a9cfdfbd895ecac48920ccb_sents.json\n",
      "english_documents/sentences/00198951ca0fcb94619e41d5256c31e8ce57d70f_sents.json\n",
      "english_documents/sentences/0020eabfcbd08554df1bd58438dcfda7f454d8c7_sents.json\n",
      "english_documents/sentences/00274763715091c9c1186a4d13f61ef5b773b923_sents.json\n"
     ]
    }
   ],
   "source": [
    "policy_dict = {}\n",
    "objs = []\n",
    "language = \"english\"\n",
    "bucket_name = 'wri-nlp-policy'\n",
    "sents_folder = f\"{language}_documents/sentences\"\n",
    "\n",
    "for i, obj in enumerate(s3.Bucket(bucket_name).objects.all().filter(Prefix=sents_folder)):\n",
    "    print(obj.key)\n",
    "    if not obj.key.endswith(\"/\"):\n",
    "        serializedObject = obj.get()['Body'].read()\n",
    "        policy_dict = {**policy_dict, **json.loads(serializedObject)}\n",
    "\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0002304f1f671ea916ae0a1f784484eb4874ceaa', '0002a815db93aaba959b04dbeaa17e87f8585734', '0005bd689cd9cc6ab99194b7bb5aed32fe0bbb47', '0007adab1d0acfc53274596b784bd85cdfbe502e', '000cc1ce72ee0d48230e5da68fe69c11ac428cfa', '0010b6d44c3d296ba1bfaa24c718ab16d9ae017b', '00175bb6c2a2b3368a9cfdfbd895ecac48920ccb', '00198951ca0fcb94619e41d5256c31e8ce57d70f', '0020eabfcbd08554df1bd58438dcfda7f454d8c7', '00274763715091c9c1186a4d13f61ef5b773b923'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: DEPRECATED\n",
    "\n",
    "For now, this only trims down by sentence size or samples from the general pool - the first one can be done in the sentence-splitting aspect, and the second one may not be necessary for now\n",
    "\n",
    "------------------------------\n",
    "The main purpose of the following code is to apply a series of filters to the sentence database to reduce the final number of sentences used:\n",
    "* The most important is to keep the sentences that are in relevant parts of the documents, and leave aside the ones which are in parts of the documents that will not contain incentives by nature.\n",
    "\n",
    "* There is a second filter by sentence length\n",
    "\n",
    "* There is a for-testing-only filter which arbitrarily selects a sample of sentences. The reason being that running the sentence embedding function takes time. The variable \"slim_by\" is the reduction factor. If it is set to 1, there will be no reduction and we will be working with the full dataset. It it is set to two, we will take one every two sentences and so one.\n",
    "\n",
    "The output of the function is a dictionary of this form:\n",
    "\n",
    "{\"\\<sentence id\\>\" : \"\\<text of the sentence\\>\"}.\n",
    "\n",
    "<span style=\"color:red\"><strong>REMEMBER</strong></span> that you have to re-run the function \"get_sentences_dict\" with the \"slim_by\" variable set to 1 when you want to go for the final shoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to shrink the sentences dict by a user set factor. It will pick only one sentence every \"slim_factor\"\n",
    "def slim_dict(counter, slim_factor):\n",
    "    if counter % slim_factor == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# This is to trim sentences that are either too short or too large to be meaningful.\n",
    "# This function is based on number of characters, but it can easily be adated to trim by word number.\n",
    "def sentence_length_filter(sentence_text, minLength, maxLength):\n",
    "    if len(sentence_text) > minLength:#len(sentence_text) < maxLength and\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_sentences_dict(docs_dict, is_not_incentive_dict, slim_factor, minLength, maxLength):\n",
    "    count = 0\n",
    "    result = {}\n",
    "    for key, value in docs_dict.items():\n",
    "        for item in value: \n",
    "            if item in is_not_incentive_dict:\n",
    "                continue\n",
    "            else:\n",
    "                for sentence in docs_dict[key][item]['sentences']:\n",
    "                    if sentence_length_filter(docs_dict[key][item]['sentences'][sentence][\"text\"], minLength, maxLength):\n",
    "                        count += 1\n",
    "                        if slim_dict(count, slim_by):\n",
    "                            result[sentence] = docs_dict[key][item]['sentences'][sentence]\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_incentive = {}\n",
    "# is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "#                     \"POR TANTO\" : 0,\n",
    "#                     \"DISPOSICIONES GENERALES\" : 0,\n",
    "#                     \"OBJETO\" : 0,\n",
    "#                     \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "# is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "#                     \"POR TANTO\" : 0,\n",
    "#                     \"DISPOSICIONES GENERALES\" : 0,\n",
    "#                     \"OBJETO\" : 0,\n",
    "#                     \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0,\n",
    "#                    \"VISTO\" : 0,\n",
    "#                    \"HEADING\" : 0}\n",
    "\n",
    "slim_by = 10000 # REMEMBER to set this variable to the desired value.\n",
    "min_length = 50 # Just to avoid short sentences which might be fragments or headings without a lot of value\n",
    "max_length = 250 # Just to avoid long sentences which might be artifacts or long legal jargon separated by semicolons\n",
    "\n",
    "sentences = get_sentences_dict(policy_dict, is_not_incentive, slim_by, min_length, max_length)\n",
    "\n",
    "# Just to check if the results look ok\n",
    "print(\"In this data set there are {} policies and {} sentences\".format(len(policy_dict),len(sentences)))\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])\n",
    "sentences[\"70be962_99\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the sentences in 1 big dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all sentences\n",
    "import sys\n",
    "sys.path.append(\"/Users/dafirebanks/Projects/policy-data-analyzer/\")\n",
    "from tasks.data_loading.src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_sentences_from_dataset(dataset):\n",
    "    sentence_tags_dict = {}\n",
    "\n",
    "    for document in dataset.values():\n",
    "        sentence_tags_dict.update(document['sentences'])\n",
    "\n",
    "    return sentence_tags_dict\n",
    "\n",
    "sentences = labeled_sentences_from_dataset(policy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Federal Register, Volume 76 Issue 146 (Friday, July 29, 2011) [Federal Register Volume 76, Number 146 (Friday, July 29, 2011)] [Rules and Regulations] [Pages 45397-45399] From the Federal Register Online via the Government Publishing Office [[URL] [FR Doc No: 2011-19250] \\x00======================================================================== \\x00Rules and Regulations \\x00 Federal Register \\x00________________________________________________________________________ \\x00 \\x00This section of the FEDERAL REGISTER contains regulatory documents \\x00having general applicability and legal effect, most of which are keyed \\x00to and codified in the Code of Federal Regulations, which is published \\x00under 50 titles pursuant to 44 USC 1510.',\n",
       " 'label': []}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences['0002304f1f671ea916ae0a1f784484eb4874ceaa_sent_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "Then we build a simple function that takes four inputs:\n",
    "1. The model as we have set it in the previous line of code\n",
    "2. A dictionary that contains the sentences {\"\\<sentence_ID\\>\" : {\"text\" : \"The actual sentence\", labels : []}\n",
    "3. A query in the form of a string\n",
    "4. A similarity treshold. It is a float that we can use to limit the results list to the most relevant.\n",
    "\n",
    "The output of the function is a list with three columns with the following content:\n",
    "1. Column 1 contains the id of the sentence\n",
    "2. Column 2 contains the similarity score\n",
    "3. Column 3 contains the text of the sentence that has been compared with the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently two multi language models available for sentence similarity\n",
    "\n",
    "* xlm-r-bert-base-nli-stsb-mean-tokens: Produces similar embeddings as the bert-base-nli-stsb-mean-token model. Trained on parallel data for 50+ languages.\n",
    "<span style=\"color:red\"><strong>Attention!</strong></span> Model \"xlm-r-100langs-bert-base-nli-mean-tokens\" which was the name used in the original Omdena-challenge script has changed to this \"xlm-r-bert-base-nli-stsb-mean-tokens\"\n",
    "\n",
    "* distiluse-base-multilingual-cased-v2: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. While the original mUSE model only supports 16 languages, this multilingual knowledge distilled version supports 50+ languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to create the embeddings for each transformer the embeddings in a json with the following structure:\n",
    "# INPUT PARAMETERS\n",
    "# transformers: a list with transformer names\n",
    "# sentences_dict: a dictionary with the sentences of the database with the form {\"<sentence id>\" : \"<sentence text>\"}}\n",
    "# file: the filepath and filename of the output json\n",
    "# OUTPUT\n",
    "# the embeddings of the sentences in a json with the following structure:\n",
    "# {\"<transformer name>\" : {\"<sentence id>\" : <sentence embedding>}}\n",
    "\n",
    "def create_sentence_embeddings(model, sentences_dict, file):\n",
    "    embeddings = {}\n",
    "    for sentence_id, sentence_map in sentences_dict.items():\n",
    "        embeddings[sentence_id] = model.encode(sentence_map['text'].lower(), show_progress_bar=False)\n",
    "        \n",
    "    return embeddings\n",
    "#     with open(file, 'w') as fp:\n",
    "#         json.dump(embeddings, fp, cls = NumpyArrayEncoder)\n",
    "#     file_name = \"pre_embeddings/\" + file.split(\"/\")[3]\n",
    "#     s3.Object('wri-latin-talent', file_name).put(Body=open(file, 'rb'))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for sentences in the database with the pre-trained model\n",
    "\n",
    "This piece of code it's to be executed only once every time the database is changed or when we want to get the embeddings of a new database. For example, we are going to use it once for El Salvador policies and we don't need to use it again until we add new policies to this database.\n",
    "\n",
    "Instead, whenever we want to run experiments on this database, we will load the json files with the embeddings which are in the \"input\" folder.\n",
    "\n",
    "So, the next cell will be kept commented for safety reasons. Un comment it and execute it whenvere you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_ids = random.sample(list(sentences), 10)\n",
    "sample_sentences = {}\n",
    "for s_id in sample_sentence_ids:\n",
    "    sample_sentences.update({s_id: sentences[s_id]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00274763715091c9c1186a4d13f61ef5b773b923_sent_3727': {'text': '(B) Annual precipitation: Greater than 75 in (190 cm).',\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_1207': {'text': 'The recovery outline guides the immediate implementation of urgent recovery actions and describes the process to be used to develop a recovery plan.',\n",
       "  'label': []},\n",
       " '0020eabfcbd08554df1bd58438dcfda7f454d8c7_sent_26': {'text': 'Submit the non-CBI copy of your objection or hearing request, identified by docket ID number EPA-HQ-OPP-2015-0717, by one of the following methods: Federal eRulemaking Portal: [URL] Follow the online instructions for submitting comments.',\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_4487': {'text': 'Cyrtandra sessilis (HAIWALE) Oahu--Lowland Wet--Unit 6, Oahu--Lowland Wet--Unit 7, Oahu--Lowland Wet--Unit 8, Oahu--Lowland Wet--Unit 9, Oahu--Lowland Wet--Unit 10, Oahu--Lowland Wet--Unit 11, Oahu--Lowland Wet--Unit 12, Oahu--Lowland Wet--Unit 13, Oahu--Lowland Wet--Unit 14, Oahu--Lowland Wet--Unit 15, Oahu--Lowland Wet--Unit 16, Oahu--Wet Cliff--Unit 6, Oahu--Wet Cliff-- Unit 7, and Oahu--Wet Cliff--Unit 8, identified in the legal descriptions in paragraph (i) of this section, constitute critical habitat for Cyrtandra sessilis Oahu.',\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_860': {'text': 'Habitat Destruction and Modification by Climate Change Climate change will be a particular challenge for biodiversity because the introduction and interaction of additional stressors may push species beyond their ability to survive (Lovejoy et al 2005, pp 325-326).',\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_2467': {'text': 'Oahu--Lowland Wet--Unit 12 (and) Blackline Hawaiian Damselfly--Unit 7-- Lowland Wet (and) Crimson Hawaiian Damselfly--Unit 7--Lowland Wet (and) Oceanic Hawaiian Damselfly--Unit 8--Lowland Wet This area consists of 28 ac (11 ha) of City and County of Honolulu land and 26 ac (10 ha) of privately-owned land in the lowland wet ecosystem on the windward side of the Koolau Mountains, along Kahaluu Stream and tributary.',\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_960': {'text': \"While fish predation has been an important factor in the evolution of behavior in damselfly naiads in continental systems (Johnson 1991, p 8), it can only be speculated that Hawaii's stream- dwelling damselflies adapted behaviors to avoid the benthic feeding habits of native fish species.\",\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_2261': {'text': \"skottsbergii (Ewa Plains akoko)'' (Service 2012, entire).\",\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_2136': {'text': 'This unit also contains unoccupied habitat that is essential to the conservation of this species by providing the PCEs necessary for the expansion of the existing wild populations.',\n",
       "  'label': []},\n",
       " '00274763715091c9c1186a4d13f61ef5b773b923_sent_232': {'text': 'cornuta (NCN) is a palmoid (leaves dividing or radiating from one point) shrub in the rue family (Rutaceae) (Stone et al 1999, pp 1,209-1,210).',\n",
       "  'label': []}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6befc5ab859148f89adf21e2e26a0ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b116ec817a514a9daa0bb9efb656434a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105e68a618cc4ed184ab3c8e722e492d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd169ac12c44c5cb22258c50cc4bebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96cdd2f6084421fb75700d5d1811865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbd37a453c74b8cadd6230ac1206e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76103f0779649f89b6630ad9a51e864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd732618fb47465a8caa99b73e3f3479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062247d00eab493b9147bc6d32644273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dc9a746d594c36bda6f127241d3e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The building of a sentence embedding database for El Salvador in the two current models has taken 13.9182 seconds\n"
     ]
    }
   ],
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "# We will use only one transformer to compute embeddings\n",
    "transformer_name = 'xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "path = \"../../input/\"\n",
    "today = datetime.date.today()\n",
    "today = today.strftime('%Y-%m-%d')\n",
    "filename = \"Embeddings_\" + today + \"_ES.json\"\n",
    "file = path + filename\n",
    "\n",
    "\n",
    "model = SentenceTransformer(transformer_name)\n",
    "embs = create_sentence_embeddings(model, sample_sentences, file)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"The building of a sentence embedding database for El Salvador in the two current models has taken {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs['00274763715091c9c1186a4d13f61ef5b773b923_sent_3727'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: DEPRECATED\n",
    "\n",
    "We won't be saving the embeddings in S3 because that would cost too much memory and time\n",
    "\n",
    "### Loading the pre-trained model embeddings for database sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** in 1 pre_embeddings/Embeddings_2021-03-04_ES.json\n",
      "\n",
      "*** in 2 pre_embeddings/Embeddings_2021-03-04_ES.json\n"
     ]
    }
   ],
   "source": [
    "filter_language = \"_ES\"\n",
    "filter_prefix = \"pre_embeddings/\" #TODO: move everything to a \"Sentences\" folder\n",
    "\n",
    "sentence_embeddings = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix=filter_prefix):\n",
    "    if filter_language in obj.key:\n",
    "          serializedObject = obj.get()['Body'].read()\n",
    "          sentence_embeddings = {**sentence_embeddings, **json.loads(serializedObject)}\n",
    "    #     obj = s3.Object('wri-latin-talent',filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_embeddings))\n",
    "# for key in sentence_embeddings:\n",
    "#     print(key)\n",
    "#     print(len(sentence_embeddings[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assisted labeling by query search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is to use a set of queries to search a database for similar sentences with different transformers.\n",
    "The input parameters are:\n",
    "\n",
    "- Transformer_names: A list with the names of the transformers to be used. For multilingual similarity search we have two transformers\n",
    "- Queries: a list of the queries as strings, that we want to use for searching the database\n",
    "\n",
    "- Similarity_limit: The results are in the form of a similarity coefficient where 1 is a perfect match between the query embedding and the sentence in the database (the two vectors overlap). If the similarity coefficient is 0 the two vectors are orthogonal, they do not share anything in common. Thus, in order to restrict the number of results that are kept from the experiment we can set a similarity threshold. When we have a huge database a good treshold would be 0.3 to 0.5 or even higher.\n",
    "\n",
    "- Results_limit: instead of or complementary to Similarity_limit, we can limit our list of search results by the first sentences in the similarity ranking. We can set the limit to high numbers in an exploration phase and then reduce this number in a \"production\" phase\n",
    "\n",
    "- Filename: The results will be exported to the \"output/\" folder in json format, we need to give it a name witout extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity_search(model, queries, sentence_embeddings, sentences, similarity_limit, results_limit, filename):\n",
    "    results = {}\n",
    "    for query in queries:\n",
    "        Ti = time.perf_counter()\n",
    "        similarities = get_distance(model, sentence_embeddings, sentences, query, similarity_limit)\n",
    "        results[query] = similarities[0:results_limit]#results[transformer][query] = similarities[0:results_limit]\n",
    "        Tf = time.perf_counter()\n",
    "        print(f\"similarity search for query {query} has been done in {Tf - Ti:0.4f} seconds\")\n",
    "\n",
    "    path = \"../../output/\"\n",
    "    filename = filename + \".json\"\n",
    "    file = path + filename\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "    return results\n",
    "\n",
    "# This function helps debugging misspelling in the values of the dictionary\n",
    "def check_dictionary_values(dictionary):\n",
    "    check_country = {}\n",
    "    check_incentive = {}\n",
    "    for key, value in dictionary.items():\n",
    "        incentive, country = value.split(\"-\")\n",
    "        check_incentive[incentive] = 0\n",
    "        check_country[country] = 0\n",
    "    print(check_incentive)\n",
    "    print(check_country)\n",
    "\n",
    "def get_distance( model, sentence_emb, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query.lower(), show_progress_bar=False)\n",
    "    highlights = []\n",
    "    for sentence in sentences_dict:\n",
    "        sentence_embedding = sentence_emb[sentence]\n",
    "        score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "        if score > similarity_treshold:\n",
    "            highlights.append([sentence, score, sentences_dict[sentence]['text']])\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query building\n",
    "\n",
    "The code to compute sentence similarity will take two imputs:\n",
    "\n",
    "* The queries that will by input as a list of strings. \n",
    "* The embeddings of the sentences in the database. \n",
    "\n",
    "At this point all we need to run the experiment is ready but the list of queries. One can write the list manually, or one can make it from other data flows. The next cells are ment to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the databse of tagged sentences to define queries. The database is structured by countries. From a list of model documents the sentences were separated and tagged with a policy instrument label. The labels that were used are:\n",
    "\n",
    "* Credit\n",
    "* Direct payment\n",
    "* Fine\n",
    "* Guarantee\n",
    "* Supplies\n",
    "* Tax deduction\n",
    "* Technical assistance\n",
    "\n",
    "Not all countries have tagged sentences for each category so we ended up with 26 queries\n",
    "\n",
    "The difference between this experiment and experiment 2 is that here we have reformulated the query sentences by extracting the core incentive meaning from the original sentences, eliminating all the vocabulary not strictly speaking about incentives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict = {\n",
    "    \"Otorgamiento de estímulos crediticios por parte de el estado\" : \"Credit-México\",\n",
    "\"Estos créditos podrían beneficiar a sistemas productivos asociados a la pequeña y mediana producción\" : \"Credit-Perú\",\n",
    "\"Se asocia con créditos de enlace del Banco del Estado\" : \"Credit-Chile\", \n",
    "\"Acceso al programa de garantía crediticia para la actividad económica\" : \"Credit-Guatemala\",\n",
    "\"El banco establecerá líneas de crédito para que el sistema financiero apoye la pequeña, mediana y microempresa\" : \"Credit-El Salvador\",\n",
    "\"Dentro de los incentivos económicos se podrá crear un bono para retribuir a los propietarios por los bienes y servicios generados.\" : \"Direct_payment-México\",\n",
    "\"Acceso a los fondos forestales para el pago de actividad\" : \"Direct_payment-Perú\",\n",
    "\"Se bonificará el 90% de los costos de repoblación para las primeras 15 hectáreas y de un 75% respecto las restantes\" : \"Direct_payment-Chile\",\n",
    "\"El estado dará un incentivo que se pagará una sola vez a los propietarios forestales\" : \"Direct_payment-Guatemala\",\n",
    "\"Incentivos en dinero para cubrir los costos directos e indirectos del establecimiento y manejo de areas de producción\" : \"Direct_payment-El Salvador\",\n",
    "\"Toda persona física o moral que cause daños estará obligada a repararlo o compensarlo\" : \"Fine-México\",\n",
    "\"Disminuir los riesgos para el inversionista implementando mecanismos de aseguramiento\" : \"Guarantee-México\",\n",
    "\"Podrá garantizarse el cumplimiento de la actividad mediante fianza otorgada a favor del estado por cualquiera de las afianzadoras legalmente autorizadas.\" : \"Guarantee-Guatemala\",\n",
    "\"El sujeto de derecho podrá recibir insumos para la instalación y operación de infraestructuras para la actividad económica.\" : \"Supplies-México\",\n",
    "\"Se facilitará el soporte técnico a  través de la utilización de guías, manuales, protocolos, paquetes tecnológicos, procedimientos, entre otros.\" : \"Supplies-Perú\",\n",
    "\"Se concederán incentivos en especie para fomentar la actividad en forma de insumos\" : \"Supplies-El Salvador\",\n",
    "\"Se otorgarán incentivos fiscales para la actividad primaria y también la actividad de transformación\" : \"Tax_deduction-México\",\n",
    "\"De acuerdo con los lineamientos aprobados se concederá un 25% de descuento en el pago del derecho de aprovechamiento\" : \"Tax_deduction-Perú\",\n",
    "\"Las bonificaciones percibidas o devengadas se considerarán como ingresos diferidos en el pasivo circulante y no se incluirán para el cálculo de la tasa adicional ni constituirán renta para ningún efecto legal hasta el momento en que se efectúe la explotación o venta\" : \"Tax_deduction-Chile\",\n",
    "\"Los contratistas que suscriban contratos de exploración y/o explotación, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades\" : \"Tax_deduction-Guatemala\",\n",
    "\"Exención de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestación de Servicios, en la importación de sus bienes, equipos y accesorios, maquinaria, vehículos, aeronaves o embarcaciones\" : \"Tax_deduction-El Salvador\",\n",
    "\"Se facilitará formación Permanente Además del acompañamiento técnico, los sujetos de derecho participarán en un proceso permanente de formación a lo largo de todo el año, que les permita enriquecer sus habilidades y capacidades \" : \"Technical_assistance-México\",\n",
    "\"Contribuir en la promoción para la gestión, a través de la capacitación, asesoramiento, asistencia técnica y educación de los usuarios\" : \"Technical_assistance-Perú\",\n",
    "\"Asesoría prestada al usuario por un operador acreditado, conducente a elaborar, acompañar y apoyar la adecuada ejecución técnica en terreno de aquellas prácticas comprometidas en el Plan de Manejo\" : \"Technical_assistance-Chile\",\n",
    "\"Para la ejecución de programas de capacitación, adiestramiento y otorgamiento de becas para la preparación de personal , así como para el desarrollo de tecnología en actividades directamente relacionadas con las operaciones objeto del contrato\" : \"Technical_assistance-Guatemala\",\n",
    "\"Apoyo técnico y en formulación de proyectos y conexión con mercados\" : \"Technical_assistance-El Salvador\"}\n",
    "\n",
    "queries = []\n",
    "for query in queries_dict:\n",
    "    queries.append(query)\n",
    "        \n",
    "# print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just to check the presence of misspelling in the values of the queries dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Credit': 0, 'Direct_payment': 0, 'Fine': 0, 'Guarantee': 0, 'Supplies': 0, 'Tax_deduction': 0, 'Technical_assistance': 0}\n",
      "{'México': 0, 'Perú': 0, 'Chile': 0, 'Guatemala': 0, 'El Salvador': 0}\n"
     ]
    }
   ],
   "source": [
    "check_dictionary_values(queries_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity search for query Otorgamiento de estímulos crediticios por parte de el estado it's been done in 0.4415 seconds\n",
      "similarity search for query Estos créditos podrían beneficiar a sistemas productivos asociados a la pequeña y mediana producción it's been done in 0.0155 seconds\n",
      "similarity search for query Se asocia con créditos de enlace del Banco del Estado it's been done in 0.0154 seconds\n",
      "similarity search for query Acceso al programa de garantía crediticia para la actividad económica it's been done in 0.0148 seconds\n",
      "similarity search for query El banco establecerá líneas de crédito para que el sistema financiero apoye la pequeña, mediana y microempresa it's been done in 0.0165 seconds\n",
      "similarity search for query Dentro de los incentivos económicos se podrá crear un bono para retribuir a los propietarios por los bienes y servicios generados. it's been done in 0.0172 seconds\n",
      "similarity search for query Acceso a los fondos forestales para el pago de actividad it's been done in 0.0152 seconds\n",
      "similarity search for query Se bonificará el 90% de los costos de repoblación para las primeras 15 hectáreas y de un 75% respecto las restantes it's been done in 0.0165 seconds\n",
      "similarity search for query El estado dará un incentivo que se pagará una sola vez a los propietarios forestales it's been done in 0.0153 seconds\n",
      "similarity search for query Incentivos en dinero para cubrir los costos directos e indirectos del establecimiento y manejo de areas de producción it's been done in 0.0162 seconds\n",
      "similarity search for query Toda persona física o moral que cause daños estará obligada a repararlo o compensarlo it's been done in 0.0159 seconds\n",
      "similarity search for query Disminuir los riesgos para el inversionista implementando mecanismos de aseguramiento it's been done in 0.0149 seconds\n",
      "similarity search for query Podrá garantizarse el cumplimiento de la actividad mediante fianza otorgada a favor del estado por cualquiera de las afianzadoras legalmente autorizadas. it's been done in 0.0157 seconds\n",
      "similarity search for query El sujeto de derecho podrá recibir insumos para la instalación y operación de infraestructuras para la actividad económica. it's been done in 0.0167 seconds\n",
      "similarity search for query Se facilitará el soporte técnico a  través de la utilización de guías, manuales, protocolos, paquetes tecnológicos, procedimientos, entre otros. it's been done in 0.0192 seconds\n",
      "similarity search for query Se concederán incentivos en especie para fomentar la actividad en forma de insumos it's been done in 0.0152 seconds\n",
      "similarity search for query Se otorgarán incentivos fiscales para la actividad primaria y también la actividad de transformación it's been done in 0.0155 seconds\n",
      "similarity search for query De acuerdo con los lineamientos aprobados se concederá un 25% de descuento en el pago del derecho de aprovechamiento it's been done in 0.0147 seconds\n",
      "similarity search for query Las bonificaciones percibidas o devengadas se considerarán como ingresos diferidos en el pasivo circulante y no se incluirán para el cálculo de la tasa adicional ni constituirán renta para ningún efecto legal hasta el momento en que se efectúe la explotación o venta it's been done in 0.0189 seconds\n",
      "similarity search for query Los contratistas que suscriban contratos de exploración y/o explotación, quedan exentos de cualquier impuesto sobre los dividendos, participaciones y utilidades it's been done in 0.0182 seconds\n",
      "similarity search for query Exención de los derechos e impuestos, incluyendo el Impuesto a la Transferencia de Bienes Muebles y a la Prestación de Servicios, en la importación de sus bienes, equipos y accesorios, maquinaria, vehículos, aeronaves o embarcaciones it's been done in 0.0187 seconds\n",
      "similarity search for query Se facilitará formación Permanente Además del acompañamiento técnico, los sujetos de derecho participarán en un proceso permanente de formación a lo largo de todo el año, que les permita enriquecer sus habilidades y capacidades  it's been done in 0.0186 seconds\n",
      "similarity search for query Contribuir en la promoción para la gestión, a través de la capacitación, asesoramiento, asistencia técnica y educación de los usuarios it's been done in 0.0149 seconds\n",
      "similarity search for query Asesoría prestada al usuario por un operador acreditado, conducente a elaborar, acompañar y apoyar la adecuada ejecución técnica en terreno de aquellas prácticas comprometidas en el Plan de Manejo it's been done in 0.0181 seconds\n",
      "similarity search for query Para la ejecución de programas de capacitación, adiestramiento y otorgamiento de becas para la preparación de personal , así como para el desarrollo de tecnología en actividades directamente relacionadas con las operaciones objeto del contrato it's been done in 0.0185 seconds\n",
      "similarity search for query Apoyo técnico y en formulación de proyectos y conexión con mercados it's been done in 0.0149 seconds\n"
     ]
    }
   ],
   "source": [
    "transformer_name ='xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "# similarity_threshold = 0.2\n",
    "# search_results_limit = 1000\n",
    "# today = datetime.date.today()\n",
    "# today = today.strftime('%Y-%m-%d')\n",
    "# name = \"Pre_tagged_\" + today + \"_\" + filter_language\n",
    "\n",
    "model = SentenceTransformer(transformer_name)\n",
    "results_dict = sentence_similarity_search(model, queries, similarity_threshold, search_results_limit, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a temporary section to explore how to analyze the results. It is organized with the same structure as the section <strong>Defining queries</strong> as we are exploring the best search strategies based on different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the functions that are going to be used in the post-processing and in the analysis of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the contents of the results dict, particularly, the length of the first element and its contents\n",
    "def show_results(results_dictionary):\n",
    "    i = 0\n",
    "    for key1 in results_dictionary:\n",
    "        for key2 in results_dictionary[key1]:\n",
    "            if i == 0:\n",
    "                print(len(results_dictionary[key1][key2]))\n",
    "                print(results_dictionary[key1][key2])\n",
    "            i += 1\n",
    "\n",
    "# Adding the rank to each result\n",
    "def add_rank(results_dictionary):\n",
    "#     for model in results_dictionary:\n",
    "    for keyword in results_dictionary:#[model]:\n",
    "        i = 1\n",
    "        for result in results_dictionary[keyword]:#[model][keyword]:\n",
    "            result.insert(1, i)\n",
    "            i += 1\n",
    "    return results_dictionary\n",
    "\n",
    "# For experiments 2 and 3 this function is to save results in separate csv files\n",
    "def save_results_as_separate_csv(results_dictionary, queries_dictionary, date):\n",
    "    path = \"../../output/pre_labeled/\"\n",
    "#     for model, value in results_dictionary.items():\n",
    "    for exp_title, result in results_dictionary.items():#value.items():\n",
    "        filename = queries_dictionary[exp_title]\n",
    "        file = path + filename + \".csv\"\n",
    "        with open(file, 'w', newline='', encoding='utf-8') as f:\n",
    "            write = csv.writer(f)\n",
    "            write.writerows(result)\n",
    "#             print(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the analysis are saved as a json file. To further process the information we can upload the file contents into a dictionary.\n",
    "\n",
    "After loading the results, a rank value is added to the results from the highest similarity score to the lower one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json where there are the results that you want to analyze. CHANGE the file name accordingly.\n",
    "path = \"../../output/\"\n",
    "filename = \"Pre_tagged_2021-03-04__ES.json\"\n",
    "file = path + filename\n",
    "with open(file, \"r\") as f:\n",
    "    results_ = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = add_rank(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the rank to the results dictionary if we use the computed version not the uploaded\n",
    "results = copy.deepcopy(add_rank(results_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to simplify the analysis process and to make it available for a broader spectrum of analysts, the results are split into small \"csv\" documents that can be easily imported in spreadsheets.\n",
    "\n",
    "The new files will contain only the results of a single query, this is it will contain all the 100 (or whatever number has been retrieved) sentences from the database which have the highest similarity score with the query. There will be the following columns:\n",
    "\n",
    "* Sentence ID\n",
    "* Rank of the sentence in the similarity results\n",
    "* Similarity score\n",
    "* Text of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results as separete csv files\n",
    "\n",
    "save_results_as_separate_csv(results, queries_dict, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "757px",
    "left": "586px",
    "top": "630px",
    "width": "523px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
