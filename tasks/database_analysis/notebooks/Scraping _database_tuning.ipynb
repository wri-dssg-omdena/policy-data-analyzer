{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7948a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO;\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4386a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/propietari/Documents/fitxers importants/WRI/Scraping_results/\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c8225f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ElSalvador_20210730.csv',\n",
       " 'USFR_2_20210311.csv',\n",
       " 'USFR_20210311.csv',\n",
       " 'USFR_1_20210311.csv',\n",
       " 'USFR_20210702.csv',\n",
       " 'LeyChile_20210702.csv',\n",
       " 'Mexico_20210808.csv',\n",
       " 'India_20210310.csv',\n",
       " 'Mexico_20210703.csv',\n",
       " 'USFR_20210310.csv',\n",
       " 'India_20210703.csv',\n",
       " 'Mexico_20210809.csv',\n",
       " 'El_Salvador_no_duplicates.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2d2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/jordi/Documents/claus/'\n",
    "key_path = '/home/propietari/Documents/claus/'\n",
    "filename = 'AWS_S3_keys_wri.json'\n",
    "file = key_path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "# s3BucketName = \"wri-testing\"\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "# region = 'eu-central-1'\n",
    "region = \"us-east-1\"\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25223d",
   "metadata": {},
   "source": [
    "### El Salvador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46d8e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv files from scraping to be compared\n",
    "file_position = 0\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    ElSalvador_20210730 = [row[3] for row in reader]\n",
    "\n",
    "file_position = 12\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    El_Salvador_no_duplicates = [row[7] for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c24dea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 460 items in the old file and 271 in the new one\n",
      "There are 0 duplicates in the new version\n",
      "The difference between the first scraping and the last version is of 189 entries\n",
      "There are 236 files in the old version that are not in the new version\n",
      "There are 47 files in the new version that are not in the old version\n",
      "There are 507 unique ids in the union of the files\n"
     ]
    }
   ],
   "source": [
    "ElSalvador_20210730_SET = set(ElSalvador_20210730)\n",
    "print(f\"There are {len(El_Salvador_no_duplicates)} items in the old file and {len(ElSalvador_20210730)} in the new one\")\n",
    "print(f\"There are {len(ElSalvador_20210730) - len(ElSalvador_20210730_SET)} duplicates in the new version\")\n",
    "print(f\"The difference between the first scraping and the last version is of {len(El_Salvador_no_duplicates) - len(ElSalvador_20210730)} entries\")\n",
    "not_in_newest_version = set(El_Salvador_no_duplicates) - ElSalvador_20210730_SET\n",
    "print(f\"There are {len(not_in_newest_version)} files in the old version that are not in the new version\")\n",
    "not_in_oldest_version =  ElSalvador_20210730_SET - set(El_Salvador_no_duplicates)\n",
    "print(f\"There are {len(not_in_oldest_version)} files in the new version that are not in the old version\")\n",
    "print(f\"There are {len(set(El_Salvador_no_duplicates) | ElSalvador_20210730_SET)} unique ids in the union of the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5755b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to merge the two files in one. The fields were not saved in the sae order so it is necessary to rearrange them\n",
    "\n",
    "file_position = 0\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    ElSalvador_20210730 = list(reader)\n",
    "\n",
    "file_position = 12\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    El_Salvador_no_duplicates = list(reader)\n",
    "    \n",
    "final_list = []\n",
    "sv = []\n",
    "ids = {}\n",
    "for item in El_Salvador_no_duplicates:\n",
    "    if item[7] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[7])\n",
    "        final_item.append('El Salvador')\n",
    "        final_item.append('Diario Oficial')\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[4])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[6])\n",
    "        final_item.append(item[5])\n",
    "        final_item.append(item[3])\n",
    "        final_list.append(final_item)\n",
    "        sv.append(item[7])\n",
    "    \n",
    "for item in ElSalvador_20210730:\n",
    "    if item[3] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[3])\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[0])\n",
    "        final_item.append(item[7])\n",
    "        final_item.append(item[6])\n",
    "        final_item.append(item[10])\n",
    "        final_item.append(item[9])\n",
    "        final_item.append(item[4])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[3]] = 0\n",
    "        sv.append(item[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abandoned-geometry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'765d4ea2383aaf45bb29bf75feadea6e06a80fd2'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14153d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the differences with bucket files\n",
    "\n",
    "language = \"spanish\"\n",
    "in_prefix = f\"{language}_documents/HSSC/sentences/\"\n",
    "ids_S3 = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        ids_S3[name] = 0\n",
    "        \n",
    "processed_list = []\n",
    "for key in ids_S3:\n",
    "    processed_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7eabc89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped items that are not in sentences is 236\n",
      "The number of scraped items that are not in sentences is 271\n",
      "507\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped items that are not in sentences is {len(set(sv) - set(processed_list))}\")\n",
    "print(f\"The number of scraped items that are not in sentences is {len(set(sv) - set(new_list))}\")\n",
    "print(len(set(sv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a8d81b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "# Here we go and fetch the missing files and put them in the raw_pdf_updated folder\n",
    "prefix = f\"{language}_documents/raw_pdf_updated/\"\n",
    "\n",
    "\n",
    "counter = 0\n",
    "bucket = s3.Bucket(s3BucketName)\n",
    "for item in final_list:\n",
    "    if item[0] not in ids_S3:\n",
    "        counter += 1\n",
    "        key = prefix + item[0] + \".pdf\"\n",
    "        with requests.get(item[8], stream=True) as r:\n",
    "            bucket.upload_fileobj(r.raw, key)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24facd84",
   "metadata": {},
   "source": [
    "### Chile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1c80e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_position = 5\n",
    "with open(path + onlyfiles[file_position], 'r', encoding=\"ISO-8859-1\") as f:#, encoding = \"Latin1\"\n",
    "    reader = csv.reader(f)\n",
    "    chile = list(reader)\n",
    "    \n",
    "# final_list = []\n",
    "# ids = {}\n",
    "# dates = {}\n",
    "ch = []\n",
    "for item in chile:\n",
    "    if item[3].split(\".\")[0] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[3].split(\".\")[0])\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[0])\n",
    "        final_item.append(item[5])\n",
    "        final_item.append(item[6])\n",
    "#         dates[item[6]] = 0\n",
    "        final_item.append(item[10])\n",
    "        final_item.append(item[9])\n",
    "        final_item.append(item[4])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[3].split(\".\")[0]] = 0\n",
    "        ch.append(item[3].split(\".\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5088afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped items that are not in sentences is 4\n",
      "The number of scraped items that are not in sentences is 856\n",
      "860\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped items that are not in sentences is {len(set(ch) - set(processed_list))}\")\n",
    "print(f\"The number of scraped items that are not in sentences is {len(set(ch) - set(new_list))}\")\n",
    "print(len(set(ch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fb4c2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the differences with bucket files\n",
    "\n",
    "language = \"spanish\"\n",
    "in_prefix = f\"{language}_documents/HSSC/sentences/\"\n",
    "ids_S3 = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        ids_S3[name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f05095d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped items that are not in sentences is 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped items that are not in sentences is {len(ids.keys() - ids_S3.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f051aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Here we go and fetch the missing files and put them in the raw_pdf_updated folder\n",
    "prefix = f\"{language}_documents/text_files/HSSC/new/\"\n",
    "    \n",
    "counter = 0\n",
    "for item in final_list:\n",
    "    if item[0] not in ids_S3:\n",
    "        counter += 1\n",
    "        key = prefix + item[0] + \".txt\"\n",
    "        with requests.get(item[8], stream=True) as r:\n",
    "            s3.Object(s3BucketName, key).put(Body=r.content)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f5802",
   "metadata": {},
   "source": [
    "### Mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "171c5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_position = 6\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:#, encoding=\"ISO-8859-1\"\n",
    "    reader = csv.reader(f)\n",
    "    mexico_august = list(reader)\n",
    "    \n",
    "# final_list = []\n",
    "ids = {}\n",
    "# dates = {}\n",
    "mx_au = []\n",
    "for item in mexico_august:\n",
    "    if item[3].split(\".\")[0] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[3].split(\".\")[0])\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[0])\n",
    "        final_item.append(item[5])\n",
    "        final_item.append(item[6])\n",
    "#         dates[item[6]] = 0\n",
    "        final_item.append(item[10])\n",
    "        final_item.append(item[9])\n",
    "        final_item.append(item[4])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[3].split(\".\")[0]] = 0\n",
    "        mx_au.append(item[3].split(\".\")[0])\n",
    "        \n",
    "file_position = 8\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:#, encoding=\"ISO-8859-1\"\n",
    "    reader = csv.reader(f)\n",
    "    mexico_july = list(reader)\n",
    "    \n",
    "mx_ju = []\n",
    "for item in mexico_july:\n",
    "    if item[3].split(\".\")[0] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[3].split(\".\")[0])\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[0])\n",
    "        final_item.append(item[5])\n",
    "        final_item.append(item[6])\n",
    "#         dates[item[6]] = 0\n",
    "        final_item.append(item[10])\n",
    "        final_item.append(item[9])\n",
    "        final_item.append(item[4])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[3].split(\".\")[0]] = 0\n",
    "        mx_ju.append(item[3].split(\".\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c49b7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of newly scraped items that are not processed is 3956\n",
      "The number of scraped items that are not in sentences is 1140\n",
      "5096\n",
      "The number of newly scraped items that are not processed is 20\n",
      "The number of scraped items that are not in sentences is 3946\n",
      "3948\n",
      "The number of scraped items that are not in sentences is 3948\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of newly scraped items that are not processed is {len(set(mx_au) - set(processed_list))}\")\n",
    "print(f\"The number of scraped items that are not in sentences is {len(set(mx_au) - set(new_list))}\")\n",
    "print(len(set(mx_au)))\n",
    "print(f\"The number of newly scraped items that are not processed is {len(set(mx_ju) - set(processed_list))}\")\n",
    "print(f\"The number of scraped items that are not in sentences is {len(set(mx_ju) - set(new_list))}\")\n",
    "print(len(set(mx_ju)))\n",
    "print(f\"The number of scraped items that are not in sentences is {len(set(mx_ju) - set(mx_au))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a75f48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the differences with bucket files\n",
    "\n",
    "language = \"spanish\"\n",
    "in_prefix = f\"{language}_documents/HSSC/sentences/\"\n",
    "ids_S3 = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        ids_S3[name] = 0\n",
    "\n",
    "processed_list = []\n",
    "for key in ids_S3:\n",
    "    processed_list.append(key)\n",
    "        \n",
    "in_prefix = f\"{language}_documents/text_files/HSSC/new/\"        \n",
    "ids_S3_new = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".txt\" in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0].split(\".\")[0]\n",
    "        ids_S3_new[name] = 0\n",
    "        \n",
    "new_list = []\n",
    "for key in ids_S3_new:\n",
    "    new_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "268e701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped new that are not in processed is 4351\n",
      "The number of scraped processed that are not in new is 8562\n",
      "00009282cf7af3ed1a9e25e1b43791837e7c144a\n",
      "000fe0bdf5e9a0024ca7e9bb8b15dadf5648ae8c\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped new that are not in processed is {len(set(new_list) - set(processed_list))}\")\n",
    "print(f\"The number of scraped processed that are not in new is {len(set(processed_list) - set(new_list))}\")\n",
    "print(processed_list[0])\n",
    "print(new_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33dfcbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\r"
     ]
    }
   ],
   "source": [
    "# Here we go and fetch the missing files and put them in the raw_pdf_updated folder\n",
    "language = \"spanish\"\n",
    "prefix = f\"{language}_documents/text_files/HSSC/new/\"\n",
    "\n",
    "\n",
    "def save_to_s3(s3, bucket, file_key, text):\n",
    "    s3.Object(bucket, file_key).put(Body = text)\n",
    "\n",
    "def parse_other(response, key):\n",
    "    soup = BeautifulSoup(response.css('div#DivDetalleNota').get(), features = \"lxml\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \"\"\n",
    "    if len(paragraphs) == 0:\n",
    "        text = text + soup.text\n",
    "    else:\n",
    "        tables = soup.find_all(\"td\")\n",
    "        for line in paragraphs[1:]:\n",
    "            text = text + line.text + \"\\n\"\n",
    "        text = text + \"<table>\" + \"\\n\"\n",
    "        for cell in tables:\n",
    "            if \"En el documento que usted est√° visualizando\" not in cell.text:\n",
    "                text = text + cell.text + \"\\n\"\n",
    "        text = text + \"<\\\\table>\" + \"\\n\"\n",
    "    save_to_s3(s3, s3BucketName, key, text.replace(\"\\t\", \"\"))  \n",
    "\n",
    "\n",
    "counter = 0\n",
    "for item in final_list:\n",
    "    if item[0] not in ids_S3:\n",
    "        counter += 1\n",
    "        key = prefix + item[0] + \".txt\"\n",
    "        with requests.get(item[8], stream=True) as r:\n",
    "            resp = TextResponse(body=r.content, url=item[8])\n",
    "            parse_other(resp, key)\n",
    "        print(counter, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246265e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6366eded",
   "metadata": {},
   "source": [
    "### India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "91f11a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv files from scraping to be compared\n",
    "file_position = 6\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    india_old = [row[3] for row in reader]\n",
    "\n",
    "file_position = 9\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    india_new = [row[7] for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2a893226",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_position = 6\n",
    "with open(path + onlyfiles[file_position], 'r', encoding=\"ISO-8859-1\") as f:#, encoding = \"Latin1\"\n",
    "    reader = csv.reader(f)\n",
    "    india_old = list(reader)\n",
    "\n",
    "file_position = 9\n",
    "with open(path + onlyfiles[file_position], 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    india_new = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bda33ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/ Department of AGRICULTURE AND MARKETING',\n",
       " 'India',\n",
       " 'India Code',\n",
       " '859644d69fdd0b1e66f1dcf5a42f2d9a3f31fe90.pdf',\n",
       " 'https://www.indiacode.nic.in/bitstream/123456789/6515/1/kishan_ayog_2016_hindi_.pdf#search=Miner OR Ore OR Pit OR Bog OR Buffer OR Corridor OR (Country planning) OR Cropland OR (Degraded land) OR Desert OR Floodplain OR Forestland OR Freshwater OR Grassland OR (Land use) OR Landowner OR Mangrove OR Marsh OR Meadow [1950 TO 2021]',\n",
       " 'Act',\n",
       " ' 2016-11-30',\n",
       " ' 201629',\n",
       " ' Uttarakhand',\n",
       " '',\n",
       " 'The Uttarakhand state farmer comission Act',\n",
       " 'https://www.indiacode.nic.in/handle/123456789/2109?view_type=search&sam_handle=123456789/1362']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_old[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dda4de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'india_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cf58889838cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindia_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'india_new' is not defined"
     ]
    }
   ],
   "source": [
    "india_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-spending",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
